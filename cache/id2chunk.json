{"0": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "1", "chunk_id": "1", "chunk_text": "Zero-Shot Information Extraction via Chatting with ChatGPT Xiang Wei1, Xingyu Cui1, Ning Cheng1, Xiaobin Wang2, Xin Zhang, Shen Huang2, Pengjun Xie2, Jinan Xu1, Yufeng Chen1, Meishan Zhang, Yong Jiang2, and Wenjuan Han1 1 Beijing Jiaotong University, Beijing, China 2 DAMO Academy, Alibaba Group, China Abstract Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little hu- man intervention. Challenging but worth- while, zero-shot IE reduces the time and ef- fort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT- 3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to ex- plore prompt-based methods. In this work, we ask whether strong IE models can be con- structed by directly prompting LLMs. Specif- ically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evalu- ate our framework on three IE tasks: entity- relation triple extract, named entity recogni- tion, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on sev- eral datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources. 1 1 Introduction Information extraction aims to extract structured information from unstructured text into structured data formats, including tasks such as entity-relation triple extract (RE), named entity recognition (NER), event extraction (EE) (Tjong Kim Sang, 2002; Ratinov and Roth, 2009; Wei et al., 2020; Zheng et al., 2021; Li et al., 2020a), etc. It is a fun- damental and crucial task in natural language pro- cessing (Sarawagi et al., 2008). Working with an enormous amount of labeling data is always hectic, labor-intensive, and time-consuming. Hence, many organizations and companies rely on IE techniques to automate manual work with zero/few-shot meth- ods, e.g., clinical IE (Agrawal et al., 2022). 1https://github.com/cocacola-lab/ChatIE Recent works (Agrawal et al., 2022; Jeblick et al., 2022; Zhang et al., 2022) on large-scale pre-trained language models (LLM), such as GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022) and ChatGPT 2, suggest that LLMs perform well in various downstream tasks even without tuning the parameters but only with a few examples as instruc- tions. Hence, it is a timing question: Is it feasible to prompt LLMs to do zero-shot IE tasks under a uni\ufb01ed framework? It is challenging because the structured data containing multiple dependent ele- ments are dif\ufb01cult to extract through one-time pre- diction, especially for some complex tasks like RE. Previous works decompose these complex tasks into different parts and train several modules to solve each part. For example, in the RE task, the pipeline method PURE (Zhong and Chen, 2021) \ufb01rst identi\ufb01es two entities and then predicts the re- lations between them. However, supervision from labeled data is required in this model. Additionally, Li et al. (2019b) regard RE as a question-answering process by \ufb01rst extracting subjects and then objects according to the relation templates. Based on these clues, in this paper, we turn to ChatGPT and hypothesize that ChatGPT is born with the right abilities to deposit a uni\ufb01ed zero-shot IE model in an interactive mode. More speci\ufb01cally, we propose ChatIE by transforming the zero-shot IE task into a multi-turn question-answering prob- lem with a two-stage framework. In the \ufb01rst stage, we aim to \ufb01nd out the corresponding element types that may exist in a sentence. Then in the second stage, we perform a chained information extrac- tion to each element type from Stage I. Each stage is implemented with a multi-turn QA process. In each turn, we construct prompts based on designed templates and previously extracted information as input to ask ChatGPT. Finally, we compose the results of each turn into structured data. We con- duct extensive experiments on IE, NER, and EE 2https://openai.com/blog/chatgpt arXiv", "chunk_summary": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u5c06\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u8f6c\u5316\u4e3a\u4e00\u4e2a\u591a\u8f6e\u95ee\u7b54\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\u6765\u5b9e\u73b0\u3002\u5728\u6bcf\u4e2a\u9636\u6bb5\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u8bbe\u8ba1\u7684\u6a21\u677f\u548c\u4e4b\u524d\u63d0\u53d6\u7684\u4fe1\u606f\u6784\u5efa\u63d0\u793a\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u591a\u8f6e\u95ee\u7b54\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u8f6e\u6b21\u7684\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u6570\u636e\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChatIE\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u5e76\u751a\u81f3\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002"}, "1": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "1,2,3", "chunk_id": "2", "chunk_text": "ations between them. However, supervision from labeled data is required in this model. Additionally, Li et al. (2019b) regard RE as a question-answering process by \ufb01rst extracting subjects and then objects according to the relation templates. Based on these clues, in this paper, we turn to ChatGPT and hypothesize that ChatGPT is born with the right abilities to deposit a uni\ufb01ed zero-shot IE model in an interactive mode. More speci\ufb01cally, we propose ChatIE by transforming the zero-shot IE task into a multi-turn question-answering prob- lem with a two-stage framework. In the \ufb01rst stage, we aim to \ufb01nd out the corresponding element types that may exist in a sentence. Then in the second stage, we perform a chained information extrac- tion to each element type from Stage I. Each stage is implemented with a multi-turn QA process. In each turn, we construct prompts based on designed templates and previously extracted information as input to ask ChatGPT. Finally, we compose the results of each turn into structured data. We con- duct extensive experiments on IE, NER, and EE 2https://openai.com/blog/chatgpt arXiv:2302.10205v1  [cs.CL]  20 Feb 2023 Figure 1: Illustration for the framework. For convenience, we use the samples of DuIE2.0 as examples of three tasks to show. tasks, including six datasets across two languages: English and Chinese. Empirical results show that while vanilla ChatGPT without using ChatIE fails in solving IE with original task instruction, our pro- posed two-stage framework instantiated on Chat- GPT succeeds when the IE task is decomposed into multiple simpler and easier sub-tasks. Surprisingly, ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL (Takanobu et al., 2019)). 2 ChatIE 2.1 Multi-Turn QA framework for zero-shot IE We decompose the IE task into two stages, each containing several turns of QA, which refer to the dialogue with ChatGPT. In the \ufb01rst stage, we aim to \ufb01nd out the existing types of entities, relations, or events in the sentence respectively in three tasks. In this way, we \ufb01lter out the element types that do not exist to reduce the search space and com- putational complexity, conducing to extracting in- formation. Then in the second stage, we further extract relevant information based on the element types extracted in the \ufb01rst stage as well as the cor- responding task-speci\ufb01c scheme. The overview of our framework is shown in Figure 1, which we will describe in detail later. Stage I: For one sample, this stage generally includes only one turn of QA. In order to \ufb01nd the element types presented in the sentence, we \ufb01rst utilize the task-speci\ufb01c TypeQuesTemplates and the list of element types to construct the question. Then we combine the question and sentence as input to ChatGPT. To facilitate answer extraction, we ask the system to reply in the list form. If the sentence does not contain any element types, the system will generate a response with NONE Token. Stage II: This stage generally includes multiple QA turns. In advance, we design a series of speci\ufb01c ChainExtractionTemplates for element types ac- cording to the scheme of the task. The ChainExtrac- tionTemplates de\ufb01ne a chain of question templates and the length of the chain is usually one. But for complicated schemes such as complex-object value extraction in entity-relation triple extraction, the length of the chain is greater than one. At this point, the extraction of an element may depend on another previous element, so we call it chained templates. We perform multi turns QA in the order of previously extracted element types as well as the order of ChainExtractionTemplates. To generate a question, we need to retrieve the template with the element type and \ufb01ll the corresponding slots if necessary. Then we access ChatGPT and get a response. Finally, we compose structured informa- tion based on the elements extracted in each turn. Similarly, for the convenience of answer extraction, we ask the system to reply in table form. If nothing is extracted, the system will generate a response with NONE token. 2.2 Applying the Framework to IE tasks After curating the uni\ufb01ed framework, ChatIE, we\u2019ll then start applying the framework to information extraction tasks, to process and build models for each task. 2.2.1 Entity-Relation Triple Extraction Given a sentence x and question prompt q, the model is desired to predict triples Tpxq \u201c tps1, r1, o1q, \u00a8 \u00a8 \u00a8 , psn, rn, onqu", "chunk_summary": "\u672c\u6587\u5c06\u96f6-shot\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u8f6c\u5316\u4e3a\u4e00\u4e2a\u57fa\u4e8e\u591a\u8f6e\u95ee\u7b54\u7684\u6846\u67b6\u2014\u2014ChatIE\uff0c\u5e76\u5229\u7528ChatGPT\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\uff0c\u5c06IE\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u7b80\u5355\u7684\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7QA\u6a21\u5f0f\u5b9e\u73b0\u4fe1\u606f\u63d0\u53d6\u3002ChatIE\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u5305\u62ec\u591a\u8f6e\u95ee\u7b54\uff0c\u901a\u8fc7\u6784\u5efa\u6a21\u677f\u548c\u5229\u7528ChatGPT\u8fdb\u884cQA\u5b9e\u73b0\u4fe1\u606f\u63d0\u53d6\u3002\u6b64\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5c55\u73b0\u51fa\u8f83\u597d\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u5b9e\u4f53-\u5173\u7cfb\u4e09\u5143\u7ec4\u63d0\u53d6\u4efb\u52a1\uff0c\u6a21\u578b\u5728\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50\u548c\u95ee\u9898\u63d0\u793a\u540e\uff0c\u9884\u6d4b\u4e09\u5143\u7ec4 Tpxq \" tps1\uff0cr1\uff0co1q\uff0c...\uff0cpsn\uff0crn\uff0conq\u3002"}, "2": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "2,3", "chunk_id": "3", "chunk_text": "-object value extraction in entity-relation triple extraction, the length of the chain is greater than one. At this point, the extraction of an element may depend on another previous element, so we call it chained templates. We perform multi turns QA in the order of previously extracted element types as well as the order of ChainExtractionTemplates. To generate a question, we need to retrieve the template with the element type and \ufb01ll the corresponding slots if necessary. Then we access ChatGPT and get a response. Finally, we compose structured informa- tion based on the elements extracted in each turn. Similarly, for the convenience of answer extraction, we ask the system to reply in table form. If nothing is extracted, the system will generate a response with NONE token. 2.2 Applying the Framework to IE tasks After curating the uni\ufb01ed framework, ChatIE, we\u2019ll then start applying the framework to information extraction tasks, to process and build models for each task. 2.2.1 Entity-Relation Triple Extraction Given a sentence x and question prompt q, the model is desired to predict triples Tpxq \u201c tps1, r1, o1q, \u00a8 \u00a8 \u00a8 , psn, rn, onqu, where typeppsi, ri, oiqq P TT, a list of potential triple types. Formally for an output triple ps, r, oq, we can express the process as: ppps, r, oq|x, qq \u201c ppr|x, q1q loooomoooon Stage I ppps, oq|qrq complex object hkkikkj \u00a8 \u00a8 \u00a8 \u00a8 \u00a8 \u00a8 looooooooooooomooooooooooooon Stage II (1) Where q1 is the question generated using rela- tion types list R and the corresponding template in Stage I. And qr is the question generated using the template related to the previously extracted relation type in Stage II. It is worth noting that we omit x in Stage II, because ChatGPT can record the relevant information of each turn QA. In addition, we need further several turns QA for samples with complex- object values. The complex-object value refers to an object with multiple attributes. 2.2.2 Named Entity Recognition For the NER task, the \ufb01rst stage is to \ufb01lter out the existing entity types in the sentence given the desired type list. Once we get the entity types, we can construct the input for the second stage accordingly. In the second stage, each turn aims to extract the entities of one type. So the number of turns in Stage II is up to the number of entities obtained in Stage I, and Stage II is omitted if the \ufb01rst stage gets no types at all. In the experiment, the BIO annotation is not considered since it\u2019s kind of hard for ChatGPT. Besides, because the type of entities is few (3 types in conllpp, 4 types in msra) in the datasets, the \ufb01rst stage is skipped in the actual experiment, asking for every type of entity in the second stage. 2.2.3 Event Extraction ChatIE divides the zero-shot EE task into two sub- tasks: event classi\ufb01cation and argument extrac- tion, solved with two stages in a pipelined fash- ion. The \ufb01rst stage is designed for event classi\ufb01- cation, formalized as a text classi\ufb01cation problem getting event types from a given text. The sec- ond stage is then devoted to argument extraction, formalized as an extractive machine read compre- hension (MRC) problem that identi\ufb01es arguments of speci\ufb01c roles associated with predicted event types from the Stage I. 3 Experiment 3.1 Setup Datasets RE. NYT11-HRL (Takanobu et al., 2019) is a preprocessed version of NYT11 (Riedel et al., 2010; Hoffmann et al., 2011) and contains 12 pre- de\ufb01ned relation types. DuIE2.0 (Li et al., 2019a) is the industry\u2019s largest schema-based Chinese RE dataset and contains 48 prede\ufb01ned relation types. Some of the objects in the triples have multiple attributes, called complex-object values. NER. The conllpp (Wang et al., 2019) dataset is a modi\ufb01ed version of the conll2003 (Tjong Kim Sang and De Meulder, 2003) and contains 4 entity types. MSRA (Levow, 2006) is a Chinese named entity recognition dataset for the news \ufb01eld and contains 3 entity types. EE. DuEE1.0 (Li et al., 2020b) is a Chinese event extraction dataset released by Baidu, which contains 65 event types. The", "chunk_summary": "\u5728\u8fd9\u4e00\u90e8\u5206\u4e2d\uff0c\u4ecb\u7ecd\u4e86\u5c06ChatGPT\u5e94\u7528\u4e8e\u4fe1\u606f\u63d0\u53d6\uff08IE\uff09\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u5305\u62ec\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\u63d0\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u3002\u5bf9\u4e8e\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\u63d0\u53d6\uff0c\u5c06\u5176\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u751f\u6210\u95ee\u9898\u6a21\u677f\u5e76\u63d0\u53d6\u4e3b\u8bed\u548c\u8c13\u8bed\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u6839\u636e\u524d\u4e00\u4e2a\u63d0\u53d6\u7684\u5173\u7cfb\u7c7b\u578b\u751f\u6210\u95ee\u9898\u6a21\u677f\u5e76\u63d0\u53d6\u5ba2\u4f53\u3002\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\uff0c\u7b2c\u4e00\u9636\u6bb5\u662f\u8fc7\u6ee4\u51fa\u5b58\u5728\u4e8e\u53e5\u5b50\u4e2d\u7684\u5b9e\u4f53\u7c7b\u578b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u6839\u636e\u7b2c\u4e00\u9636\u6bb5\u7684\u5b9e\u4f53\u7c7b\u578b\u751f\u6210\u95ee\u9898\u5e76\u63d0\u53d6\u5b9e\u4f53\u3002\u5728\u4e8b\u4ef6\u63d0\u53d6\u4e2d\uff0c\u5c06\u5176\u5206\u4e3a\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a\u4e8b\u4ef6\u5206\u7c7b\u548c\u53c2\u6570\u63d0\u53d6\u3002\u7b2c\u4e00\u9636\u6bb5\u662f\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u4ee5\u83b7\u53d6\u4e8b\u4ef6\u7c7b\u578b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u9488\u5bf9\u7b2c\u4e00\u9636\u6bb5\u9884\u6d4b\u7684\u4e8b\u4ef6\u7c7b\u578b\u8fdb\u884c\u53c2\u6570\u63d0\u53d6\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5173\u7cfb\u63d0\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u6570\u636e\u96c6\u3002"}, "3": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "3,4", "chunk_id": "4", "chunk_text": "\ufffd\ufffdc roles associated with predicted event types from the Stage I. 3 Experiment 3.1 Setup Datasets RE. NYT11-HRL (Takanobu et al., 2019) is a preprocessed version of NYT11 (Riedel et al., 2010; Hoffmann et al., 2011) and contains 12 pre- de\ufb01ned relation types. DuIE2.0 (Li et al., 2019a) is the industry\u2019s largest schema-based Chinese RE dataset and contains 48 prede\ufb01ned relation types. Some of the objects in the triples have multiple attributes, called complex-object values. NER. The conllpp (Wang et al., 2019) dataset is a modi\ufb01ed version of the conll2003 (Tjong Kim Sang and De Meulder, 2003) and contains 4 entity types. MSRA (Levow, 2006) is a Chinese named entity recognition dataset for the news \ufb01eld and contains 3 entity types. EE. DuEE1.0 (Li et al., 2020b) is a Chinese event extraction dataset released by Baidu, which contains 65 event types. The ACE05 3 corpus pro- vides event annotations in document and sentence levels from a variety of domains such as newswires and online forums. Evaluation Metrics RE. We report the standard micro F1 measure and adopt two evaluate metrics: 1) border evalua- tion (BE): an extracted relation triple (subject, rela- tion, object) is considered as correct if the whole entity span of both subject and object and relation are all correct. 2) strict evaluation (SE): in addi- tion to what is required in the border evaluation, the type of both subject and object also must be correct. We use BE on NYT11-HRL because there is no annotation of entity types and use SE on DuIE2.0. 3https://catalog.ldc.upenn.edu/LDC2006T06 RE NER EE DuIE2.0 NYT11-HRL MSRA collnpp DuEE1.0 ACE05 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 fs-1 0.0 0.0 0.0 0.0 0.0 0.0 14.7 7.9 9.7 2.71 17.2 4.66 0.4 0.2 0.3 0.0 0.0 0.0 fs-5 0.0 0.0 0.0 0.0 0.0 0.0 34.5 10.3 15.5 2.53 16.65 4.38 0.2 0.6 0.3 0.0 0.0 0.0 fs-10 16.5 0.1 0.2 0.0 0.0 0.0 60.0 30.9 40.6 2.49 18.54 4.38 2.1 0.7 1.0 0.0 0.0 0.0 fs-20 41.4 0.4 0.8 3.4 2.7 0.5 63.4 44.8 52.5 2.48 19.36 4.41 1.7 0.8 1.1 4.6 0.1 0.2 fs-50 45.7 2.5 4.7 11.7 1.9 3.3 71.6 62.4 66.6 41.94 11.55 8.93 3.2 8.5 4.6 6.7 1.6 2.6 fs-100 50.8 7.2 12.0 34.8 6.2 10.6 81.3 76.1 78.6 50.26 24.97 32.89 8.7 12.0 10.1 8.0 4.9 6.0 full-shot 68.9 72.2 70.5 47.9 55.1 51.3 96.33 95.63 95.98 94.18 94.61 94.39 50.9 42.8 46.5 45.3 54.3 49.4 FCM - - - 43.2 29.4 35.0 - - - - - - - -", "chunk_summary": "\u8be5\u6587\u6863\u4ecb\u7ecd\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u62bd\u53d6\u6280\u672f\uff0c\u5e76\u5229\u7528\u4e0eChatGPT\u7684\u5bf9\u8bdd\u8fdb\u884c\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u5305\u62ecRE\u3001NER\u548cEE\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u8bc4\u4f30\u6307\u6807\u4f7f\u7528\u4e86\u6807\u51c6\u7684\u5faeF1\u5ea6\u91cf\u548c\u4e24\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5373\u8fb9\u7f18\u8bc4\u4f30\u548c\u4e25\u683c\u8bc4\u4f30\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0cFCM\u8868\u73b0\u7684\u6700\u4f73\uff0c\u53ef\u4ee5\u8fbe\u5230\u6574\u4e2a\u6570\u636e\u96c6\u7684F1\u5f97\u5206\u3002"}, "4": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "4", "chunk_id": "5", "chunk_text": "0.1 0.2 fs-50 45.7 2.5 4.7 11.7 1.9 3.3 71.6 62.4 66.6 41.94 11.55 8.93 3.2 8.5 4.6 6.7 1.6 2.6 fs-100 50.8 7.2 12.0 34.8 6.2 10.6 81.3 76.1 78.6 50.26 24.97 32.89 8.7 12.0 10.1 8.0 4.9 6.0 full-shot 68.9 72.2 70.5 47.9 55.1 51.3 96.33 95.63 95.98 94.18 94.61 94.39 50.9 42.8 46.5 45.3 54.3 49.4 FCM - - - 43.2 29.4 35.0 - - - - - - - - - - - - MultiR - - - 32.8 30.6 31.7 - - - - - - - - - - - - single 17.8 7.7 10.7 10.8 5.7 7.4 56.3 57.3 56.8 61.4 43.0 50.6 61.7 77.5 68.7 18.2 23.9 20.7 ChatIE 74.6 67.5 70.9 30.6 48.4 37.5 58.4 57.0 57.7 62.3 55.0 58.4 66.5 78.5 72.0 25.3 35.5 29.5 Table 1: F1 score on six datasets over two languages. NER. We only consider the complete match- ing and use the micro F1 to evaluate NER task. Only when both the border and the type of the pre- dicted entity and the true entity are the same will we regard it as a correct prediction. EE. We adopt the different evaluation metrics on the DuEE1.0 dataset and ACE05 dataset. For the DuEE1.0 dataset, F-measure (F14) is scored ac- cording to the word-level matching. For the ACE05 dataset, the predicted argument results are matched with the manually marked argument results at the entity level and evaluated by the micro F1. 3.2 Main Results We summarize the main results in Table 1. We observe that while vanilla ChatGPT (Row single, ChatGPT using a single-turn QA instead of ChatIE) performs poorly in solving IE, our proposed two- stage framework based on ChatGPT (Row ChatIE) succeeds. ChatIE generally improves performance over six widely used IE datasets by 18.98% points signi\ufb01cantly on average. Notably, the gains become more signi\ufb01cant com- pared with few-shot approaches. For each few-shot experiment, we randomly select 3 sets of the train- ing data, and train 3 times on each set to get an aver- age result. The baselines are PaddleNLP LIC2021 IE 5 and CaseRel (Wei et al., 2020) for RE, AdaSeq Bert-CRF 6 for NER, PaddleNLP LIC2021 EE 7 4https://github.com/PaddlePaddle/PaddleNLP/ tree/develop/examples/information_extraction/ DuEE 5github.com/PaddlePaddle/PaddleNLP/tree/ develop/examples/information_extraction/DuIE 6github.com/modelscope/AdaSeq/tree/master/ examples/bert_crf 7github.com/PaddlePaddle/PaddleNLP/tree/ develop/examples/information_extraction/DuEE and Text2event (Lu et al., 2021) for EE. ChatIE is comparable to fs-20 on MSRA, or outperforms fs- 100 on NYT11-HRL, collnpp and ACE05, or even surpasses the full-shot on DuIE2.0 and DuEE1.0 in terms of performance. More surprisingly, compared with two super- vised models FCM (Gormley et al., 2015) and MultiR (Hoffmann et al., 2011) on NYT11- HRL, ChatIE surpassed them by 2.5% and 5.8% respectively. Supervised learning models are computationally-intensive and require high-quality labeled data. Additionally, for each task, an", "chunk_summary": "\u672c\u6bb5\u4e3b\u8981\u4ecb\u7ecd\u4e86\u8bba\u6587\u4e2d\u91c7\u7528\u7684\u8bc4\u4ef7\u6307\u6807\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002\u5728\u8bc4\u4ef7\u6307\u6807\u65b9\u9762\uff0c\u5bf9\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\uff0c\u4f7f\u7528 micro F1 \u8861\u91cf\u8bc4\u4ef7\u7ed3\u679c\uff1b\u5bf9\u4e8e\u4e8b\u4ef6\u62bd\u53d6\uff08EE\uff09\u4efb\u52a1\uff0c\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u91c7\u7528\u4e0d\u540c\u7684\u8bc4\u4ef7\u6307\u6807\u3002\u5728\u5b9e\u9a8c\u7ed3\u679c\u65b9\u9762\uff0c\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e ChatGPT \u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff08ChatIE\uff09\u76f8\u8f83\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728\u516d\u4e2a\u5e38\u7528\u4fe1\u606f\u62bd\u53d6\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e8618.98%\u5de6\u53f3\uff0c\u8868\u73b0\u4f18\u4e8e\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u751a\u81f3\u8d85\u8fc7\u4e86\u5168\u6837\u672c\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u76f8\u8f83\u4e8e\u5927\u91cf\u8ba1\u7b97\u548c\u9ad8\u6807\u6ce8\u8d28\u91cf\u9700\u6c42\u7684\u4f20\u7edf\u6a21\u578b\uff0cChatIE \u5728 NYT11-HRL \u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86 FCM \u6a21\u578b\u548c MultiR \u6a21\u578b\u3002"}, "5": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "4,5", "chunk_id": "6", "chunk_text": "ER, PaddleNLP LIC2021 EE 7 4https://github.com/PaddlePaddle/PaddleNLP/ tree/develop/examples/information_extraction/ DuEE 5github.com/PaddlePaddle/PaddleNLP/tree/ develop/examples/information_extraction/DuIE 6github.com/modelscope/AdaSeq/tree/master/ examples/bert_crf 7github.com/PaddlePaddle/PaddleNLP/tree/ develop/examples/information_extraction/DuEE and Text2event (Lu et al., 2021) for EE. ChatIE is comparable to fs-20 on MSRA, or outperforms fs- 100 on NYT11-HRL, collnpp and ACE05, or even surpasses the full-shot on DuIE2.0 and DuEE1.0 in terms of performance. More surprisingly, compared with two super- vised models FCM (Gormley et al., 2015) and MultiR (Hoffmann et al., 2011) on NYT11- HRL, ChatIE surpassed them by 2.5% and 5.8% respectively. Supervised learning models are computationally-intensive and require high-quality labeled data. Additionally, for each task, an indi- vidual model is trained from scratch. In contrast, ChatIE works without any \ufb01netuning and training to update parameters. It vastly reduces the compu- tation and time investment. With all these bene\ufb01ts, ChatIE still outperforms these supervised learning. 4 Case Study The \ufb01rst sentence \u201cJust as the JAMA article was being published, three dozen children began dy- ing of acute renal failure at two hospitals in Delhi, India.\u201d is an RE case where the same pair of enti- ties belong to two different types of relations. The triples are (India, location-contains, Delhi) and (Delhi, administration_division-country, India). In the \ufb01rst stage, ChatIE detects the two relation types. Then in the second stage, ChatIE further extract the words Delhi and India and con\ufb01rms which one is the source entity and which is the target. This shows ChatIE\u2019s ability to give different la- bels to the same entity in different relations. It is worth noting that we convert location-contains to location-located_in to predict in the actual exper- iment, which means we regard (Delhi, location- located_in, India) and (India, location-contains, Delhi) as equivalent. The second sentence \u201cFour other Google exec- utives the chief \ufb01nancial of\ufb01cer, George Reyes; the senior vice president for business operations, Shona Brown; the chief legal of\ufb01cer, David Drum- mond; and the senior vice president for prod- uct management, Jonathan Rosenberg earned salaries of $ 250,000 each.\u201d is an RE example where one relation involves multiple triples. It\u2019s hard for many methods to extract but it is ac- complished by ChatIE. The extracted triples are (George Reyes, person-company, Google), (Shona Brown, person-company, Google), (David Drum- mond, person-company, Google) and (Jonathan Rosenberg, person-company, Google). ChatIE \ufb01rst \ufb01lters out the person-company and outputs the 4 triples related to the relation at the same time in the second stage. The third sentence \u201cScore on the \ufb01rst day of the four-day Shef\ufb01eld Shield match between Tasmania and Victoria at Bellerive Oval on Friday.\u201d is a NER example with confusing entities. The word \u201cTasmania\u201d and \u201cVictoria\u201d can be categorized as \u201cLOCATION\u201d types, but are actually team names in this sentence, which are \u201cORGANIZATION\u201d types. ChatIE can recognize the confusing point, showing its advantage in understanding the sentence and choosing the right word meanings. The last sentence \u201cClinton suffered greatly over the 19 Rangers that died, 18 on the 3rd of October and MattReersen (ph) three days later.\u201d is an EE example. In the \ufb01rst stage, ChatIE gets the event type when scanning the word \u201cdied\u201d. Then it goes from this word to catch the victim \u201c19 rangers\u201d, further detects the agent \u201cClinton\u201d before the pred- icate, and targets on \u201c3rd of October\u201d and \u201cthree days later\u201d. 5 Vanilla Prompt vs. Our Chat-based Prompt Table 2, 3 and 4 demonstrate the comparison of vanilla prompts and our Chat-based prompts in terms of IE. 8 6 Related Work Working with an enormous amount of labeling data is always hectic, labor-intensive, and time- consuming. Hence, researchers focus on zero/few- shot technologies even though IE is challenging 8The experiments are conducted using the version of Chat- GPT prior to January 30, 2023. in low-resource scenarios, such as few-shot rela- tion", "chunk_summary": "\u8fd9\u6bb5\u6587\u5b57\u4ecb\u7ecd\u4e86ChatIE\u5728\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u4e9b\u6848\u4f8b\u7814\u7a76\u3002\u76f8\u6bd4\u4e8e\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0cChatIE\u4e0d\u9700\u8981\u7ecf\u8fc7\u5fae\u8c03\u548c\u8bad\u7ec3\u5c31\u53ef\u4ee5\u5de5\u4f5c\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u65f6\u95f4\u7684\u6295\u5165\uff0c\u8868\u73b0\u4f9d\u7136\u8f83\u597d\u3002\u6b64\u5916\uff0c\u5bf9\u6bd4\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0cChatIE\u5177\u6709\u8bc6\u522b\u540c\u4e00\u5b9e\u4f53\u5728\u4e0d\u540c\u5173\u7cfb\u4e2d\u5177\u6709\u4e0d\u540c\u6807\u7b7e\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u8bc6\u522b\u5b9e\u4f53\u5728\u53e5\u5b50\u4e2d\u7684\u6b63\u786e\u7c7b\u578b\u3002\u6587\u7ae0\u8fd8\u5bf9Chat-based prompts\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u6700\u540e\uff0c\u4f5c\u8005\u63d0\u5230\uff0c\u5c3d\u7ba1\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u5728\u5c11\u91cf\u6570\u636e\u60c5\u51b5\u4e0b\u8fdb\u884c\u96f6/\u51e0-shot\u6280\u672f\u7684\u7814\u7a76\u4ecd\u7136\u53d7\u5230\u7814\u7a76\u4eba\u5458\u7684\u5173\u6ce8\u3002"}, "6": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "5,6", "chunk_id": "7", "chunk_text": " team names in this sentence, which are \u201cORGANIZATION\u201d types. ChatIE can recognize the confusing point, showing its advantage in understanding the sentence and choosing the right word meanings. The last sentence \u201cClinton suffered greatly over the 19 Rangers that died, 18 on the 3rd of October and MattReersen (ph) three days later.\u201d is an EE example. In the \ufb01rst stage, ChatIE gets the event type when scanning the word \u201cdied\u201d. Then it goes from this word to catch the victim \u201c19 rangers\u201d, further detects the agent \u201cClinton\u201d before the pred- icate, and targets on \u201c3rd of October\u201d and \u201cthree days later\u201d. 5 Vanilla Prompt vs. Our Chat-based Prompt Table 2, 3 and 4 demonstrate the comparison of vanilla prompts and our Chat-based prompts in terms of IE. 8 6 Related Work Working with an enormous amount of labeling data is always hectic, labor-intensive, and time- consuming. Hence, researchers focus on zero/few- shot technologies even though IE is challenging 8The experiments are conducted using the version of Chat- GPT prior to January 30, 2023. in low-resource scenarios, such as few-shot rela- tion classi\ufb01cation or extraction (Sainz et al., 2021; Han et al., 2018), few-shot event argument extrac- tion (Sainz et al., 2022a) and few-shot information extraction(Sainz et al., 2022b). ChatGPT has gained widespread attention re- cently. Many \ufb01elds received its impacts and evolv- ing fast, such as Medicine (Jeblick et al., 2022; King, 2022) and Online Exam (Susnjak, 2022). In the NLP community, there are new investigations with ChatGPT in several tasks as well. For exam- ple, (Zhang et al., 2022) use ChatGPT achieved state-of-the-art performance on Stance Detection, (Guo et al., 2023) evaluated its helpfulness on ques- tion answering, (Jiao et al., 2023) state that it is a good translator for spoken language. We try to dig into its information extraction ability, suggesting a simple zero-shot IE framework. 7 Conclusion We presented ChatIE, a multi-turn QA framework for zero-shot information extraction based on Chat- GPT. Through this interactive mode, ChatIE can decompose complex IE tasks into several parts and compose the results of each turn into a \ufb01nal struc- tured result. We apply this framework to RE, NER, and EE tasks and conduct extensive experiments on six datasets across two languages to validate its ef- fectiveness. Surprisingly, ChatIE achieves impres- sive performance and even surpasses some full-shot models on several datasets. This work paves the way for a new paradigm for zero-shot IE, where the experts decompose IE task into multiple simpler and easier sub-tasks, de\ufb01ne chat-like prompts, and directly runs those speci\ufb01cations without training and \ufb01netuning. References Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022. Large language models are zero-shot clinical information extractors. arXiv preprint arXiv:2205.12689. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Matthew R Gormley, Mo Yu, and Mark Dredze. 2015. Improved relation extraction with feature-rich com- positional embedding models. In Proceedings of the 1 Vanilla Prompt Chat-based Prompt STAGE I Question: Suppose you are an entity-relationship triple ex- traction model. I\u2019ll give you list of head entity types: subject_types, list of tail entity types: ob- ject_types, list of relations: relations. Give you a sentence, please extract the subject and object in the sentence based on these three lists, and form a triplet in the form of (subject, relation, object). The given sentence is \"Bono said that President Jacques Chirac of France had spoken eloquently of the need to support Africa , though he added that France had not yet come through with the resources .\" relations:[\u2018location- located_in\u2019,\u2018administrative_division-country\u2019, \u2018person-place_lived\u2019, \u2018person-company\u2019, \u2018person-nationality\u2019, \u2018company-founders\u2019, \u2018country-administrative_divisions\u2019, \u2018person- children\u2019, \u2018country-capital\u2019,\u2018deceased_person- place_of_death\u2019,\u2018neighborhood-", "chunk_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eChatGPT\u7684\u96f6\u6837\u672c\u4fe1\u606f\u62bd\u53d6\u6846\u67b6ChatIE\uff0c\u901a\u8fc7\u591a\u8f6e\u95ee\u7b54\u7684\u4ea4\u4e92\u5f62\u5f0f\u5c06\u590d\u6742\u7684\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u5206\u89e3\u6210\u591a\u4e2a\u7b80\u5355\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u5c06\u6bcf\u8f6e\u7684\u7ed3\u679c\u7ec4\u5408\u6210\u6700\u7ec8\u7684\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u8bba\u5143\u62bd\u53d6\u4efb\u52a1\uff0c\u5e76\u5728\u4e24\u79cd\u8bed\u8a00\u7684\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0cChatIE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8868\u73b0\uff0c\u5e76\u751a\u81f3\u8d85\u8fc7\u4e86\u4e00\u4e9b\u5168\u6837\u672c\u6a21\u578b\u3002\u8be5\u5de5\u4f5c\u4e3a\u96f6\u6837\u672c\u4fe1\u606f\u62bd\u53d6\u5f00\u8f9f\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5373\u5c06\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u5206\u89e3\u6210\u591a\u4e2a\u7b80\u5355\u6613\u4e8e\u5904\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u76f4\u63a5\u8fd0\u884c\u8fd9\u4e9b\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002"}, "7": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "5,6,7", "chunk_id": "8", "chunk_text": " Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Matthew R Gormley, Mo Yu, and Mark Dredze. 2015. Improved relation extraction with feature-rich com- positional embedding models. In Proceedings of the 1 Vanilla Prompt Chat-based Prompt STAGE I Question: Suppose you are an entity-relationship triple ex- traction model. I\u2019ll give you list of head entity types: subject_types, list of tail entity types: ob- ject_types, list of relations: relations. Give you a sentence, please extract the subject and object in the sentence based on these three lists, and form a triplet in the form of (subject, relation, object). The given sentence is \"Bono said that President Jacques Chirac of France had spoken eloquently of the need to support Africa , though he added that France had not yet come through with the resources .\" relations:[\u2018location- located_in\u2019,\u2018administrative_division-country\u2019, \u2018person-place_lived\u2019, \u2018person-company\u2019, \u2018person-nationality\u2019, \u2018company-founders\u2019, \u2018country-administrative_divisions\u2019, \u2018person- children\u2019, \u2018country-capital\u2019,\u2018deceased_person- place_of_death\u2019,\u2018neighborhood- neighborhood_of\u2019, \u2018person-place_of_birth\u2019] subject_types: [\u2018organization\u2019, \u2018person\u2019, \u2018loca- tion\u2019, \u2018country\u2019] object_types: [\u2018person\u2019, \u2018location\u2019, \u2018country\u2019, \u2018organization\u2019, \u2018city\u2019] In the given sentence, what triples might be con- tained? Please answer in the form (subject, rela- tion, object): Expected Output: [(Jacques Chirac, person- nationality, France)] Output: [] Question: The given sentence is \" Bono said that President Jacques Chirac of France had spoken eloquently of the need to support Africa , though he added that France had not yet come through with the resources .\" List of given relations: [\u2018location- located_in\u2019,\u2018administrative_division-country\u2019, \u2018person-place_lived\u2019, \u2018person-company\u2019, \u2018person-nationality\u2019, \u2018company-founders\u2019, \u2018country-administrative_divisions\u2019, \u2018person- children\u2019, \u2018country-capital\u2019,\u2018deceased_person- place_of_death\u2019,\u2018neighborhood- neighborhood_of\u2019, \u2018person-place_of_birth\u2019] What relations in the given list might be included in this given sentence? If not present, answer: none. Respond as a tuple, e.g. (relation 1, relation 2, ......): Expected Output: (person-nationality) Output: (person-nationality) STAGE II None Question: According to the given sentence, the two entities are of type (\u2018person\u2019, \u2018country\u2019) and the relation between them is \u2018person-nationality\u2019, \ufb01nd the two entities and list them all by group if there are multiple groups. If not present, answer: none. Respond in the form of a table with two columns and a header of (\u2018person\u2019, \u2018country\u2019): Expected Output: (Jacques Chirac, France) Output: (Jacques Chirac, France) Table 2: Illustration of vanilla prompts vs our Chat-based prompts in terms of RE. The text highlighted with red represents the prompt template. The text following Question: represents the prompt that is used in ChatIE. 1 Vanilla Prompt Chat-based Prompt STAGE I Question: I\u2019m going to give you a sentence and ask you to identify the entities and label the entity category. There will only be 4 types of entities: [\u2018LOC\u2019, \u2018MISC\u2019, \u2018ORG\u2019, \u2018PER\u2019]. Please present your re- sults in list form. \"Japan then laid siege to the Syrian penalty area and had a goal disallowed for offside in the 16th minute.\" Make the list like: [\u2018entity name1\u2019, \u2018entity type1\u2019],[\u2018entity name2\u2019, \u2018entity type2\u2019]...... Expected Output: [\"Japan\", \"LOC\"], [\"Syrian\", \"MISC\"] Output: [] Question: Given sentence: \"Japan then laid siege to the Syrian penalty area and had a goal disallowed for offside in the 16th minute.\" The known en- tity types are: [\u2018LOC\u2019, \u2018MISC\u2019, \u2018ORG\u2019, \u2018PER\u2019]. Please answer: What types of entities are in- cluded in this sentence? Expected Output: LOC, MISC Output: LOC, MISC STAGE II None Question: According to the sentence above, please output the entities of \u2018LOC\u2019 in the form of list like: [\u2018entity name1\u2019, \u2018entity type1\u2019], [\u2018entity name2\u2019, \u2018entity type2\u2019]...... According to the sentence above, please output the entities of \u2018MISC\u2019 in the form of list like: [\u2018entity name1\u2019, \u2018entity type1\u2019], [\u2018entity name2\u2019, \u2018entity type2\u2019]...... Expected Output: [\"Japan\", \"LOC\"], [\"Syrian", "chunk_summary": "\u8fd9\u6bb5\u6587\u5b57\u4e3b\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u804a\u5929\u7684\u4fe1\u606f\u63d0\u53d6\uff08ChatIE\uff09\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728\u8be5\u65b9\u6cd5\u4e2d\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\u548c\u6a21\u677f\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u96f6-shot\u5b66\u4e60\uff0c\u5176\u6a21\u578b\u4ee5\u4eba\u7c7b\u65b9\u5f0f\u4e0e\u7528\u6237\u4ea4\u4e92\u6765\u63d0\u53d6\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u9700\u8981\u6839\u636e\u7ed9\u5b9a\u7684\u5934\u5b9e\u4f53\u7c7b\u578b\u3001\u5c3e\u5b9e\u4f53\u7c7b\u578b\u548c\u5173\u7cfb\u6765\u63d0\u53d6\u53e5\u5b50\u4e2d\u7684\u4e3b\u8bed\u548c\u5bbe\u8bed\uff0c\u5e76\u5f62\u6210\u4e00\u4e2a\u4e09\u5143\u7ec4\u3002\u8be5\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6b63\u5219\u8868\u8fbe\u5f0f\u548c\u89c4\u5219\u7684\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8e\u804a\u5929\u7684\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\u66f4\u52a0\u6709\u6548\u3002"}, "8": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "7,8", "chunk_id": "9", "chunk_text": " disallowed for offside in the 16th minute.\" Make the list like: [\u2018entity name1\u2019, \u2018entity type1\u2019],[\u2018entity name2\u2019, \u2018entity type2\u2019]...... Expected Output: [\"Japan\", \"LOC\"], [\"Syrian\", \"MISC\"] Output: [] Question: Given sentence: \"Japan then laid siege to the Syrian penalty area and had a goal disallowed for offside in the 16th minute.\" The known en- tity types are: [\u2018LOC\u2019, \u2018MISC\u2019, \u2018ORG\u2019, \u2018PER\u2019]. Please answer: What types of entities are in- cluded in this sentence? Expected Output: LOC, MISC Output: LOC, MISC STAGE II None Question: According to the sentence above, please output the entities of \u2018LOC\u2019 in the form of list like: [\u2018entity name1\u2019, \u2018entity type1\u2019], [\u2018entity name2\u2019, \u2018entity type2\u2019]...... According to the sentence above, please output the entities of \u2018MISC\u2019 in the form of list like: [\u2018entity name1\u2019, \u2018entity type1\u2019], [\u2018entity name2\u2019, \u2018entity type2\u2019]...... Expected Output: [\"Japan\", \"LOC\"], [\"Syrian\", \"MISC\"]Output: [\"Japan\", \"LOC\"], [\"Syrian\", \"LOC\"] Table 3: Illustration of vanilla prompts vs our Chat-based prompts in terms of NER. The text highlighted with red represents the prompt template. The text following Question: represents the prompt that is used in ChatIE. 1 Vanilla Prompt Chat-based Prompt STAGE I Question: The list of argument roles corresponding to the event type \u2018Contact:Phone-Write\u2019 is [\u2018En- tity\u2019, \u2018Time\u2019], The list of argument roles corre- sponding to the event type \u2018Business:Declare- Bankruptcy\u2019 is [\u2018Org\u2019, \u2018Time\u2019, \u2018Place\u2019], The list of argument roles corresponding to the event type \u2018Justice:Arrest-Jail\u2019 is [\u2018Person\u2019, \u2018Agent\u2019, \u2018Crime\u2019, \u2018Time\u2019, \u2018Place\u2019], The list of argument roles corresponding to the event type \u2018Life:Die\u2019 is [\u2018Agent\u2019, \u2018Victim\u2019, \u2018Instrument\u2019, \u2018Time\u2019, \u2018Place\u2019], The list of argument roles correspond- ing to the event type \u2018Personnel:Nominate\u2019 is [\u2018Person\u2019, \u2018Agent\u2019, \u2018Position\u2019, \u2018Time\u2019, \u2018Place\u2019], The list of argument roles corresponding to the event type \u2018Con\ufb02ict:Attack\u2019 is [\u2018Attacker\u2019, \u2018Tar- get\u2019, \u2018Instrument\u2019, \u2018Time\u2019, \u2018Place\u2019], The list of argument roles corresponding to the event type \u2018Justice:Sue\u2019 is [\u2018Plaintiff\u2019, \u2018Defendant\u2019, \u2018Adju- dicator\u2019, \u2018Crime\u2019, \u2018Time\u2019, \u2018Place\u2019], The list of argument roles corresponding to the event type \u2018Life:Marry\u2019 is [\u2018Person\u2019, \u2018Time\u2019, \u2018Place\u2019]. Give a sentence:\"What I do know is Saddam Hus- sein has butchered over a million of his own citizens.\", please extract the event arguments ac- cording to the argument roles, and return them in the form of a table.The header of the table is \u2018event type\u2019, \u2018argument role\u2019, \u2018argument con- tent\u2019. If no argument role has a corresponding argument content, the argument content returns \"None\". Expected Output: \"event_type\": \"Life:Die\", \"ar- guments\": [ \"role\": \"Victim\", \"argument\": \"over a million of his own citizens\" , { \"role\": \"Agent\", \"argument\": \"Saddam Hussein\" } Output: None Question: The list of event types: [\u2018Life:Die\u2019, \u2018Justice:Arrest-Jail\u2019, \u2018Contact:Phone-Write\u2019, \u2018Life:Marry\u2019, \u2018Con\ufb02ict:Attack\u2019, \u2018Person- nel:Nominate\u2019, \u2018Business:Declare-Bankruptcy\u2019, \u2018Justice:Sue\u2019] Give a sentence: \"What I do know is Saddam Hussein has butchered over a million of his own citizens.\" What types of events are included in this sen- tence? Please return the most likely answer according to the list of event types above. Require the answer in the form: Event type Expected Output: Life:Die Output: Life:Die STAGE II None Question: The list of argument roles corresponding to the event type \u2018Life: Die\u2019 is [\u2018Agent\u2019, \u2018Victim\u2019, \u2018In- strument\u2019, \u2018Time\u2019, \u2018Place\u2019]. please extract the event arguments in the given sentence according to the argument roles, and return them in the form of a table. The header of the table is \u2018event type\u2019, \u2018argument role\u2019, \u2018argu- ment content\u2019. If no argument role has a corresponding ar- gument content, the argument content returns \"None\". Expected Output: \"arguments\": [ \"role\": \"Vic- tim\", \"argument\": \"over a million", "chunk_summary": "\u8fd9\u4e00\u6bb5\u4ecb\u7ecd\u4e86ChatIE\u6a21\u578b\u5728\u5904\u7406\u4fe1\u606f\u63d0\u53d6\u95ee\u9898\u65f6\u91c7\u7528\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u8fdb\u884c\u96f6\u6837\u672c\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u6a21\u578b\u901a\u8fc7\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u672a\u77e5\u5b9e\u4f53\u548c\u5173\u7cfb\u7684\u51c6\u786e\u63d0\u53d6\u3002\u8be5\u6a21\u578b\u7684\u5e94\u7528\u573a\u666f\u5305\u62ec\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u7b49\u591a\u4e2a\u65b9\u9762\u3002\u6587\u7ae0\u8fd8\u6bd4\u8f83\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u91c7\u7528\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u7684\u65b9\u6cd5\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u65b9\u9762\u7684\u8868\u73b0\u3002\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9a8c\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u7684\u65b9\u6cd5\u5728\u63d0\u5347\u4fe1\u606f\u63d0\u53d6\u6548\u679c\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}, "9": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "8,9", "chunk_id": "10", "chunk_text": "Die\u2019, \u2018Justice:Arrest-Jail\u2019, \u2018Contact:Phone-Write\u2019, \u2018Life:Marry\u2019, \u2018Con\ufb02ict:Attack\u2019, \u2018Person- nel:Nominate\u2019, \u2018Business:Declare-Bankruptcy\u2019, \u2018Justice:Sue\u2019] Give a sentence: \"What I do know is Saddam Hussein has butchered over a million of his own citizens.\" What types of events are included in this sen- tence? Please return the most likely answer according to the list of event types above. Require the answer in the form: Event type Expected Output: Life:Die Output: Life:Die STAGE II None Question: The list of argument roles corresponding to the event type \u2018Life: Die\u2019 is [\u2018Agent\u2019, \u2018Victim\u2019, \u2018In- strument\u2019, \u2018Time\u2019, \u2018Place\u2019]. please extract the event arguments in the given sentence according to the argument roles, and return them in the form of a table. The header of the table is \u2018event type\u2019, \u2018argument role\u2019, \u2018argu- ment content\u2019. If no argument role has a corresponding ar- gument content, the argument content returns \"None\". Expected Output: \"arguments\": [ \"role\": \"Vic- tim\", \"argument\": \"over a million of his own citizens\" , { \"role\": \"Agent\", \"argument\": \"Sad- dam Hussein\" } Output: \"arguments\": [ \"role\": \"Victim\", \"argument\": \"over a million of his own citizens\" , { \"role\": \"Agent\", \"argument\": \"Saddam Hussein\" } Table 4: Illustration of vanilla prompts vs our Chat-based prompts in terms of EE. The text highlighted with red represents the prompt template. The text following Question: represents the prompt that is used in ChatIE. 2015 Conference on Empirical Methods in Natural Language Processing, pages 1774\u20131784. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arxiv:2301.07597. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classi\ufb01ca- tion dataset with state-of-the-art evaluation. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 4803\u2013 4809, Brussels, Belgium. Association for Computa- tional Linguistics. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge- based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 541\u2013550. Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa St\u00fcber, Jo- hanna Topalis, Tobias Weber, Philipp Wesp, Bas- tian Sabel, Jens Ricke, et al. 2022. Chatgpt makes medicine easy to swallow: An exploratory case study on simpli\ufb01ed radiology reports. arXiv preprint arXiv:2212.14882. Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745. Michael R King. 2022. The future of ai in medicine: a perspective from a chatbot. Annals of Biomedical Engineering, pages 1\u20135. Gina-Anne Levow. 2006. The third international Chi- nese language processing bakeoff: Word segmen- tation and named entity recognition. In Proceed- ings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108\u2013117, Sydney, Aus- tralia. Association for Computational Linguistics. Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang, Lu Pan, Yajuan Lyu, and Yong Zhu. 2020a. Event extraction as multi-turn question answering. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2020, pages 829\u2013838. Shuangjie Li, Wei He, Yabing Shi, Wenbin Jiang, Hai- jin Liang, Ye Jiang, Yang Zhang, Yajuan Lyu, and Yong Zhu. 2019a. Duie: A large-scale chinese dataset for information extraction. In C", "chunk_summary": "\u672c\u6bb5\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\uff0c\u5e76\u89e3\u91ca\u4e86\u5b83\u5982\u4f55\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u6765\u63d0\u53d6\u4e8b\u4ef6\u548c\u4e8b\u4ef6\u53c2\u6570\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u4f7f\u7528ChatIE\u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u8f93\u5165\u4e00\u4e2a\u5173\u4e8e\u7279\u5b9a\u4e8b\u4ef6\u7684\u53e5\u5b50\uff0c\u5e76\u6839\u636e\u7cfb\u7edf\u8fd4\u56de\u7684\u95ee\u9898\u8fdb\u4e00\u6b65\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\u3002\u8be5\u7cfb\u7edf\u8fd8\u652f\u6301\u6839\u636e\u7279\u5b9a\u4e8b\u4ef6\u89d2\u8272\u63d0\u53d6\u4e8b\u4ef6\u53c2\u6570\uff0c\u5e76\u5c06\u7ed3\u679c\u4ee5\u8868\u683c\u5f62\u5f0f\u8fd4\u56de\u3002\u6b64\u5916\uff0c\u672c\u6bb5\u8fd8\u5217\u4e3e\u4e86\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u3002"}, "10": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "9,10", "chunk_id": "11", "chunk_text": "? a preliminary study. arXiv preprint arXiv:2301.08745. Michael R King. 2022. The future of ai in medicine: a perspective from a chatbot. Annals of Biomedical Engineering, pages 1\u20135. Gina-Anne Levow. 2006. The third international Chi- nese language processing bakeoff: Word segmen- tation and named entity recognition. In Proceed- ings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108\u2013117, Sydney, Aus- tralia. Association for Computational Linguistics. Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang, Lu Pan, Yajuan Lyu, and Yong Zhu. 2020a. Event extraction as multi-turn question answering. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2020, pages 829\u2013838. Shuangjie Li, Wei He, Yabing Shi, Wenbin Jiang, Hai- jin Liang, Ye Jiang, Yang Zhang, Yajuan Lyu, and Yong Zhu. 2019a. Duie: A large-scale chinese dataset for information extraction. In CCF Interna- tional Conference on Natural Language Processing and Chinese Computing, pages 791\u2013800. Springer. Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019b. Entity-relation extraction as multi-turn question an- swering. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 1340\u20131350. Xinyu Li, Fayuan Li, Lu Pan, Yuguang Chen, Wei- hua Peng, Quan Wang, Yajuan Lyu, and Yong Zhu. 2020b. Duee: a large-scale dataset for chinese event extraction in real-world scenarios. In CCF Interna- tional Conference on Natural Language Processing and Chinese Computing, pages 534\u2013545. Springer. Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi Chen. 2021. Text2event: Controllable sequence-to- structure generation for end-to-end event extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 2795\u20132806. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow in- structions with human feedback. arXiv preprint arXiv:2203.02155. Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the thirteenth conference on compu- tational natural language learning (CoNLL-2009), pages 147\u2013155. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions with- out labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 148\u2013163. Springer. Oscar Sainz, Itziar Gonzalez-Dios, Oier Lopez de La- calle, Bonan Min, and Eneko Agirre. 2022a. Textual entailment for event argument extraction: Zero- and few-shot with multi-source learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2439\u20132455, Seattle, United States. Association for Computational Linguistics. Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, Ander Barrena, and Eneko Agirre. 2021. Label verbalization and entailment for effective zero and few-shot relation extraction. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1199\u20131212, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Oscar Sainz, Haoling Qiu, Oier Lopez de Lacalle, Eneko Agirre, and Bonan Min. 2022b. ZS4IE: A toolkit for zero-shot information extraction with sim- ple verbalizations. In Proceedings of the 2022 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies: System Demonstrations, pages 27\u201338, Hybrid: Seattle, Washington +", "chunk_summary": "\u672c\u6bb5\u843d\u5217\u4e3e\u4e86\u4e00\u7cfb\u5217\u5173\u4e8e\u4fe1\u606f\u62bd\u53d6\u7684\u7814\u7a76\u8bba\u6587\uff0c\u5305\u62ec\u4e8b\u4ef6\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u7cfb\u62bd\u53d6\u7b49\u65b9\u9762\u3002\u5176\u4e2d\uff0c\u6709\u4e00\u4e9b\u8bba\u6587\u63a2\u8ba8\u4e86\u96f6\u6837\u672c\u5b66\u4e60\uff08zero-shot learning\uff09\u7684\u5e94\u7528\uff0c\u4e5f\u6709\u4e00\u4e9b\u8bba\u6587\u63d0\u51fa\u4e86\u5de5\u5177\u7bb1\u6216\u5de5\u5177\uff0c\u5982ZS4IE\uff0c\u53ef\u4ee5\u5b9e\u73b0\u96f6-shot\u4fe1\u606f\u62bd\u53d6\u3002"}, "11": {"source": "Zero-Shot Information Extraction via Chatting with ChatGPT.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatIE\u7684\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u4e3b\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u8bbe\u8ba1\u7684\u6a21\u677f\u548cChatGPT\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6846\u67b6\uff0c\u6700\u7ec8\u5c06\u6bcf\u8f6e\u7ed3\u679c\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u7ed3\u679c\u3002\u4f5c\u8005\u5728\u5173\u7cfb\u62bd\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5168\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u4e2a\u6846\u67b6\u5f00\u8f9f\u4e86\u96f6\u6837\u672c\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c\u5177\u4f53\u89c4\u683c\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\u548c\u5fae\u8c03\u3002\u6587\u7ae0\u8fd8\u6d89\u53ca\u5230\u4e00\u4e9b\u76f8\u5173\u7814\u7a76\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u9886\u57df\u3002\u8fd9\u662f\u4e00\u4e2a\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u6709\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002", "source_page_nums": "10", "source_chunk_nums": "12", "semantic_tags": "", "regular_tags": "", "page_span": "9", "chunk_id": "12", "chunk_text": ". 2022a. Textual entailment for event argument extraction: Zero- and few-shot with multi-source learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2439\u20132455, Seattle, United States. Association for Computational Linguistics. Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, Ander Barrena, and Eneko Agirre. 2021. Label verbalization and entailment for effective zero and few-shot relation extraction. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1199\u20131212, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Oscar Sainz, Haoling Qiu, Oier Lopez de Lacalle, Eneko Agirre, and Bonan Min. 2022b. ZS4IE: A toolkit for zero-shot information extraction with sim- ple verbalizations. In Proceedings of the 2022 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies: System Demonstrations, pages 27\u201338, Hybrid: Seattle, Washington + Online. Asso- ciation for Computational Linguistics. Sunita Sarawagi et al. 2008. Information extraction. Foundations and Trends\u00ae in Databases, 1(3):261\u2013 377. Teo Susnjak. 2022. Chatgpt: The end of online exam integrity? arXiv preprint arXiv:2212.09292. Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, and Minlie Huang. 2019. A hierarchical framework for relation extraction with reinforcement learning. In Proceedings of the AAAI conference on arti\ufb01cial in- telligence, volume 33, pages 7072\u20137079. Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002). Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natu- ral Language Learning at HLT-NAACL 2003, pages 142\u2013147. Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Ji- acheng Liu, and Jiawei Han. 2019. Crossweigh: Training named entity tagger from imperfect anno- tations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5154\u20135163. Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A novel cascade binary tagging framework for relational triple extraction. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 1476\u2013 1488. Bowen Zhang, Daijun Ding, and Liwen Jing. 2022. How would stance detection techniques evolve af- ter the launch of chatgpt? arXiv preprint arXiv:2212.14548. Hengyi Zheng, Rui Wen, Xi Chen, Yifan Yang, Yun- yan Zhang, Ziheng Zhang, Ningyu Zhang, Bin Qin, Xu Ming, and Yefeng Zheng. 2021. Prgc: Poten- tial relation and global correspondence based joint relational triple extraction. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 6225\u20136235. Zexuan Zhong and Danqi Chen. 2021. A frustratingly easy approach for entity and relation extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 50\u201361. ", "chunk_summary": "\u672c\u6bb5\u4e3b\u8981\u5217\u4e3e\u4e86\u4e0e\u96f6\u6837\u672c\u4fe1\u606f\u62bd\u53d6\u76f8\u5173\u7684\u8bba\u6587\u548c\u5de5\u5177\u5305\uff0c\u6d89\u53ca\u5230\u7684\u6280\u672f\u5305\u62ec\u6587\u672c\u8574\u6db5\u3001\u6807\u7b7e\u8bcd\u6c47\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u3002\u540c\u65f6\u8fd8\u5305\u62ec\u4e86\u4e0e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u76f8\u5173\u7684\u8bba\u6587\u3002\u8fd9\u4e2a\u9886\u57df\u4ecd\u5728\u4e0d\u65ad\u53d1\u5c55\uff0c\u5e76\u4e14\u6709\u7740\u8bb8\u591a\u4ee4\u4eba\u632f\u594b\u7684\u6210\u679c\u3002"}, "12": {"source": "How will Language Modelers like ChatGPT Affect.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002\n\n\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\n\n\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u5f88\u5927\u7684\u53c2\u8003\u548c\u501f\u9274\u4ef7\u503c\u3002\n\n\u672c\u6587\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002", "source_page_nums": "12", "source_chunk_nums": "6", "semantic_tags": "", "regular_tags": "", "page_span": "1,2,3", "chunk_id": "1", "chunk_text": "1        How will Language Modelers like ChatGPT Affect   Occupations and Industries?      Ed Felten (Princeton)  Manav Raj (University of Pennsylvania)  Robert Seamans (New York University)      1 March 2023      Abstract: Recent dramatic increases in AI language modeling capabilities has led to many  questions about the effect of these technologies on the economy. In this paper we present a  methodology to systematically assess the extent to which occupations, industries and  geographies are exposed to advances in AI language modeling capabilities. We find that the top  occupations exposed to language modeling include telemarketers and a variety of post-secondary  teachers such as English language and literature, foreign language and literature, and history  teachers. We find the top industries exposed to advances in language modeling are legal services  and securities, commodities, and investments.        Keywords: artificial intelligence, ChatGPT, language modeling, occupation, technology        2    1. Introduction    Artificial Intelligence (AI) will likely affect the economy in many ways, potentially boosting  economic growth and changing the way people work and play. The effect of AI on work will likely  be multi-faceted. In some cases, AI may substitute for work previously done by humans, and in  other cases AI may complement work done by humans. The effect on work will likely also vary  across industries. Recent research by Goldfarb et al (2020) document that adoption of AI is  relatively high in some industries such as information technology and finance, but low in others  such as health care and construction. Moreover, trying to understand how AI will affect work is  like trying to hit a moving target because the capabilities of AI are still advancing.    A prominent example of how AI capabilities continue to advance are the recent improvements in  AI language modeling. In particular, ChatGPT, a language modeler released by Open AI in late  2022, has garnered a huge amount of attention and controversy. Some worry about the negative  effects of tools like ChatGPT on jobs, as in the New York Post article headlined \u201cChatGPT could  make these jobs obsolete: \u2018The wolf is at the door.\u2019\u201d1 Others see practical and commercial promise  from language modeling. For example, Microsoft announced a $10 billion partnership with Open  AI and has linked ChatGPT with its Bing search engine.2 Google felt compelled to demonstrate  its own language modeler, Bard, but mistakes during the demonstration led Google\u2019s stock price  to drop 7%.3 ChatGPT has been banned by J.P. Morgan.4 However, at present, most of this is  speculation.    In order to better understand how language modelers such as ChatGPT will affect occupations,  industries and geographies, we use a methodology developed by Felten et al (2018, 2021). Felten  et al created the AI Occupational Exposure (AIOE) measure and used this measure to identify  which occupations, industries and geographies are most exposed to AI. In this paper, we describe  how the AIOE approach can be adapted to account for the recent advancement of language  modeling.       1 https://nypost.com/2023/01/25/chat-gpt-could-make-these-jobs-obsolete/   2 https://www.bloomberg.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-investment-in- openai   3 https://www.cnbc.com/2023/02/08/alphabet-shares-slip-following-googles-ai-event-.html   4 https://www.cbsnews.com/news/chatgpt-jpmorgan-chase-bars-workers-from-using-ai-tool/   3    We find that the top occupations affected include telemarketers and a variety of post-secondary  teachers such as English language and literature, foreign language and literature, and history  teachers. We also find the top industries exposed to advances in language modeling are legal  services and securities, commodities, and investments.    This article contributes to several literatures. First, by providing a systematic examination of the  effect of language modeling across occupations, industries and geographies, it contributes to a  nascent literature on the effects of ChatGPT and other language modelers on the economy (e.g.  Agarwal et al., 2022; Zarifhonarvar, 2023). More generally, the article builds on a broader set of  literature studying the effect of AI on the economy (Furman and Seamans, 2019; Goldfarb et al.,  2019). Second, the article builds on and extends a set of papers that provide systematic  methodologies for studying how AI affects occupations (e.g., Brynjolfsson et al, 2018; Frey &  Osborne, 2017; Tolan et al., 2021; Webb, 2020). The article specifically", "chunk_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002"}, "13": {"source": "How will Language Modelers like ChatGPT Affect.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002\n\n\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\n\n\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u5f88\u5927\u7684\u53c2\u8003\u548c\u501f\u9274\u4ef7\u503c\u3002\n\n\u672c\u6587\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002", "source_page_nums": "12", "source_chunk_nums": "6", "semantic_tags": "", "regular_tags": "", "page_span": "2,3,4", "chunk_id": "2", "chunk_text": "ai-tool/   3    We find that the top occupations affected include telemarketers and a variety of post-secondary  teachers such as English language and literature, foreign language and literature, and history  teachers. We also find the top industries exposed to advances in language modeling are legal  services and securities, commodities, and investments.    This article contributes to several literatures. First, by providing a systematic examination of the  effect of language modeling across occupations, industries and geographies, it contributes to a  nascent literature on the effects of ChatGPT and other language modelers on the economy (e.g.  Agarwal et al., 2022; Zarifhonarvar, 2023). More generally, the article builds on a broader set of  literature studying the effect of AI on the economy (Furman and Seamans, 2019; Goldfarb et al.,  2019). Second, the article builds on and extends a set of papers that provide systematic  methodologies for studying how AI affects occupations (e.g., Brynjolfsson et al, 2018; Frey &  Osborne, 2017; Tolan et al., 2021; Webb, 2020). The article specifically builds off and extends  the methodology described in Felten et al. (2018, 2021). In so doing, the article demonstrates the  flexibility of the original Felten et al methodology; it can be adjusted dynamically to assess the  impact of changes in AI capabilities. Finally, the article adds to a large literature on the effect of  automating technologies on labor (e.g., Acemoglu et al., 2022; Autor, 2015; Frank et al., 2019;  Genz et al., 2021).     The article proceeds as follows. Section 2 describes the AI Occupational Exposure (AIOE)  measure developed by Felten et al (2018, 2021). Section 3 extends the AIOE to account for recent  advances in language modeling. Section 4 provides results, including listing the top 20 most  affected occupations and industries. Section 5 concludes.    2. AI Occupational Exposure Methodology    According to Felten et al (2021), the AI Occupational Exposure (AIOE) is a measure of each  occupation\u2019s \u201cexposure\u201d to AI. The term \u201cexposure\u201d is used so as to be agnostic as to the effects  of AI on the occupation, which could involve substitution or augmentation depending on various  factors associated with the occupation itself.     The AIOE measure was constructed by linking 10 AI applications (abstract strategy games, real- time video games, image recognition, visual question answering, image generation, reading  comprehension, language modeling, translation, speech recognition, and instrumental track  4    recognition) to 52 human abilities (e.g., oral comprehension, oral expression, inductive reasoning,  arm-hand steadiness, etc) using a crowd-sourced matrix that indicates the level of relatedness  between each AI application and human ability. Data on the AI applications come from the  Electronic Frontier Foundation (EFF) which collects and maintains statistics about the progress  of AI across multiple applications. Data on human abilities comes from the Occupational  Information Network (O*NET) database developed by the United States Department of Labor.  O*NET uses these 52 human abilities to describe the occupational makeup of each of 800+  occupations that it tracks. Each of 800+ occupations can be thought of as a weighted combination  of the 52 human abilities. O*NET uses two sets of weights: prevalence and importance.     Once the 10 AI categories and 52 human abilities are linked through the matrix, the AIOE can  then be calculated for each occupation. To do this, first we calculate an ability-level exposure as  follows:    \ud835\udc34\ud835\udc56\ud835\udc57 = \u221110 \ud835\udc56=1 \ud835\udc65\ud835\udc56\ud835\udc57                  (1)    Where i indexes the AI application and j indexes the occupational ability. The ability-level  exposure, A, is calculated as the sum of the 10 application-ability relatedness scores, x, as  constructed using the matrix of crowd-sourced survey data.     We then calculate the AIOE for each occupation k as follows:     \ud835\udc34\ud835\udc3c\ud835\udc42\ud835\udc38\ud835\udc58 = \u221152 \ud835\udc57=1 \ud835\udc34\ud835\udc56\ud835\udc57\u00d7\ud835\udc3f\ud835\udc57\ud835\udc58\u00d7\ud835\udc3c\ud835\udc57\ud835\udc58 \u221152 \ud835\udc57=1 \ud835\udc3f\ud835\udc57\ud835\udc58\u00d7\ud835\udc3c\ud835\udc57\ud835\udc58                 (2)    In this equation, i indexes the AI application, j indexes the occupational ability, and k indexes the  occupation. Aij represents the ability-level exposure score. We weight the", "chunk_summary": "\u8fd9\u7bc7\u6587\u7ae0\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u672c\u6587\u5bf9\u73b0\u6709\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u6b64\u5916\uff0c\u8be5\u6587\u8fd8\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u7ecf\u6d4e\u3001AI \u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u4ee5\u53ca\u81ea\u52a8\u5316\u6280\u672f\u5bf9\u52b3\u52a8\u529b\u7684\u5f71\u54cd\u7b49\u65b9\u9762\u7684\u95ee\u9898\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\u6839\u636e\u516c\u5f0f (1) \u548c (2)\uff0c\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u804c\u4e1a\u7684 AIOE\u3002"}, "14": {"source": "How will Language Modelers like ChatGPT Affect.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002\n\n\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\n\n\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u5f88\u5927\u7684\u53c2\u8003\u548c\u501f\u9274\u4ef7\u503c\u3002\n\n\u672c\u6587\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002", "source_page_nums": "12", "source_chunk_nums": "6", "semantic_tags": "", "regular_tags": "", "page_span": "4,5,6", "chunk_id": "3", "chunk_text": " abilities are linked through the matrix, the AIOE can  then be calculated for each occupation. To do this, first we calculate an ability-level exposure as  follows:    \ud835\udc34\ud835\udc56\ud835\udc57 = \u221110 \ud835\udc56=1 \ud835\udc65\ud835\udc56\ud835\udc57                  (1)    Where i indexes the AI application and j indexes the occupational ability. The ability-level  exposure, A, is calculated as the sum of the 10 application-ability relatedness scores, x, as  constructed using the matrix of crowd-sourced survey data.     We then calculate the AIOE for each occupation k as follows:     \ud835\udc34\ud835\udc3c\ud835\udc42\ud835\udc38\ud835\udc58 = \u221152 \ud835\udc57=1 \ud835\udc34\ud835\udc56\ud835\udc57\u00d7\ud835\udc3f\ud835\udc57\ud835\udc58\u00d7\ud835\udc3c\ud835\udc57\ud835\udc58 \u221152 \ud835\udc57=1 \ud835\udc3f\ud835\udc57\ud835\udc58\u00d7\ud835\udc3c\ud835\udc57\ud835\udc58                 (2)    In this equation, i indexes the AI application, j indexes the occupational ability, and k indexes the  occupation. Aij represents the ability-level exposure score. We weight the ability-level AI exposure  by the ability's prevalence (Ljk) and importance (Ijk) within each occupation as measured by  O*NET by multiplying the ability-level AI exposure by the prevalence and importance scores for  that ability within each occupation, scaled so that they are equally weighted.    Felten et al (2021) explain the construction of the AIOE scores in more detail, describe how they  can be weighted at the industry level to construct an AI Industry Exposure score, or weighted at  the geographic level to construct an AI Geographic Exposure score. They also provide results  5    from a number of validation exercises and describe a number of ways in which the scores can be  used by scholars and practitioners.5    3. Language Modeling AI Occupational Exposure    The original AIOE described in Felten et al (2021) explicitly weighted each of the AI applications  the same. In order to update the AI Occupational Exposure score to account for advances in  Language Modeling we modify equation (1) as follows.     \ud835\udc34\ud835\udc56\ud835\udc57 = \u221110 \ud835\udc56=1 \ud835\udefc\ud835\udc56\ud835\udc65\ud835\udc56\ud835\udc57                  (3)    Where i indexes the AI application and j indexes the occupational ability. The ability-level  exposure, A, is calculated as the weighted sum of the 10 application-ability relatedness scores,  x, as constructed using the matrix of crowd-sourced survey data. \ud835\udefc\ud835\udc56 is the weight placed on each  application i. The weights used in Felten et al (2021) set \ud835\udefc\ud835\udc56 equal to 1 for each application i.     Next, we set \ud835\udefc\ud835\udc56 equal to 0 for every AI application except for language modeling, which retains a  weight of 1. This then constructs an ability-level exposure measure that only \u201ccounts\u201d the value of  abilities that are related to language modeling. We then proceed to calculate the \ud835\udc34\ud835\udc3c\ud835\udc42\ud835\udc38\ud835\udc58 for each  occupation k using this new \u201clanguage modeling\u201d weighted \ud835\udc34\ud835\udc56\ud835\udc57. The resulting \ud835\udc34\ud835\udc3c\ud835\udc42\ud835\udc38\ud835\udc58 therefore  captures the extent to which each occupation is exposed to advances in language modeling due  to AI. A complete list of the occupations and their resulting AIOE language modeling score are  listed in an appendix.     The resulting scores are highly correlated with the original AIOE scores (correlation coefficient:  0.979). This can be seen in Figure 1 which plots the original AIOE score and the new language  modeling adjusted AIOE score for each occupation.     << Insert Figure 1 Here >>    4. Results    5 The Felten et al (2021) paper is open access and available here:  https://onlinelibrary.wiley.com/doi/full/10.1002/smj.3286 The data and code used to create the AIOE  scores described in Felten et al (2021) is available on GitHub: https://github.com/AIOE-Data/AIOE   6      In this section we present and briefly discuss tables of \u201ctop 20\u201d occupations and industries  exposed to language modeling.    4.1. Top 20 Occupations Exposed to Language Modeling    Table 1 provides the list of top 20 occupations exposed to AI based on the original Felten et al  (2021) AI Occupational Exposure (AIOE) measure as well as the top 20 occupations exposed to  AI enabled advances in language modeling capabilities.    << Insert Table 1 Here >>   ", "chunk_summary": "\u672c\u6bb5\u843d\u4ecb\u7ecd\u4e86\u5982\u4f55\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5bf9AI\u804c\u4e1a\u66b4\u9732\u8fdb\u884c\u4fee\u6539\uff0c\u4ece\u800c\u5f97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51faAIOE\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002"}, "15": {"source": "How will Language Modelers like ChatGPT Affect.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002\n\n\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\n\n\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u5f88\u5927\u7684\u53c2\u8003\u548c\u501f\u9274\u4ef7\u503c\u3002\n\n\u672c\u6587\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002", "source_page_nums": "12", "source_chunk_nums": "6", "semantic_tags": "", "regular_tags": "", "page_span": "5,6,7,8", "chunk_id": "4", "chunk_text": " are highly correlated with the original AIOE scores (correlation coefficient:  0.979). This can be seen in Figure 1 which plots the original AIOE score and the new language  modeling adjusted AIOE score for each occupation.     << Insert Figure 1 Here >>    4. Results    5 The Felten et al (2021) paper is open access and available here:  https://onlinelibrary.wiley.com/doi/full/10.1002/smj.3286 The data and code used to create the AIOE  scores described in Felten et al (2021) is available on GitHub: https://github.com/AIOE-Data/AIOE   6      In this section we present and briefly discuss tables of \u201ctop 20\u201d occupations and industries  exposed to language modeling.    4.1. Top 20 Occupations Exposed to Language Modeling    Table 1 provides the list of top 20 occupations exposed to AI based on the original Felten et al  (2021) AI Occupational Exposure (AIOE) measure as well as the top 20 occupations exposed to  AI enabled advances in language modeling capabilities.    << Insert Table 1 Here >>    Some occupations occur in both lists, including \u201cclinical, counseling, and school psychologists\u201d,   and \u201chistory teachers, postsecondary\u201d. Notably, the language modeling list includes more  education-related occupations, indicating that occupations in the field of education are likely to be  relatively more impacted by advances in language modeling than other occupations. This accords  well with the recent spate of articles around how ChatGPT and other language modeling tools  affect the way teachers assign work and detect cheating or could use language modeling tools to  develop teaching materials.     Also of interest, the top occupation in the language modeling list is \u201ctelemarketer.\u201d One might  imagine that human telemarketers could benefit from language modeling being used to augment  their work. For example, customer responses can be fed into a language modeling engine in real  time and relevant, customer-specific prompts quickly fed to the telemarketer. Or, one might  imagine that human telemarketers are substituted with language modeling enabled bots. The  potential for language modeling to augment or substitute for human telemarketers work highlights  one aspect of the AIOE measure: it measures \u201cexposure\u201d to AI, but whether that exposure leads  to augmentation or substitution will depend on specifics of any given occupation.    4.2. Top 20 Industries Exposed to Language Modeling    Table 2 provides the list of 20 industries most exposed to AI based on the original Felten et al.  (2021) AI Industry Exposure (AIIE) measure as well as the top 20 industries exposed to AI enabled  advances in language modeling capabilities.  7      << Insert Table 2 Here >>    As before, we see some similarities in the industries categorized as most exposed to AI based on  the original AIOE as well as the version that focuses on advances in language modeling  capabilities. For example, \u201cSecurities, Commodity Contracts, and Other Financial Investments  and Related Activities\u201d is categorized as the most exposed industry using the original AIOE and  is the second most exposed industry using the language modeling-focused version of the AIOE.  Legal services, insurance and employee benefit funds, and agencies, brokerages, and other  insurance related activities are among the top five most exposed industries across both lists.    However, some differences emerge. One salient difference is that the language modeling-focused  AIOE suggests a higher exposure to advances in AI within higher education and higher education- adjacent industries. Junior colleges, grantmaking and giving services, and business schools and  computer and management training all appear within the top twenty exposed industries.     5. Conclusion    In this paper we present a methodology to systematically assess the extent to which occupations  and industries are exposed to advances in AI language modeling capabilities. This methodology  relies on the approach described in Felten et al (2021) but adapts it to account for recent advances  in language modeling. We find that the top occupations exposed to language modeling include  telemarketers and a variety of post-secondary teachers such as English language and literature,  foreign language and literature, and history teachers. We also find the top industries exposed to  advances in language modeling are legal services and securities, commodities, and investments.     At a broad level, this paper adds to a growing literature studying the effects of AI on labor and  work. More specifically, the paper provides a systematic approach for understanding how  ChatGPT and other language modelers will affect occupations, industries and geographies. We  believe these results will be useful for other scholars as well as practitioners and policymakers.        8    References    Acemoglu, D., Autor, D., Hazell, J., & Restrepo, P. (2022). Artificial intelligence", "chunk_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u804c\u4e1a\u548c\u884c\u4e1a\u66b4\u9732\u4e8e\u4eba\u5de5\u667a\u80fd\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8eFelten\u7b49\u4eba\uff082021\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u9002\u5e94\u4ee5\u8003\u8651\u6700\u8fd1\u7684\u8bed\u8a00\u5efa\u6a21\u8fdb\u5c55\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u7684\u524d20\u79cd\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u548c\u5404\u79cd\u9ad8\u7b49\u6559\u80b2\u6559\u5e08\uff0c\u4f8b\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u3001\u5386\u53f2\u6559\u5e08\u7b49\uff1b\u800c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u7684\u524d20\u4e2a\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u7b49\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5de5\u5177\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u5f71\u54cd\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5bf9\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u6709\u7528\u7684\u7ed3\u679c\u3002"}, "16": {"source": "How will Language Modelers like ChatGPT Affect.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002\n\n\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\n\n\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u5f88\u5927\u7684\u53c2\u8003\u548c\u501f\u9274\u4ef7\u503c\u3002\n\n\u672c\u6587\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002", "source_page_nums": "12", "source_chunk_nums": "6", "semantic_tags": "", "regular_tags": "", "page_span": "7,8,9", "chunk_id": "5", "chunk_text": " and giving services, and business schools and  computer and management training all appear within the top twenty exposed industries.     5. Conclusion    In this paper we present a methodology to systematically assess the extent to which occupations  and industries are exposed to advances in AI language modeling capabilities. This methodology  relies on the approach described in Felten et al (2021) but adapts it to account for recent advances  in language modeling. We find that the top occupations exposed to language modeling include  telemarketers and a variety of post-secondary teachers such as English language and literature,  foreign language and literature, and history teachers. We also find the top industries exposed to  advances in language modeling are legal services and securities, commodities, and investments.     At a broad level, this paper adds to a growing literature studying the effects of AI on labor and  work. More specifically, the paper provides a systematic approach for understanding how  ChatGPT and other language modelers will affect occupations, industries and geographies. We  believe these results will be useful for other scholars as well as practitioners and policymakers.        8    References    Acemoglu, D., Autor, D., Hazell, J., & Restrepo, P. (2022). Artificial intelligence and jobs:  Evidence from online vacancies. Journal of Labor Economics, 40(S1), S293-S340.    Agrawal, A., Gans, J., Goldfarb, A. 2022. \u201cChatGPT and How AI Disrupts Industries\u201d Harvard  Business Review. Available: https://hbr.org/2022/12/chatgpt-and-how-ai-disrupts-industries     Autor, D. (2015). Why Are There Still So Many Jobs? The History and Future of Workplace  Automation. Journal of Economic Perspectives, 29(3), 3\u201330.    Brynjolfsson, E., Mitchell, T., & Rock, D. (2018). What can machines learn, and what does it  mean for occupations and the economy? AEA Papers and Proceedings, 108, 43\u201347.  https://doi.org/10.1257/pandp.20181019    Felten, E. W., Raj, M., & Seamans, R. (2018). A method to link advances in artificial intelligence  to occupational abilities. In AEA Papers and Proceedings (Vol. 108, pp. 54-57).    Felten, E., Raj, M., & Seamans, R. (2021). Occupational, industry, and geographic exposure to  artificial intelligence: A novel dataset and its potential uses. Strategic Management Journal,  42(12), 2195-2217.    Frank, M. R., Autor, D., Bessen, J. E., Brynjolfsson, E., Cebrian, M., Deming, D. J., ... &  Rahwan, I. (2019). Toward understanding the impact of artificial intelligence on labor.  Proceedings of the National Academy of Sciences, 116(14), 6531-6539.    Frey, C. B., & Osborne, M. A. (2017). The future of employment: How susceptible are jobs to  computerisation? Technological Forecasting and Social Change, 114, 254\u2013280.  https://doi.org/10.1016/j.techfore.2016.08.019    Furman, J., & Seamans, R. (2019). AI and the Economy. Innovation policy and the economy,  19(1), 161-191.    9    Genz, S., Gregory, T., Janser, M., Lehmer, F., & Matthes, B. (2021). How do workers adjust  when firms adopt new technologies?. ZEW-Centre for European Economic Research  Discussion Paper, (21-073).    Goldfarb, A., Gans, J., & Agrawal, A. (2019). The economics of artificial intelligence: An agenda.  Chicago, IL: University of Chicago Press.    Goldfarb, A., Taska, B. Teodoridis, F. (2020). \u201cArtificial Intelligence in Healthcare? Evidence  from Online Job Postings\u201d, AEA Papers and Proceedings, 110 (5): 400-404    Tolan, S., Pesole, A., Mart\u00ednez-Plumed, F., Fern\u00e1ndez-Mac\u00edas, E., Hern\u00e1ndez-Orallo, J., &  G\u00f3mez, E. (2021). Measuring the occupational impact of AI: tasks, cognitive abilities and AI  benchmarks. Journal of Artificial Intelligence Research, 71, 191-236.    Webb, M. (2020). The impact of artificial intelligence on the labor market. Stanford University  working paper.    Zarifhonarvar, A. 202", "chunk_summary": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5de5\u4f5c\u5c97\u4f4d\u65b9\u9762\uff0c\u6700\u5bb9\u6613\u53d7\u5230\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u7c7b\u9ad8\u7b49\u6559\u80b2\u5e08\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5386\u53f2\u5b66\u6559\u5e08\u7b49\uff1b\u884c\u4e1a\u65b9\u9762\uff0c\u6700\u5bb9\u6613\u53d7\u5230\u5f71\u54cd\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u7406\u89e3ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u8005\u5c06\u5982\u4f55\u5f71\u54cd\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\uff0c\u5728\u5b66\u672f\u548c\u5b9e\u8df5\u65b9\u9762\u5177\u6709\u91cd\u8981\u53c2\u8003\u548c\u501f\u9274\u610f\u4e49\u3002"}, "17": {"source": "How will Language Modelers like ChatGPT Affect.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5bf9\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u4f4d\u7f6e\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66b4\u9732\u4e8e\u8bed\u8a00\u5efa\u6a21\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u548c\u5404\u79cd\u540e\u671f\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5982\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u56fd\u8bed\u8a00\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u6559\u5e08\u3002\u800c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5219\u662f\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u4e0e\u6295\u8d44\u3002\u6b64\u7814\u7a76\u4e3a\u7814\u7a76ChatGPT\u548c\u5176\u4ed6\u8bed\u8a00\u5efa\u6a21\u5668\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u7684\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\u3002\n\n\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u804c\u4e1a\u3001\u884c\u4e1a\u548c\u5730\u7406\u7684\u56e0\u7d20\uff0c\u6269\u5c55\u4e86\u9488\u5bf9\u804c\u4e1a\u7684 AI \u5f71\u54cd\u7814\u7a76\u7684\u65b9\u6cd5\u5b66\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u6700\u5927\u7684\u804c\u4e1a\u5305\u62ec\u7535\u8bdd\u9500\u552e\u5458\u3001\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5b66\u3001\u5916\u8bed\u548c\u6587\u5b66\u4ee5\u53ca\u5386\u53f2\u8001\u5e08\uff1b\u53d7\u5f71\u54cd\u6700\u5927\u7684\u884c\u4e1a\u5305\u62ec\u6cd5\u5f8b\u670d\u52a1\u548c\u8bc1\u5238\u3001\u5546\u54c1\u548c\u6295\u8d44\u3002\u5176\u5206\u6790\u57fa\u4e8e Felten \u7b49\u4eba (2018, 2021) \u5efa\u7acb\u7684 AI \u804c\u4e1a\u66b4\u9732\u5ea6 (AIOE) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u662f\u901a\u8fc7\u5c06 10 \u79cd AI \u5e94\u7528\u7a0b\u5e8f\u548c 52 \u79cd\u4eba\u7c7b\u80fd\u529b\u8fdb\u884c\u5339\u914d\u8fde\u63a5\u800c\u6784\u5efa\u7684\u3002\n\n\u4f5c\u8005\u4f7f\u7528\u52a0\u6743\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u804c\u4e1a\u4e0eAI\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u7a0b\u5ea6\uff0c\u8fdb\u800c\u8ba1\u7b97\u51fa\u4e0eAI\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u804c\u4e1a\u66b4\u9732(AIOE)\u8bc4\u5206\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cbAIOE\u5206\u6570\u9ad8\u5ea6\u76f8\u5173\uff0c\u6392\u540d\u524d20\u7684\u804c\u4e1a\u4e5f\u4f1a\u53d1\u751f\u4e00\u4e9b\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e8621\u79cd\u884c\u4e1a\u7684AIOE\u6570\u5b57\uff0c\u4f9b\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u4f7f\u7528\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30AI\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8fdb\u5c55\u5bf9\u804c\u4e1a\u548c\u884c\u4e1a\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u5b66\u8005\u3001\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u5f88\u5927\u7684\u53c2\u8003\u548c\u501f\u9274\u4ef7\u503c\u3002\n\n\u672c\u6587\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002", "source_page_nums": "12", "source_chunk_nums": "6", "semantic_tags": "", "regular_tags": "", "page_span": "9", "chunk_id": "6", "chunk_text": "er, M., Lehmer, F., & Matthes, B. (2021). How do workers adjust  when firms adopt new technologies?. ZEW-Centre for European Economic Research  Discussion Paper, (21-073).    Goldfarb, A., Gans, J., & Agrawal, A. (2019). The economics of artificial intelligence: An agenda.  Chicago, IL: University of Chicago Press.    Goldfarb, A., Taska, B. Teodoridis, F. (2020). \u201cArtificial Intelligence in Healthcare? Evidence  from Online Job Postings\u201d, AEA Papers and Proceedings, 110 (5): 400-404    Tolan, S., Pesole, A., Mart\u00ednez-Plumed, F., Fern\u00e1ndez-Mac\u00edas, E., Hern\u00e1ndez-Orallo, J., &  G\u00f3mez, E. (2021). Measuring the occupational impact of AI: tasks, cognitive abilities and AI  benchmarks. Journal of Artificial Intelligence Research, 71, 191-236.    Webb, M. (2020). The impact of artificial intelligence on the labor market. Stanford University  working paper.    Zarifhonarvar, A. 2023. \u201cEconomics of ChatGPT: A Labor Market View on the Occupational  Impact of Artificial Intelligence\u201d Indiana University working paper. Available:  http://dx.doi.org/10.2139/ssrn.4350925       10    Figure 1: Comparison between Original AIOE and Language Modeling Adjusted AIOE      Notes: This figure plots the original AIOE score (x-axis) and the new language modeling  adjusted AIOE score (y-axis) for each occupation.  11    Table 1: Top 20 Occupations Exposed to AI, Original and with Language Modeling Adjustment        Notes: This table lists the top 20 occupations most exposed to AI from the original AIOE (Felten et al., 2021) and the top 20  occupations most exposed to language modeling.            12    Table 2: Top 20 Industries Exposed to AI, Original and with Language Modeling Adjustment        Notes: This table lists the top 20 industries most exposed to AI from the original AIOE (Felten et al., 2021) and the top 20 industries  most exposed to language modeling.    ", "chunk_summary": "\u672c\u6bb5\u843d\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u7684\u7ecf\u6d4e\u5b66\u5f71\u54cd\u3001\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u5728\u4efb\u52a1\u3001\u8ba4\u77e5\u80fd\u529b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u5f71\u54cd\u3001\u4ee5\u53ca ChatGPT \u5bf9\u4eba\u5de5\u667a\u80fd\u804c\u4e1a\u5f71\u54cd\u7684\u7ecf\u6d4e\u5b66\u89c6\u89d2\u7b49\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u7ed9\u51fa\u4e86\u6bd4\u8f83\u539f\u59cb AIOE \u8bc4\u5206\u548c\u7ecf\u8fc7\u8bed\u8a00\u5efa\u6a21\u8c03\u6574\u540e\u7684 AIOE \u8bc4\u5206\u7684\u997c\u72b6\u56fe\u548c\u8868\u683c\uff0c\u5217\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u6700\u5927\u7684 20 \u4e2a\u804c\u4e1a\u548c\u884c\u4e1a\u3002"}, "18": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "1", "chunk_id": "1", "chunk_text": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders Fangyu Liu, Ivan Vuli\u00b4c, Anna Korhonen, Nigel Collier Language Technology Lab, TAL, University of Cambridge {fl399, iv250, alk23, nhc30}cam.ac.uk Abstract Previous work has indicated that pretrained Masked Language Models (MLMs) are not ef- fective as universal lexical and sentence en- coders off-the-shelf, i.e., without further task- speci\ufb01c \ufb01ne-tuning on NLI, sentence similar- ity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any addi- tional data, relying simply on self-supervision. We propose an extremely simple, fast, and ef- fective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20\u201330 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modi\ufb01ed string pairs as positive (i.e., synonymous) \ufb01ne-tuning exam- ples, and aims to maximise their similarity dur- ing \u201cidentity \ufb01ne-tuning\u201d. We report huge gains over off-the-shelf MLMs with Mirror- BERT both in lexical-level and in sentence- level tasks, across different domains and differ- ent languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence- BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and sug- gest some evidence on why this simple Mirror- BERT \ufb01ne-tuning approach can yield effective universal lexical and sentence encoders. 1 Introduction Transfer learning with pretrained Masked Lan- guage Models (MLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) has been widely successful in NLP, offering unmatched per- formance in a large number of tasks (Wang et al., 2019a). Despite the wealth of semantic knowledge stored in the MLMs (Rogers et al., 2020), they do not produce high-quality lexical and sentence em- beddings when used off-the-shelf, without further dist(  ,  ) < dist(  / ,  / /\u2026) f(x1) f(\u00afx1) f(x1) f(\u00afx1) f(x2) f(x3) BERT/ RoBERTa BERT/ RoBERTa weight  sharing ( random span masking ) multiple  dropout layers x1 \u00afx1 f(x1) f(\u00afx1) f(x2) f(x3) f(x4) f(x5) Figure 1: Illustration of the main concepts behind the proposed self-supervised Mirror-BERT method. The same text sequence can be observed from two addi- tional \u201cviews\u201d: 1) by performing random span mask- ing in the input space, and/or 2) by applying dropout (inside the BERT/RoBERTa MLM) in the feature space, yielding identity-based (i.e., \u201cmirrored\u201d) positive exam- ples for \ufb01ne-tuning. A contrastive learning objective is then applied to encourage such \u201cmirrored\u201d positive pairs to obtain more similar representations in the em- bedding space relatively to negative pairs. task-speci\ufb01c \ufb01ne-tuning (Feng et al., 2020; Li et al., 2020). In fact, previous work has shown that their performance is sometimes even below static word embeddings and specialised sentence encoders (Cer et al., 2018) in lexical and sentence-level seman- tic similarity tasks (Reimers and Gurevych, 2019; Vuli\u00b4c et al., 2020b; Litschko et al., 2021). In order to address this gap, recent work has trained dual-encoder networks on labelled exter- nal resources to convert MLMs into universal lan- guage encoders. Most notably, Sentence-BERT (SBERT, Reimers and Gurevych 2019) further trains BERT and RoBERTa on Natural Language Inference (NLI, Bowman et al. 2015; Williams et al. 2018) and sentence similarity data (Cer et al., 2017) to obtain high-quality universal sentence embed- dings. Recently, SapBERT (Liu et al., 2021) self- arXiv:2104.08027v2  [cs.CL]  9", "chunk_summary": "\u672c\u6587\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff08MLMs\uff09\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u6ce8\u91ca\u7684\u4efb\u52a1\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5feb\u901f\u3001\u6709\u6548\u7684\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\u2014\u2014\u955c\u50cfBERT\uff08Mirror-BERT\uff09\uff0c\u5c06MLMs\uff08\u5982BERT\u548cRoBERTa\uff09\u8f6c\u5316\u4e3a\u7f16\u7801\u5668\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\uff0c\u4ec5\u970020-30\u79d2\u3002Mirror-BERT\u4f9d\u8d56\u4e8e\u76f8\u540c\u548c\u7565\u5fae\u4fee\u6539\u7684\u5b57\u7b26\u4e32\u5bf9\u4f5c\u4e3a\u6b63\uff08\u5373\u540c\u4e49\uff09\u5fae\u8c03\u6837\u672c\uff0c\u5e76\u65e8\u5728\u5728\u201c\u8eab\u4efd\u5fae\u8c03\u201d\u671f\u95f4\u6700\u5927\u5316\u5b83\u4eec\u7684\u76f8\u4f3c\u6027\u3002\u4f5c\u8005\u6c47\u62a5\u4e86Mirror-BERT\u76f8\u6bd4\u4f7f\u7528\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u6c47\u548c\u53e5\u5b50\u7ea7\u4efb\u52a1\u4e0a\u53d6\u5f97\u7684\u5de8\u5927\u6536\u76ca\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u9886\u57df\u548c\u4e0d\u540c\u8bed\u8a00\u4e2d\u90fd\u6709\u7740\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6700\u540e\uff0c\u4f5c\u8005\u6df1\u5165\u63a2\u8ba8\u4e86MLMs\u7684\u5185\u90e8\u8fd0\u4f5c\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u8bc1\u636e\uff0c\u8bf4\u660e\u4e3a\u4ec0\u4e48\u8fd9\u79cd\u7b80\u5355\u7684Mirror-BERT\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u4ea7\u751f\u6709\u6548\u7684\u901a\u7528\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\u3002"}, "19": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "1,2", "chunk_id": "2", "chunk_text": " task-speci\ufb01c \ufb01ne-tuning (Feng et al., 2020; Li et al., 2020). In fact, previous work has shown that their performance is sometimes even below static word embeddings and specialised sentence encoders (Cer et al., 2018) in lexical and sentence-level seman- tic similarity tasks (Reimers and Gurevych, 2019; Vuli\u00b4c et al., 2020b; Litschko et al., 2021). In order to address this gap, recent work has trained dual-encoder networks on labelled exter- nal resources to convert MLMs into universal lan- guage encoders. Most notably, Sentence-BERT (SBERT, Reimers and Gurevych 2019) further trains BERT and RoBERTa on Natural Language Inference (NLI, Bowman et al. 2015; Williams et al. 2018) and sentence similarity data (Cer et al., 2017) to obtain high-quality universal sentence embed- dings. Recently, SapBERT (Liu et al., 2021) self- arXiv:2104.08027v2  [cs.CL]  9 Sep 2021 aligns phrasal representations of the same meaning using synonyms extracted from the UMLS (Boden- reider, 2004), a large biomedical knowledge base, obtaining lexical embeddings in the biomedical domain that reach state-of-the-art (SotA) perfor- mance in biomedical entity linking tasks. However, both SBERT and SapBERT require annotated (i.e., human-labelled) data as external knowledge: it is used to instruct the model to produce similar repre- sentations for text sequences (e.g., words, phrases, sentences) of similar/identical meanings. In this paper, we fully dispose of any external supervision, demonstrating that the transformation of MLMs into universal language encoders can be achieved without task-labelled data. We pro- pose a \ufb01ne-tuning framework termed Mirror-BERT, which simply relies on duplicating and slightly aug- menting the existing text input (or their representa- tions) to achieve the transformation, and show that it is possible to learn universal lexical and sentence encoders with such \u201cmirrored\u201d input data through self-supervision (see Fig. 1). The proposed Mirror- BERT framework is also extremely ef\ufb01cient: the whole MLM transformation can be completed in less than one minute on two 2080Ti GPUs. Our \ufb01ndings further con\ufb01rm a general hypothe- sis from prior work (Liu et al., 2021; Ben-Zaken et al., 2020; Glava\u0161 and Vuli\u00b4c, 2021) that \ufb01ne- tuning exposes the wealth of (semantic) knowledge stored in the MLMs. In this case in particular, we demonstrate that the Mirror-BERT procedure can rewire the MLMs to serve as universal language en- coders even without any external supervision. We further show that data augmentation in both input space and feature space are key to the success of Mirror-BERT, and they provide a synergistic effect. Contributions. 1) We propose a completely self- supervised approach that can quickly transform pretrained MLMs into capable universal lexical and sentence encoders, greatly outperforming off- the-shelf MLMs in similarity tasks across different languages and domains. 2) We investigate the ra- tionales behind why Mirror-BERT works at all, aiming to understand the impact of data augmen- tation in the input space as well as in the feature space. We release our code and models at https: //github.com/cambridgeltl/mirror-bert. 2 Mirror-BERT: Methodology Mirror-BERT consists of three main parts, de- scribed in what follows. First, we create positive pairs by duplicating the input text (\u00a72.1). We then further process the positive pairs by simple data augmentation operating either on the input text or on the feature map inside the model (\u00a72.2). Finally, we apply standard contrastive learning, \u2018attracting\u2019 the texts belonging to the same class (i.e., positives) while pushing away the negatives (\u00a72.3). 2.1 Training Data through Self-Duplication The key to success of dual-network representa- tion learning (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020; Liu et al., 2021, inter alia) is the construction of positive and negative pairs. While negative pairs can be eas- ily obtained from randomly sampled texts, posi- tive pairs usually need to be manually annotated. In practice, they are extracted from labelled task data (e.g., NLI) or knowledge bases that store rela- tions such as synonym", "chunk_summary": "\u672c\u7bc7\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b(MLMs)\u8f6c\u6362\u4e3a\u901a\u7528\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u4ee5\u63d0\u9ad8\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u4e4b\u524d\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u53ea\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u6709\u65f6\u751a\u81f3\u4f4e\u4e8e\u9759\u6001\u8bcd\u5411\u91cf\u548c\u4e13\u95e8\u7684\u53e5\u5b50\u7f16\u7801\u5668\u3002\u6700\u8fd1\u7684\u7814\u7a76\u91c7\u7528\u76d1\u7763\u6570\u636e\u6765\u8bad\u7ec3\u53cc\u7f16\u7801\u5668\u7f51\u7edc\uff0c\u4ee5\u5c06MLMs\u8f6c\u6362\u4e3a\u901a\u7528\u8bed\u8a00\u7f16\u7801\u5668\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aMirror-BERT\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u6837\u672c\u81ea\u6211\u590d\u5236\u7684\u65b9\u5f0f\u5c06\u73b0\u6709\u7684\u8f93\u5165\u6570\u636e\u7a0d\u52a0\u6269\u5145\uff0c\u4f7f\u5176\u901a\u8fc7\u81ea\u6211\u76d1\u7763\u65b9\u5f0f\u6210\u4e3a\u901a\u7528\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\u3002Mirror-BERT\u6846\u67b6\u975e\u5e38\u9ad8\u6548\uff0c\u53ef\u4ee5\u5728\u5c0f\u4e8e\u4e00\u5206\u949f\u7684\u65f6\u95f4\u5185\u5b8c\u6210MLM\u8f6c\u6362\u3002\u672c\u6587\u7684\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u81ea\u6211\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5feb\u901f\u5c06\u9884\u8bad\u7ec3\u7684MLMs\u8f6c\u5316\u4e3a\u529f\u80fd\u5f3a\u5927\u7684\u901a\u7528\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u9886\u57df\u7684\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002Mirror-BERT\u6846\u67b6\u7684\u91cd\u8981\u6027\u5728\u4e8e\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u955c\u50cf\u8f93\u5165\u6570\u636e\uff0c\u7121\u9700\u4f7f\u7528\u4eba\u5de5\u6807\u7b7e\u7684\u9644\u52a0\u4fe1\u606f\uff0c\u4e5f\u53ef\u4f7fMLMs\u6210\u4e3a\u66f4\u5f3a\u5927\u7684\u901a\u7528\u7f16\u7801\u5668\u3002"}, "20": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "2,3", "chunk_id": "3", "chunk_text": "/cambridgeltl/mirror-bert. 2 Mirror-BERT: Methodology Mirror-BERT consists of three main parts, de- scribed in what follows. First, we create positive pairs by duplicating the input text (\u00a72.1). We then further process the positive pairs by simple data augmentation operating either on the input text or on the feature map inside the model (\u00a72.2). Finally, we apply standard contrastive learning, \u2018attracting\u2019 the texts belonging to the same class (i.e., positives) while pushing away the negatives (\u00a72.3). 2.1 Training Data through Self-Duplication The key to success of dual-network representa- tion learning (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020; Liu et al., 2021, inter alia) is the construction of positive and negative pairs. While negative pairs can be eas- ily obtained from randomly sampled texts, posi- tive pairs usually need to be manually annotated. In practice, they are extracted from labelled task data (e.g., NLI) or knowledge bases that store rela- tions such as synonymy or hypernymy (e.g., PPDB, Pavlick et al. 2015; BabelNet, Ehrmann et al. 2014; WordNet, Fellbaum 1998; UMLS). Mirror-BERT, however, does not rely on any ex- ternal data to construct the positive examples. In a nutshell, given a set of non-duplicated strings X, we assign individual labels (yi) to each string and build a dataset D = {(xi, yi)|xi \u2208 X, yi \u2208 {1, . . . , |X|}}. We then create self-duplicated training data D\u2032 simply by repeating every ele- ment in D. In other words, let X = {x1, x2, . . .}. We then have D = {(x1, y1), (x2, y2), . . .} and D\u2032 = {(x1, y1), (x1, y1), (x2, y2), (x2, y2), . . .} where x1 = x1, y1 = y1, x2 = x2, y2 = y2, . . .. In \u00a72.2, we introduce data augmentation techniques (in both input space and feature space) applied on D\u2032. Each positive pair (xi, xi) yields two different points/vectors in the encoder\u2019s representation space (see again Fig. 1), and the distance between these points should be minimised. 2.2 Data Augmentation We hypothesise that applying certain \u2018corruption\u2019 techniques to (i) parts of input text sequences or (ii) to their representations, or even (iii) doing both in combination, does not change their (captured) meaning. We present two \u2018corruption\u2019 techniques as illustrated in Fig. 1. First, we can directly mask parts of the input text. Second, we can erase (i.e., dropout) parts of their feature maps. Both tech- niques are rather simple and intuitive: (i) even when masking parts of an input sentence, humans can usually reconstruct its semantics; (ii) dropping a small subset of neurons or representation dimen- 2 v1 == v1 f(x1) f(x1) f(x2) f(x3) f(x5) f(x4) dist(f(x1), f(x1)) < dist(f(x1/x1), f(x2/x3/\u2026)) x1: Economist Paul Krugman mainly works on trade models.  x1: Econ [MASK] Paul Krugman mainly works on trade models.  BERT/ RoBERTa BERT/ RoBERTa x1 weight  sharing x1 random masking (v1) != dropout(v1) dropout multiple  dropout layers Figure 2: An example of input data augmentation via random span masking. sions, the representations of a neural network will not drift too much. Input Augmentation: Random Span Masking. The idea is inspired by random cropping in visual representation learning (Hendrycks et al., 2020). In particular, starting from the mirrored pairs (xi, yi) and (xi, yi), we randomly replace a consecutive string of length k with [MASK] in either xi or xi. The example (Fig. 2) illustrates the random span masking procedure with k = 5. Feature Augmentation: Dropout. The random span masking technique, operating directly on text input, can be applied only with sentence/phrase- level input; word-level tasks involve only short strings, usually represented as a single token under the sentence-piece tokeniser. However, data aug- mentation in the feature space based on dropout, as", "chunk_summary": "Mirror-BERT\u7684\u4e3b\u8981\u90e8\u5206\u5305\u62ec\u6784\u5efa\u6b63\u6837\u672c\u5bf9\u548c\u8d1f\u6837\u672c\u5bf9\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u6570\u636e\u589e\u5f3a\u8fdb\u4e00\u6b65\u5904\u7406\u6b63\u6837\u672c\u5bf9\uff0c\u4ee5\u53ca\u5e94\u7528\u6807\u51c6\u7684\u5bf9\u6bd4\u5b66\u4e60\u201c\u5438\u5f15\u201d\u5c5e\u4e8e\u540c\u4e00\u7c7b\u522b\u7684\u6587\u672c\uff08\u5373\u6b63\u6837\u672c\uff09\uff0c\u540c\u65f6\u63a8\u5f00\u8d1f\u6837\u672c\u3002\u6784\u5efa\u6b63\u6837\u672c\u662f\u6210\u529f\u8fdb\u884c\u53cc\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u7684\u5173\u952e\u3002Mirror-BERT\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u5916\u90e8\u6570\u636e\u6765\u6784\u5efa\u6b63\u6837\u672c\uff0c\u901a\u8fc7\u81ea\u6211\u590d\u5236\u8bad\u7ec3\u6570\u636e\u6765\u6784\u5efa\u6b63\u6837\u672c\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5728\u8f93\u5165\u7a7a\u95f4\u548c\u7279\u5f81\u7a7a\u95f4\u4e2d\uff09\uff0c\u6280\u672f\u5305\u62ec\u76f4\u63a5\u5c4f\u853d\u8f93\u5165\u6587\u672c\u7684\u90e8\u5206\u548c\u64e6\u9664\u5176\u7279\u5f81\u56fe\u7b49\u3002\u6570\u636e\u589e\u5f3a\u8fc7\u7a0b\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}, "21": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "3", "chunk_id": "4", "chunk_text": ")) x1: Economist Paul Krugman mainly works on trade models.  x1: Econ [MASK] Paul Krugman mainly works on trade models.  BERT/ RoBERTa BERT/ RoBERTa x1 weight  sharing x1 random masking (v1) != dropout(v1) dropout multiple  dropout layers Figure 2: An example of input data augmentation via random span masking. sions, the representations of a neural network will not drift too much. Input Augmentation: Random Span Masking. The idea is inspired by random cropping in visual representation learning (Hendrycks et al., 2020). In particular, starting from the mirrored pairs (xi, yi) and (xi, yi), we randomly replace a consecutive string of length k with [MASK] in either xi or xi. The example (Fig. 2) illustrates the random span masking procedure with k = 5. Feature Augmentation: Dropout. The random span masking technique, operating directly on text input, can be applied only with sentence/phrase- level input; word-level tasks involve only short strings, usually represented as a single token under the sentence-piece tokeniser. However, data aug- mentation in the feature space based on dropout, as introduced below, can be applied to any input text. Dropout (Srivastava et al., 2014) randomly drops neurons from a neural net during training with a probability p. In practice, it results in the erasure of each element with a probability of p. It has mostly been interpreted as implicitly bagging a large num- ber of neural networks which share parameters at test time (Bouthillier et al., 2015). Here, we take advantage of the dropout layers in BERT/RoBERTa to create augmented views of the input text. Given a pair of identical strings xi and xi, their repre- sentations in the embedding space slightly differ due to the existence of multiple dropout layers in the BERT/RoBERTa architecture (Fig. 6). The two data points in the embedding space can be seen as two augmented views of the same text sequence, which can be leveraged for \ufb01ne-tuning.1 It is possible to combine data augmentation via random span masking and featuure augmentation via dropout; this variant is also evaluated later. 2.3 Contrastive Learning Let f(\u00b7) denote the encoder model. The encoder is then \ufb01ne-tuned on the data constructed in \u00a72.2. 1The dropout augmentations are naturally a part of the BERT/RoBERTa network. That is, no further actions need to be taken to implement them. Note that random span masking is applied on only one side of the positive pair while dropout is applied on all data points. dropout dropout   ==   v1 \u00afv1 dropout(   ) != dropout(   ) v1 \u00afv1 Figure 3: As the same vector goes through the same dropout layer separately, the outcomes are independent. Consequently, two fully identical strings fed to the sin- gle BERT/RoBERTa model yield different representa- tions in the MLM embedding space. Given a batch of data D\u2032b, we leverage the standard InfoNCE loss (Oord et al., 2018) to cluster/attract the positive pairs together and push away the nega- tive pairs in the embedding space: Lb = \u2212 |Db| \ufffd i=1 log exp(cos(f(xi), f(xi))/\u03c4) \ufffd xj\u2208Ni exp(cos(f(xi), f(xj))/\u03c4) . (1) \u03c4 denotes a temperature parameter; Ni denotes all negatives of xi, which includes all xj, xj where i \u0338= j in the current data batch (i.e., |Ni| = |D\u2032b| \u2212 2). Intuitively, the numerator is the similarity of the self-duplicated pair (the positive example) and the denominator is the sum of the similarities between xi and all other strings besides xi (the negatives).2 3 Experimental Setup Evaluation Tasks: Lexical. We evaluate on domain-general and domain-speci\ufb01c tasks: word similarity and biomedical entity linking (BEL). For the former, we rely on the Multi-SimLex evaluation set (Vuli\u00b4c et al., 2020a): it contains human-elicited word similarity scores for multiple languages. For the latter, we use NCBI-disease (NCBI, Do\u02d8gan et al. 2014), BC5CDR-disease, BC5CDR-chemical (BC5-d, BC5-c, Li et al. 2016), AskAPatient (Limsopatham and Collier, 2016) and COMETA (strati\ufb01ed-general split, Basaldella et al. 2020) as our evaluation datasets. The \ufb01rst three datasets are in the scienti\ufb01c domain (i.e.,", "chunk_summary": "\u5728\u8f93\u5165\u548c\u7279\u5f81\u589e\u5f3a\u65b9\u9762\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6280\u672f\uff1a\u968f\u673a\u533a\u95f4\u906e\u853d\u548c\u968f\u673a\u4e22\u5931\u3002\u5176\u4e2d\uff0c\u968f\u673a\u533a\u95f4\u906e\u853d\u6280\u672f\u662f\u4ece\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u968f\u673a\u88c1\u526a\u4e2d\u5f97\u5230\u542f\u53d1\u7684\uff0c\u5176\u4ece\u5bf9\u79f0\u5bf9\u4e2d\u968f\u673a\u66ff\u6362\u957f\u5ea6\u4e3ak\u7684\u8fde\u7eed\u5b57\u7b26\u4e32\u4e3a[MASK]\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u968f\u673a\u4e22\u5931\u6280\u672f\u662f\u6307\u5728\u8bad\u7ec3\u671f\u95f4\u968f\u673a\u5c06\u795e\u7ecf\u5143\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u5220\u9664\uff0c\u5176\u7ed3\u679c\u662f\u5c06\u6bcf\u4e2a\u5143\u7d20\u64e6\u9664\u7684\u6982\u7387\u4e3ap\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u672c\u6587\u5229\u7528BERT/RoBERTa\u4e2d\u7684dropout\u5c42\u6765\u521b\u5efa\u589e\u5f3a\u7684\u8f93\u5165\u89c6\u56fe\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u4e24\u4e2a\u6570\u636e\u70b9\u53ef\u4ee5\u770b\u4f5c\u662f\u540c\u4e00\u6587\u672c\u5e8f\u5217\u7684\u4e24\u4e2a\u589e\u5f3a\u89c6\u56fe\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e24\u79cd\u6280\u672f\u8fdb\u884c\u7ec4\u5408\u8fd8\u53ef\u4ee5\u5f97\u5230\u53e6\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u5728\u5bf9\u6570\u636e\u8fdb\u884c\u8f93\u5165\u548c\u7279\u5f81\u589e\u5f3a\u4e4b\u540e\uff0c\u672c\u6587\u901a\u8fc7\u6807\u51c6\u7684InfoNCE loss\u5bf9\u6b63\u4f8b\u8fdb\u884c\u805a\u7c7b\u548c\u8d1f\u4f8b\u95f4\u7684\u63a8\u5f00\uff0c\u4ece\u800c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u3002\u6700\u540e\uff0c\u4f5c\u8005\u4f7f\u7528\u591a\u8bed\u8a00\u8bcd\u76f8\u4f3c\u5ea6\u548c\u751f\u7269\u533b\u836f\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u5bf9\u8be5\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002"}, "22": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "3,4", "chunk_id": "5", "chunk_text": "= j in the current data batch (i.e., |Ni| = |D\u2032b| \u2212 2). Intuitively, the numerator is the similarity of the self-duplicated pair (the positive example) and the denominator is the sum of the similarities between xi and all other strings besides xi (the negatives).2 3 Experimental Setup Evaluation Tasks: Lexical. We evaluate on domain-general and domain-speci\ufb01c tasks: word similarity and biomedical entity linking (BEL). For the former, we rely on the Multi-SimLex evaluation set (Vuli\u00b4c et al., 2020a): it contains human-elicited word similarity scores for multiple languages. For the latter, we use NCBI-disease (NCBI, Do\u02d8gan et al. 2014), BC5CDR-disease, BC5CDR-chemical (BC5-d, BC5-c, Li et al. 2016), AskAPatient (Limsopatham and Collier, 2016) and COMETA (strati\ufb01ed-general split, Basaldella et al. 2020) as our evaluation datasets. The \ufb01rst three datasets are in the scienti\ufb01c domain (i.e., the data have been ex- tracted from scienti\ufb01c papers), while the latter two 2We also experimented with another state-of-the-art con- trastive learning scheme proposed by Liu et al. (2021). There, hard triplet mining combined with multi-similarity loss (MS loss) is used as the learning objective. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 3 are in the social media domain (i.e., extracted from online forums discussing health-related topics). We report Spearman\u2019s rank correlation coef\ufb01cients (\u03c1) for word similarity; accuracy @1/@5 is the stan- dard evaluation measure in the BEL task. Evaluation Tasks: Sentence-Level. Evaluation on the intrinsic sentence textual similarity (STS) task is conducted on the standard SemEval 2012- 2016 datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (STS-b, Cer et al. 2017), SICK-Relatedness (SICK-R, Marelli et al. 2014) for English; STS SemEval-17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman\u2019s \u03c1 rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also as- sess the bene\ufb01ts of Mirror-BERT on cross-lingual representation learning, evaluating on cross-lingual word similarity (CLWS, Multi-SimLex is used) and bilingual lexicon induction (BLI). We rely on the standard mapping-based BLI setup (Artetxe et al., 2018), and training and test sets from Glava\u0161 et al. (2019), reporting accuracy @1 scores (with CSLS as the word retrieval method, Lample et al. 2018). Mirror-BERT: Training Resources. For \ufb01ne- tuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the Wiki- Matrix dataset (Schwenk et al., 2021). For QNLI, we sample 10k sentences from its training set. Training Setup and Details. The hyperparame- ters of word-level models are tuned on SimLex-999 (Hill et al., 2015); biomedical models are tuned on COMETA (zero-shot-general split). Sentence- level models are tuned on the dev set of STS-b. \u03c4 in Eq. (1) is 0.04 (biomedical and sentence-level models); 0.2 (word-level). Dropout rate p is 0.1. Sentence-level models use a random span masking 3github.com/deepmipt/deepPavlovEval 4We follow the setup of Li et al. (2020) and adapt QNLI to an unsupervised task by computing the AUC scores (on the development set, \u2248", "chunk_summary": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5feb\u901f\u6709\u6548\u7684\u81ea\u76d1\u7763\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u2014\u2014Mirror-BERT\u3002\u4f5c\u8005\u4f7f\u7528\u4e86InfoNCE\u5b66\u4e60\u76ee\u6807\u548cSelf-Supervised Contrastive Learning\u65b9\u6cd5\u6765\u8bad\u7ec3Mirror-BERT\u3002\u8bc4\u4f30\u4efb\u52a1\u6db5\u76d6\u4e86\u8bcd\u7ea7\u548c\u53e5\u5b50\u7ea7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4efb\u52a1\u4ee5\u53ca\u8de8\u8bed\u8a00\u8bcd\u6c47\u76f8\u4f3c\u5ea6\u548c\u53cc\u8bed\u8bcd\u5178\u5f52\u7eb3\u4efb\u52a1\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u53d1\u73b0Mirror-BERT\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9886\u57df\u7279\u5b9a\u7684\u4efb\u52a1\u4e2d\uff0c\u5982\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u548c\u793e\u4ea4\u5a92\u4f53\u9886\u57df\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u4e5f\u5bf9\u4e0d\u540c\u8d85\u53c2\u6570\u7684\u9009\u62e9\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002"}, "23": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "4,5", "chunk_id": "6", "chunk_text": " words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the Wiki- Matrix dataset (Schwenk et al., 2021). For QNLI, we sample 10k sentences from its training set. Training Setup and Details. The hyperparame- ters of word-level models are tuned on SimLex-999 (Hill et al., 2015); biomedical models are tuned on COMETA (zero-shot-general split). Sentence- level models are tuned on the dev set of STS-b. \u03c4 in Eq. (1) is 0.04 (biomedical and sentence-level models); 0.2 (word-level). Dropout rate p is 0.1. Sentence-level models use a random span masking 3github.com/deepmipt/deepPavlovEval 4We follow the setup of Li et al. (2020) and adapt QNLI to an unsupervised task by computing the AUC scores (on the development set, \u22485.4k pairs) using 0/1 labels and cosine similarity scores of QA embeddings. lang.\u2192 EN FR ET AR ZH RU ES PL avg. fastText .528 .560 .447 .409 .428 .435 .488 .396 .461 BERT .267 .020 .106 .220 .398 .202 .177 .217 .201 + Mirror .556 .621 .308 .538 .639 .365 .296 .444 .471 mBERT .105 .130 .094 .101 .261 .109 .095 .087 .123 + Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264 Table 1: Word similarity evaluation on Multi-SimLex. \u201cBERT\u201d denotes monolingual BERT models in each language (see the Appendix). \u201cmBERT\u201d denotes mul- tilingual BERT. Bold and underline denote highest and second-highest scores per column, respectively. scienti\ufb01c language social media language dataset\u2192 model\u2193 NCBI BC5-d BC5-c AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 SapBERT .920 .956 .935 .960 .965 .982 .705 .889 .659 .779 BERT .676 .770 .815 .891 .798 .912 .382 .433 .404 .477 + Mirror .872 .921 .921 .949 .957 .971 .555 .695 .547 .647 PubMedBERT .778 .869 .890 .938 .930 .946 .425 .496 .468 .532 + Mirror .909 .948 .930 .962 .958 .979 .590 .750 .603 .713 Table 2: Biomedical entity linking (BEL) evaluation. rate of k = 5, while k = 2 for biomedical phrase- level models; we do not employ span masking for word-level models (an analysis is in the Appendix). All lexical models are trained for 2 epochs, max to- ken length is 25. Sentence-level models are trained for 1 epoch with a max sequence length of 50. All models use AdamW (Loshchilov and Hut- ter, 2019) as the optimiser, with a learning rate of 2e-5, batch size of 200 (400 after duplication). In all tasks, for all \u2018Mirror-tuned\u2019 models, unless noted otherwise, we create \ufb01nal representations using [CLS], instead of another common option: mean-pooling (mp) over all token representations in the last layer (Reimers and Gurevych, 2019).5 6 4 Results and Discussion 4.1 Lexical-Level Tasks Word Similarity (Tab. 1). SotA static word em- beddings such as fastText (Mikolov et al., 2018) typically outperform off-the-shelf MLMs on word similarity datasets (Vuli\u00b4c et al., 2020a). How- ever, our results demonstrate that the Mirror-BERT procedure indeed converts the MLMs into much stronger word encoders. The Multi-SimLex results on 8 languages from Tab. 1 suggest that the \ufb01ne- 5For \u2018non-Mirrored\u2019 original MLMs, the results with mp are reported instead; they produce much better results than using [CLS]; see the Appendix. 6All reported results are averages of three runs. In general, the training is very stable, with negligible \ufb02uctuations. 4 model\u2193, dataset\u2192 STS12 STS13 STS14 STS15 STS16 STS-b SICK-R avg. SBERT* .719 .774 .742", "chunk_summary": "\u672c\u6bb5\u4e3b\u8981\u4ecb\u7ecd\u4e86\u5b9e\u9a8c\u9009\u62e9\u7684\u8bed\u8a00\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5217\u51fa\u4e86\u5728\u8fd9\u4e9b\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u6240\u7528\u7684\u8bad\u7ec3\u8bbe\u7f6e\u548c\u7ec6\u8282\u3002\u5728\u8bcd\u7ea7\u522b\u7684\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u4e86SimLex-999\u6570\u636e\u96c6\uff0c\u9488\u5bf9\u751f\u7269\u533b\u5b66\u540d\u79f0\u8868\u793a\u7684\u4efb\u52a1\uff0c\u968f\u673a\u91c7\u6837\u4e8610k\u4e2aUMLS\u540d\u79f0\u3002\u5728\u53e5\u5b50\u7ea7\u522b\u7684\u4efb\u52a1\u4e2d\uff0c\u5bf9\u4e8eSTS\uff0c\u5728STS Benchmark\u7684\u8bad\u7ec3\u96c6\u4e2d\u968f\u673a\u91c7\u6837\u4e8610k\u4e2a\u53e5\u5b50\uff08\u65e0\u6807\u7b7e\uff09\uff1b\u5bf9\u4e8e\u897f\u73ed\u7259\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u548c\u4fc4\u8bed\uff0c\u4eceWiki-Matrix\u6570\u636e\u96c6\u4e2d\u91c7\u6837\u4e8610k\u4e2a\u53e5\u5b50\u3002\u5bf9\u4e8eQNLI\uff0c\u4ece\u5176\u8bad\u7ec3\u96c6\u4e2d\u91c7\u6837\u4e8610k\u4e2a\u53e5\u5b50\u3002\u672c\u6bb5\u8fd8\u7ed9\u51fa\u4e86\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5305\u62ec\u8bcd\u7ea7\u522b\u4efb\u52a1\u7684 Multi-SimLex \u5b9e\u9a8c\u548c\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMirror-BERT \u7b56\u7565\u5c06 MLMs \u8f6c\u6362\u4e3a\u66f4\u5f3a\u5927\u7684\u5355\u8bcd\u7f16\u7801\u5668\u3002"}, "24": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "4,5", "chunk_id": "7", "chunk_text": " option: mean-pooling (mp) over all token representations in the last layer (Reimers and Gurevych, 2019).5 6 4 Results and Discussion 4.1 Lexical-Level Tasks Word Similarity (Tab. 1). SotA static word em- beddings such as fastText (Mikolov et al., 2018) typically outperform off-the-shelf MLMs on word similarity datasets (Vuli\u00b4c et al., 2020a). How- ever, our results demonstrate that the Mirror-BERT procedure indeed converts the MLMs into much stronger word encoders. The Multi-SimLex results on 8 languages from Tab. 1 suggest that the \ufb01ne- 5For \u2018non-Mirrored\u2019 original MLMs, the results with mp are reported instead; they produce much better results than using [CLS]; see the Appendix. 6All reported results are averages of three runs. In general, the training is very stable, with negligible \ufb02uctuations. 4 model\u2193, dataset\u2192 STS12 STS13 STS14 STS15 STS16 STS-b SICK-R avg. SBERT* .719 .774 .742 .799 .747 .774 .721 .754 BERT-CLS .215 .321 .213 .379 .442 .203 .427 .314 BERT-mp .314 .536 .433 .582 .596 .464 .528 .493 + Mirror .670 .801 .713 .812 .743 .764 .699 .743 + Mirror (drophead) .691 .811 .730 .819 .757 .780 .691 .754 RoBERTa-CLS .090 .327 .210 .338 .388 .317 .355 .289 RoBERTa-mp .134 .126 .124 .203 .224 .129 .320 .180 + Mirror .646 .818 .734 .802 .782 .787 .703 .753 + Mirror (drophead) .666 .827 .740 .824 .797 .796 .697 .764 Table 3: English STS. *We were able to reproduce the scores reported in the original Sentence-BERT (SBERT, Reimers and Gurevych 2019) paper. However, we found mean-pooling over all tokens (including padded ones) yield better performance (.754 vs .749). We thus report the stronger baseline. model\u2193, lang.\u2192 ES AR RU avg. BERT .599 .455 .552 .533 + Mirror .709 .669 .673 .684 mBERT .610 .447 .616 .558 + Mirror .755 .594 .692 .680 Table 4: STS evaluation in other languages. tuned +Mirror variant substantially improves the performance of base MLMs (both monolingual and multilingual ones), even beating fastText in 5 out of the 8 evaluation languages.7 We also observe that it is essential to have a strong base MLM. While Mirror-BERT does offer substantial performance gains with all base MLMs, the improvement is more pronounced when the base model is strong (e.g., EN, ZH). Biomedical Entity Linking (Tab. 2). The goal of BEL is to map a biomedical name mention to a controlled vocabulary (usually a node in a knowl- edge graph). Considered a downstream application in BioNLP, the BEL task also helps evaluate and compare the quality of biomedical name representa- tions: it requires pairwise comparisons between the biomedical mention and all surface strings stored in the biomedical knowledge graph. The results from Tab. 2 suggest that our +Mirror transformation achieves very strong gains on top of the base MLMs, both BERT and PubMedBERT (Gu et al., 2020). We note that PubMedBERT is a current SotA MLM in the biomedical domain, and performs signi\ufb01cantly better than BERT, both be- fore and after +Mirror \ufb01ne-tuning. This highlights the necessity of starting from a domain-speci\ufb01c model when possible. On scienti\ufb01c datasets, the self-supervised PubMedBERT+Mirror model is 7Language codes: see the Appendix for a full listing. 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate BERT (area = 0.545) BERT+Mirror (area = 0.674) RoBERTa+Mirror (area = 0.709) SBERT (area = 0.706) Figure 4: Unsupervised QNLI: ROC curves and AOC scores. very close to SapBERT, which \ufb01ne-tunes PubMed- BERT with more than 10 million synonyms ex- tracted from the external UMLS knowledge base. However, in the social", "chunk_summary": "\u5728\u8bcd\u6c47\u7ea7\u522b\u7684\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u955c\u50cf-BERT\u8fc7\u7a0b\u786e\u5b9e\u5c06MLMs\u8f6c\u6362\u4e3a\u66f4\u5f3a\u5927\u7684\u5355\u8bcd\u7f16\u7801\u5668\u3002\u57288\u79cd\u8bed\u8a00\u4e0a\u8fdb\u884c\u7684\u591a\u91cdSimLex\u7ed3\u679c\u8868\u660e\uff0c\u8c03\u6574\u540e\u7684+ Mirror\u53d8\u4f53\u663e\u7740\u63d0\u9ad8\u4e86\u57fa\u672cMLM\u7684\u6027\u80fd\uff0c\u751a\u81f3\u57285\u4e2a\u8bc4\u4f30\u8bed\u8a00\u4e2d\u51fb\u8d25\u4e86fastText\u3002\u5728\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u65b9\u9762\uff0c\u8c03\u6574\u540e\u7684+ Mirror\u8f6c\u6362\u5728\u57fa\u672c\u7684MLMs\u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u660e\u663e\u7684\u6536\u76ca\uff0c\u5305\u62ecBERT\u548cPubMedBERT\u3002\u6211\u4eec\u6ce8\u610f\u5230\uff0cPubMedBERT\u662f\u751f\u7269\u533b\u5b66\u9886\u57df\u76ee\u524d\u7684SotA MLM\uff0c\u5728+Mirror\u5fae\u8c03\u4e4b\u524d\u548c\u4e4b\u540e\u90fd\u8868\u73b0\u663e\u8457\u4f18\u4e8eBERT\u3002"}, "25": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "5,6", "chunk_id": "8", "chunk_text": " al., 2020). We note that PubMedBERT is a current SotA MLM in the biomedical domain, and performs signi\ufb01cantly better than BERT, both be- fore and after +Mirror \ufb01ne-tuning. This highlights the necessity of starting from a domain-speci\ufb01c model when possible. On scienti\ufb01c datasets, the self-supervised PubMedBERT+Mirror model is 7Language codes: see the Appendix for a full listing. 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate BERT (area = 0.545) BERT+Mirror (area = 0.674) RoBERTa+Mirror (area = 0.709) SBERT (area = 0.706) Figure 4: Unsupervised QNLI: ROC curves and AOC scores. very close to SapBERT, which \ufb01ne-tunes PubMed- BERT with more than 10 million synonyms ex- tracted from the external UMLS knowledge base. However, in the social media domain, PubMed- BERT+Mirror still cannot match the performance of knowledge-guided SapBERT. This in fact re- \ufb02ects the nature and complexity of the task do- main. For the three datasets in the scienti\ufb01c domain (NCBI, BC5-d, BC5-c), strings with similar surface forms tend to be associated with the same concept. On the other hand, in the social media domain, se- mantics of very different surface strings might be the same.8 This also suggests that Mirror-BERT adapts PubMedBERT to a very good surface-form encoder for biomedical names, but dealing with more dif\ufb01cult synonymy relations (e.g. as found in the social media) does need external knowledge.9 4.2 Sentence-Level Tasks English STS (Tab. 3). Regardless of the base model (BERT/RoBERTa), applying +Mirror \ufb01ne- 8For instance, HCQ and Plaquenil refer to exactly the same concept on online health forums: Hydroxychloroquine. 9Motivated by these insights, in future work we will also investigate a combined approach that blends self-supervision and external knowledge (Vuli\u00b4c et al., 2021), which could also be automatically mined (Su, 2020; Thakur et al., 2021). 5 lang.\u2192 EN-FR EN-ZH EN-HE FR-ZH FR-HE ZH-HE avg. mBERT .163 .118 .071 .142 .104 .010 .101 + Mirror .454 .385 .133 .465 .163 .179 .297 Table 5: Cross-lingual word similarity results. lang.\u2192 EN-FR EN-IT EN-RU EN-TR IT-FR RU-FR avg. BERT .014 .112 .154 .150 .025 .018 .079 + Mirror .458 .378 .336 .289 .417 .345 .371 Table 6: BLI results. tuning greatly boosts performance across all En- glish STS datasets. Surprisingly, on average, RoBERTa+Mirror, \ufb01ne-tuned with only 10k sen- tences without any external supervision, is on-par with the SBERT model, which is trained on the merged SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets, containing 570k and 430k sentence pairs, respectively. Spanish, Arabic and Russian STS (Tab. 4). The results in the STS tasks on other languages, which all have different scripts, again indicate very large gains, using both monolingual language-speci\ufb01c BERTs and mBERT as base MLMs. This con\ufb01rms that Mirror-BERT is a language-agnostic method. Question-Answer Entailment (Fig. 4). The re- sults indicate that our +Mirror \ufb01ne-tuning con- sistently improves the underlying MLMs. The RoBERTa+Mirror variant even shows a slight edge over the supervised SBERT model (.709 vs. .706). 4.3 Cross-Lingual Tasks We observe huge gains across all language pairs in CLWS (Tab. 5) and BLI (Tab. 6) after running the Mirror-BERT procedure. For language pairs that involve Hebrew, the improvement is usually smaller. We suspect that this is due to mBERT itself containing poor semantic knowledge for He- brew. This \ufb01nding aligns with our prior argument that a strong base MLM is still required to obtain prominent gains from running Mirror-BERT. 4.4 Further Discussion and Analyses Running Time", "chunk_summary": "\u8be5\u6bb5\u843d\u4ecb\u7ecd\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u4e0d\u540c\u8bed\u8a00\u5bf9\u4e2d\u4f7f\u7528Mirror-BERT\u65b9\u6cd5\u7684\u7ed3\u679c\u3002\u5728\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\uff0cPubMedBERT+Mirror\u6a21\u578b\u8868\u73b0\u975e\u5e38\u63a5\u8fd1SapBERT\u6a21\u578b\uff0c\u800c\u540e\u8005\u5229\u7528\u5916\u90e8\u7684UMLS\u77e5\u8bc6\u5e93\u62bd\u53d6\u4e86\u8d85\u8fc71000\u4e07\u4e2a\u540c\u4e49\u8bcd\u8fdb\u884cfine-tuning\u3002\u7136\u800c\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u9886\u57df\uff0cPubMedBERT+Mirror\u4ecd\u65e0\u6cd5\u4e0e\u77e5\u8bc6\u5f15\u5bfc\u7684SapBERT\u7684\u6027\u80fd\u76f8\u5339\u914d\u3002\u4f5c\u8005\u6307\u51fa\uff0c\u8fd9\u53cd\u6620\u4e86\u4efb\u52a1\u57df\u7684\u6027\u8d28\u548c\u590d\u6742\u6027\uff0c\u5e76\u63d0\u793a\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u6765\u5904\u7406\u66f4\u590d\u6742\u7684\u540c\u4e49\u8bcd\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u5f3a\u8c03\u4e86Mirror-BERT\u662f\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u8bed\u8a00\u5bf9\u7684\u4ea4\u53c9\u8bed\u8a00\u5355\u8bcd\u76f8\u4f3c\u6027\u548c\u53cc\u8bed\u8bcd\u6c47\u5f52\u7eb3\u4efb\u52a1\u4e2d\u90fd\u4ea7\u751f\u4e86\u5de8\u5927\u7684\u63d0\u9ad8\u3002"}, "26": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "6,7", "chunk_id": "9", "chunk_text": " pairs, respectively. Spanish, Arabic and Russian STS (Tab. 4). The results in the STS tasks on other languages, which all have different scripts, again indicate very large gains, using both monolingual language-speci\ufb01c BERTs and mBERT as base MLMs. This con\ufb01rms that Mirror-BERT is a language-agnostic method. Question-Answer Entailment (Fig. 4). The re- sults indicate that our +Mirror \ufb01ne-tuning con- sistently improves the underlying MLMs. The RoBERTa+Mirror variant even shows a slight edge over the supervised SBERT model (.709 vs. .706). 4.3 Cross-Lingual Tasks We observe huge gains across all language pairs in CLWS (Tab. 5) and BLI (Tab. 6) after running the Mirror-BERT procedure. For language pairs that involve Hebrew, the improvement is usually smaller. We suspect that this is due to mBERT itself containing poor semantic knowledge for He- brew. This \ufb01nding aligns with our prior argument that a strong base MLM is still required to obtain prominent gains from running Mirror-BERT. 4.4 Further Discussion and Analyses Running Time. The Mirror-BERT procedure is extremely time-ef\ufb01cient. While \ufb01ne-tuning on NLI (SBERT) or UMLS (SapBERT) data can take hours, Mirror-BERT with 10k positive pairs com- pletes the conversion from MLMs to universal lan- guage encoders within a minute on two NVIDIA RTX 2080Ti GPUs. On average, 10-20 seconds is needed for 1 epoch of the Mirror-BERT procedure. 1k 10k 20k 50k 100k input size 0.4 0.5 0.6 score task word similarity biomedical entity linking sentence similarity Figure 5: The impact of the number of \ufb01ne-tuning \u201cmir- rored\u201d examples (x-axis) on the task performance (y- axis). The scores across tasks are not directly compara- ble, and are based on different evaluation metrics (\u00a73). Input Data Size (Fig. 5). In our main experi- ments in \u00a74.1-\u00a74.3, we always use 10k examples for Mirror-BERT tuning. In order to assess the importance of the \ufb01ne-tuning data size, we run a relevant analysis for a subset of base MLMs, and on a subset of English tasks. In particular, we evaluate the following: (i) BERT, Multi-SimLex (EN) (word-level); (ii) PubMedBERT, COMETA (biomedical phrase-level); (iii) RoBERTa, STS12 (sentence-level). The results indicate that the per- formance in all tasks reaches its peak in the region of 10k-20k examples and then gradually decreases, with a steeper drop on the the word-level task.10 11 Random Span Masking + Dropout? (Tab. 7). We conduct our ablation studies on the English STS tasks. First, we experiment with turn- ing off dropout, random span masking, or both. With both techniques turned off, we observe large performance drops of RoBERTa+Mirror and BERT+Mirror (see also the Appendix). Span mask- ing appears to be the more important factor: its absence causes a larger decrease. However, the best performance is achieved when both dropout and random span masking are leveraged, suggest- ing a synergistic effect when the two augmentation techniques are used together. Other Data Augmentation Types? Dropout vs. Drophead (Tab. 7). Encouraged by the effective- ness of random span masking and dropout for Mirror-BERT, a natural question to pose is: can 10We suspect that this is due to the inclusion of lower- frequency words into the \ufb01ne-tuning data: embeddings of such words typically obtain less reliable embeddings (Pilehvar et al., 2018). 11For word-level experiments, we used the top 100k words in English according to Wikipedia statistics. For phrase-level experiments, we randomly sampled 100k names from UMLS. For sentence-level experiments we sampled 100k sentences from SNLI and MultiNLI datasets (as the STS training set has fewer than 100k sentences). 6 model con\ufb01guration avg. \u03c1 RoBERTa + Mirror .753 - dropout + drophead .764 \u2191 .011 - dropout .732 \u2193 .021 - span mask .717 \u2193 .036 - dropout & span mask .682 \u2193 .071 Table 7: Ablation study: (i) replacing dropout with drophead; (ii) the synergistic effect of dropout and ran- dom span masking in the English STS tasks. controlled  dropout controlled  dropout   ==   v1 \u00af", "chunk_summary": "\u672c\u8282\u4ecb\u7ecd\u4e86\u4f7f\u7528Mirror-BERT\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528Mirror-BERT\u53ef\u4ee5\u5728\u591a\u8bed\u79cd\u4efb\u52a1\u4e0a\u53d6\u5f97\u5de8\u5927\u7684\u6536\u76ca\uff0c\u5305\u62ecPARS\u3001STS\u3001QA\u3001CLWS\u548cBLI\u7b49\u4efb\u52a1\uff1bMirror-BERT\u7684\u8fc7\u7a0b\u975e\u5e38\u9ad8\u6548\uff0c\u4e24\u4e2aNVIDIA RTX 2080Ti GPU\u4e0a\u7684\u5904\u7406\u65f6\u95f4\u5c0f\u4e8e1\u5206\u949f\uff1b\u4ece\u82f1\u6587\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u6765\u770b\uff0cSpan Masking\u548cDropout\u90fd\u662fMirror-BERT\u5de5\u4f5c\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4e24\u8005\u7684\u8054\u5408\u4f7f\u7528\u53ef\u4ee5\u53d6\u5f97\u6700\u4f73\u6548\u679c\u3002"}, "27": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "6,7", "chunk_id": "10", "chunk_text": ". 7). Encouraged by the effective- ness of random span masking and dropout for Mirror-BERT, a natural question to pose is: can 10We suspect that this is due to the inclusion of lower- frequency words into the \ufb01ne-tuning data: embeddings of such words typically obtain less reliable embeddings (Pilehvar et al., 2018). 11For word-level experiments, we used the top 100k words in English according to Wikipedia statistics. For phrase-level experiments, we randomly sampled 100k names from UMLS. For sentence-level experiments we sampled 100k sentences from SNLI and MultiNLI datasets (as the STS training set has fewer than 100k sentences). 6 model con\ufb01guration avg. \u03c1 RoBERTa + Mirror .753 - dropout + drophead .764 \u2191 .011 - dropout .732 \u2193 .021 - span mask .717 \u2193 .036 - dropout & span mask .682 \u2193 .071 Table 7: Ablation study: (i) replacing dropout with drophead; (ii) the synergistic effect of dropout and ran- dom span masking in the English STS tasks. controlled  dropout controlled  dropout   ==   v1 \u00afv1 dropout(   ) == dropout(   ) v1 \u00afv1 Figure 6: Under controlled dropout, if two strings are identical, they will have an identical set of dropout masks throughout the encoding process. other augmentation types work as well? Recent work points out that pretrained MLM are heav- ily overparameterised and most Transformer heads can be pruned without hurting task performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). Zhou et al. (2020) propose a drop- head method: it randomly prunes attention heads at MLM training as a regularisation step. We thus evaluate a variant of Mirror-BERT where the dropout layers are replaced with such dropheads:12 this results in even stronger STS performance, cf. Tab. 7. In short, this hints that the Mirror-BERT framework might bene\ufb01t from other data and fea- ture augmentation techniques in future work.13 Regularisation or Augmentation? (Tab. 8). When using dropout, is it possible that we are sim- ply observing the effect of adding/removing regu- larisation instead of the augmentation bene\ufb01t? To answer this question, we design a simple probe that attempts to disentangle the effect of regular- 12Drophead rates for BERT and RoBERTa are set to the default values of 0.2 and 0.05, respectively. 13Besides the drophead-based feature space augmentation, in our side experiments, we also tested input space augmenta- tion techniques such as whole-word masking, random token masking, and word reordering; they typically yield perfor- mance similar or worse to random span masking. We also point to very recent work that explores text augmentation in a different context (Wu et al., 2020; Meng et al., 2021). We leave a thorough search of optimal augmentation techniques for future work. model con\ufb01guration (MLM=RoBERTa) \u03c1 on STS12 random span masking \u0017; dropout \u0017 .562 random span masking \u0017; dropout \u0013 .648 \u2191 .086 random span masking \u0017; controlled dropout \u0013 .452 \u2193 .110 Table 8: Probing the impact of dropout. isation versus augmentation; we turn off random span masking but leave the dropout on (so that the regularisation effect remains). However, instead of assigning independent dropouts to every individual string (rendering each string slightly different), we control the dropouts applied to a positive pair to be identical. As a result, it holds f(xi) = f(xi), when xi \u2261 xi, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , |D|}. We denote this as \u201ccontrolled dropout\u201d. In Tab. 8, we observe that, during the +Mirror \ufb01ne-tuning, controlled dropout largely underperforms standard dropout and is even worse than not using dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mir- ror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geome- try. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Etha- yarajh (2019) shows that (off-the-shelf) MLM", "chunk_summary": "\u672c\u6587\u901a\u8fc7\u5bf9Mirror-BERT\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u673a\u8de8\u5ea6\u5c4f\u853d\u548cdropout\u7684\u6709\u6548\u6027\u3002\u5bf9\u4e8e\u82f1\u8bedSTS\u4efb\u52a1\uff0c\u63a7\u5236dropout\u5c5e\u4e8e\u6b63\u5e38\u6c34\u5e73\uff0c\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u7684\u662fdropout\u548c\u968f\u673a\u8de8\u5ea6\u5c4f\u853d\u7684\u7ed3\u5408\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63a2\u8ba8\u4e86Mirror-BERT\u6846\u67b6\u53d8\u4f53\uff0c\u53d1\u73b0\u6392\u9664dropout\u5e76\u901a\u8fc7\u968f\u673a\u4fee\u526a\u6ce8\u610f\u529b\u5934\u6765\u4ee3\u66ff\u5176\u7684drophead\uff0c\u4f1a\u5f97\u5230\u66f4\u5f3a\u7684STS\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u901a\u8fc7\u63a7\u5236dropout\u7684\u5b9e\u9a8c\u6765\u8bc1\u660e\uff0cMirror-BERT\u7684\u589e\u5f3a\u6548\u679c\u662f\u5b9e\u8d28\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u6b63\u5219\u5316\u7684\u5f71\u54cd\u3002\u6700\u540e\uff0c\u4f5c\u8005\u8ba4\u4e3a\uff0cMirror-BERT\u7684\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6e90\u81ea\u5176\u91cd\u5851\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u3002"}, "28": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "7,8", "chunk_id": "11", "chunk_text": " individual string (rendering each string slightly different), we control the dropouts applied to a positive pair to be identical. As a result, it holds f(xi) = f(xi), when xi \u2261 xi, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , |D|}. We denote this as \u201ccontrolled dropout\u201d. In Tab. 8, we observe that, during the +Mirror \ufb01ne-tuning, controlled dropout largely underperforms standard dropout and is even worse than not using dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mir- ror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geome- try. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Etha- yarajh (2019) shows that (off-the-shelf) MLMs\u2019 representations are anisotropic: they reside in a nar- row cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs with- out \ufb01ne-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distri- butions of cosine similarities and (2) an isotropy score, as de\ufb01ned by Mu and Viswanath (2018). First, we randomly sample 1,000 sentence pairs from the Quora Question Pairs (QQP) dataset. In 14Some preliminary evidence from Tab. 7 already leads in this direction: we observe large gains over the base MLMs even without any positive examples, that is, when both span masking and dropout are not used (i.e., it always holds xi = xi and f(xi) = f(xi)). During training, this leads to a constant numerator in Eq. (1). In this case, learning collapses to the scenario where all gradients solely come from the negatives: the model is simply pushing all data points away from each other, resulting in a more isotropic space. 7 0.0 0.5 1.0 cosine similarity 0 5 10 frequency positive/negative positives negatives (a) BERT-CLS 0.0 0.5 1.0 cosine similarity 0.0 2.5 5.0 7.5 frequency positive/negative positives negatives (b) BERT-mp 0.0 0.5 1.0 cosine similarity 0 2 4 frequency positive/negative positives negatives (c) BERT + Mirror Figure 7: Cosine similarity distribution over 1k sen- tence pairs sampled from QQP. Blue and orange mean positive and negative similarities, respectively. Fig. 7, we plot the distributions of pairwise co- sine similarities of BERT representations before (Figs. 7a and 7b) and after the +Mirror tuning (Fig. 7c). The overall cosine similarities (regard- less of positive/negative) are greatly reduced and the positives/negatives become easily separable. We also leverage a quantitative isotropy score (IS), proposed in prior work (Arora et al., 2016; Mu and Viswanath, 2018), and de\ufb01ned as follows: IS(V) = minc\u2208C \ufffd v\u2208V exp(c\u22a4v) maxc\u2208C \ufffd v\u2208V exp(c\u22a4v) (2) where V is the set of vectors,15 C is the set of all possible unit vectors (i.e., any c so that \u2225c\u2225 = 1) in the embedding space. In practice, C is approx- imated by the eigenvector set of V\u22a4V (V is the stacked embeddings of V). The larger the IS value, more isotropic an embedding space is (i.e., a per- fectly isotropic space obtains the IS score of 1). IS scores in Tab. 9 con\ufb01rm that the +Mirror \ufb01ne- tuning indeed makes the embedding space more isotropic. Interestingly, with both data augmenta- tion techniques switched off, a naive expectation is that IS will increase as the gradients now solely come from negative examples, pushing apart points in the space. However, we observe the increase of IS only for word-level representations. This hints at more complex dynamics between isotropy and gra-", "chunk_summary": "\u672c\u8282\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u63a7\u5236dropout\u6765\u63d0\u9ad8MLM\u7684\u6027\u80fd\uff0c\u4f46\u7ed3\u679c\u8868\u660e\u6807\u51c6\u7684dropout\u548c\u4e0d\u4f7f\u7528dropout\u6bd4\u63a7\u5236dropout\u66f4\u6709\u6548\u3002\u4f5c\u8005\u8ba4\u4e3aMirror-BERT\u7684\u6536\u76ca\u4e3b\u8981\u6765\u81ea\u4e8e\u5176\u5bf9\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u5f62\u72b6\u7684\u6539\u53d8\uff0cMirror-BERT\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u8ba9\u5d4c\u5165\u7a7a\u95f4\u66f4\u52a0\u5404\u5411\u540c\u6027\uff0c\u8fd9\u79cd\u6539\u53d8\u53ef\u4ee5\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5206\u5e03\u548cIsotropy\u5f97\u5206\u5f97\u5230\u4f53\u73b0\u3002\u7279\u522b\u662f\u5728\u4e0d\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u60c5\u51b5\u4e0b\uff0cISonly\u5bf9\u5355\u8bcd\u7ea7\u8868\u793a\u8fdb\u884c\u4e86\u589e\u52a0\uff0c\u8fd9\u8868\u660eisotropy\u548c\u68af\u5ea6\u4e4b\u95f4\u5b58\u5728\u66f4\u590d\u6742\u7684\u52a8\u6001\u3002"}, "29": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "8,9", "chunk_id": "12", "chunk_text": " Mu and Viswanath, 2018), and de\ufb01ned as follows: IS(V) = minc\u2208C \ufffd v\u2208V exp(c\u22a4v) maxc\u2208C \ufffd v\u2208V exp(c\u22a4v) (2) where V is the set of vectors,15 C is the set of all possible unit vectors (i.e., any c so that \u2225c\u2225 = 1) in the embedding space. In practice, C is approx- imated by the eigenvector set of V\u22a4V (V is the stacked embeddings of V). The larger the IS value, more isotropic an embedding space is (i.e., a per- fectly isotropic space obtains the IS score of 1). IS scores in Tab. 9 con\ufb01rm that the +Mirror \ufb01ne- tuning indeed makes the embedding space more isotropic. Interestingly, with both data augmenta- tion techniques switched off, a naive expectation is that IS will increase as the gradients now solely come from negative examples, pushing apart points in the space. However, we observe the increase of IS only for word-level representations. This hints at more complex dynamics between isotropy and gra- dients from positive and negative examples, where positives might also contribute to isotropy in some settings. We will examine these dynamics more in future work.16 Learning New Knowledge or Exposing Avail- able Knowledge? Running Mirror-BERT for more epochs, or with more data (see Fig. 5) does not re- 15V comprises the corresponding text data used for Mirror- BERT \ufb01ne-tuning (10k items for each task type). 16Introducing positive examples also naturally yields stronger task performance, as the original semantic space is better preserved. Gao et al. (2021) provide an insightful analysis on the balance of learning uniformity and alignment preservation, based on the method of Wang and Isola (2020). level\u2192 word phrase sentence BERT .169 .205 .222 + Mirror .599 .252 .265 + Mirror (w/o aug.) .825 .170 .255 Table 9: IS of word, phrase, and sentence-level models. model \u03c1 fastText .528 BERT-CLS .105 BERT-mp .267 + Mirror .556 + Mirror (random string) .393 + Mirror (random string, lr 5e-5) .481 Table 10: Running Mirror-BERT with a set of \u2018zero- semantics\u2019 random strings. Evaluation is conducted on Multi-SimLex (EN). 0 10k 20k 30k 40k 50k 60k 70k 80k 90k 100k word frequency rank in English Wikipedia 0 250 500 750 1000 1250 Count 0.55 0.56 0.57 0.58 Spearman's rho Multi-SimLex (en) word freq. distribution BERT+Mirror (w/ words of different freq.) Figure 8: Blue: words in Multi-SimLex (EN) follow a long-tail distribution. Yellow: BERT+Mirror trained with frequent words tend to perform better. sult in performance gains. This hints that, instead of gaining new knowledge from the \ufb01ne-tuning data, Mirror-BERT in fact \u2018rewires\u2019 existing knowl- edge in MLMs (Ben-Zaken et al., 2020). To fur- ther verify this, we run Mirror-BERT with random \u2018zero-semantics\u2019 words, generated by uniformly sampling English letters and digits, and evaluate on (EN) Multi-SimLex. Surprisingly, even these data can transform off-the-shelf MLMs into effective word encoders: we observe a large improvement over the base MLM in this extreme scenario, from \u03c1 =0.267 to 0.481 (Tab. 10). We did a similar experiment on the sentence-level and observed sim- ilar trends. However, we note that using the actual English texts for \ufb01ne-tuning still performs better as they are more \u2018in-domain\u2019 (with further evidence and discussions in the following paragraph). Selecting Examples for Fine-Tuning. Using raw text sequences from the end task should be the de- fault option for Mirror-BERT \ufb01ne-tuning since they are in-distribution by default, as semantic similarity models tend to underperform when faced with a domain shift (Zhang et al., 2020). In the general- domain STS tasks, we \ufb01nd that using sentences 8 extracted from the STS training set, Wikipedia ar- ticles, or NLI datasets all yield similar STS per- formance after running Mirror-BERT (though op- timal hyperparameters differ). However, porting BERT+Mirror trained on STS data to QNLI results in AUC drops from .674 to .665.", "chunk_summary": "\u672c\u6bb5\u843d\u4ecb\u7ecd\u4e86\u901a\u8fc7\u8ba1\u7b97\u5185\u79ef\u76f8\u4f3c\u6027\uff08IS\uff09\u8bc4\u4f30\u5d4c\u5165\u7a7a\u95f4\u5404\u5411\u540c\u6027\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u4f7f\u7528\u968f\u673a\u5b57\u7b26\u4e32\u548c\u4e0d\u540c\u6587\u672c\u6570\u636e\u8fdb\u884cMirror-BERT Fine-tuning\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528+Mirror Fine-tuning\u53ef\u4ee5\u4f7f\u5d4c\u5165\u7a7a\u95f4\u66f4\u52a0\u5404\u5411\u540c\u6027\uff0c\u4f46\u5728\u8bed\u4e49\u8868\u793a\u65b9\u9762\u7684\u6548\u679c\u5e76\u672a\u5f97\u5230\u660e\u663e\u63d0\u5347\uff0c\u8fd9\u8868\u660e+Mirror Fine-tuning\u66f4\u50cf\u662f\u5bf9MLMs\u4e2d\u5df2\u5b58\u5728\u7684\u77e5\u8bc6\u8fdb\u884c\u91cd\u7ec4\uff0c\u800c\u4e0d\u662f\u83b7\u53d6\u65b0\u7684\u77e5\u8bc6\u3002\u540c\u65f6\uff0c\u672c\u6bb5\u4e5f\u63d0\u5230\u4e86\u9009\u62e9Fine-tuning\u6837\u672c\u65f6\u5e94\u5f53\u4f18\u5148\u9009\u62e9\u4e0e\u6700\u7ec8\u4efb\u52a1\u76f8\u4f3c\u7684\u771f\u5b9e\u6587\u672c\u6570\u636e\uff0c\u4ee5\u51cf\u5c11\u9886\u57df\u504f\u79fb\u7684\u5f71\u54cd\u3002"}, "30": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "8,9", "chunk_id": "13", "chunk_text": " transform off-the-shelf MLMs into effective word encoders: we observe a large improvement over the base MLM in this extreme scenario, from \u03c1 =0.267 to 0.481 (Tab. 10). We did a similar experiment on the sentence-level and observed sim- ilar trends. However, we note that using the actual English texts for \ufb01ne-tuning still performs better as they are more \u2018in-domain\u2019 (with further evidence and discussions in the following paragraph). Selecting Examples for Fine-Tuning. Using raw text sequences from the end task should be the de- fault option for Mirror-BERT \ufb01ne-tuning since they are in-distribution by default, as semantic similarity models tend to underperform when faced with a domain shift (Zhang et al., 2020). In the general- domain STS tasks, we \ufb01nd that using sentences 8 extracted from the STS training set, Wikipedia ar- ticles, or NLI datasets all yield similar STS per- formance after running Mirror-BERT (though op- timal hyperparameters differ). However, porting BERT+Mirror trained on STS data to QNLI results in AUC drops from .674 to .665. This suggests that slight or large domain shifts do affect task perfor- mance, further corroborated by our \ufb01ndings from \ufb01ne-tuning with fully random strings (see before). Further, Fig. 8 shows a clear tendency that more frequent strings are more likely to yield good task performance. There, we split the 100k most frequent words from English Wikipedia into 10 equally sized \ufb01ne-tuning buckets of 10k examples each, and run +Mirror \ufb01ne-tuning on BERT with each bucket. In sum, using frequent in-domain examples seems to be the optimal choice. 5 Related Work Self-supervised text representations have a large body of literature. Here, due to space constraints, we provide a highly condensed summary of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most represen- tation models followed the distributional hypothe- sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- pora (Mikolov et al., 2013a,b; Pennington et al., 2014; Kiros et al., 2015; Hill et al., 2016; Lo- geswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- pothesis and formulates sentence embedding train- ing as a contrastive learning task where span pairs sampled from the same document are treated as pos- itive pairs. Very recently, there has been a growing interest in using individual raw sentences for self- supervised contrastive learning on top of PLMs. Wu et al. (2020) explore input augmentation techniques for sentence representation learning with contrastive objectives. However, they use it as an auxiliary loss during full-\ufb02edged MLM pre- training from scratch (Rethmeier and Augenstein, 2021). In contrast, our post-hoc approach offers a lightweight and fast self-supervised transformation from any pretrained MLM to a universal language encoder at lexical or sentence level. Carlsson et al. (2021) use two distinct models to produce two views of the same text, where we rely on a single model, that is, we propose to use dropout and random span masking within the same model to produce the two views, and demonstrate their synergistic effect. Our study also explores word-level and phrase-level representations and tasks, and to domain-specialised representations (e.g., for the BEL task). SimCSE (Gao et al., 2021), a work concurrent to ours, adopts the same contrastive loss as Mirror- BERT, and also indicates the importance of data augmentation through dropout. However, they do not investigate random span masking as data aug- mentation in the input space, and limit their model to general-domain English sentence representations only, effectively rendering SimCSE a special case of the Mirror-BERT framework. Other concurrent papers explore a similar idea, such as Self-Guided Contrastive Learning (Kim et al., 2021), ConSERT (Yan et al., 2021), and BSL (Zhang et al., 2021), inter alia. They all create two views of the same sentence for contrastive learning, with different strategies in feature extraction, data augmentation, model updating or choice of loss function. How- ever, they offer less complete empirical \ufb01ndings compared to our work: we additionally evaluate on (1) lexical-level tasks, (2) tasks in a specialised biomedical domain and (3) cross-lingual tasks. 6 Conclusion We proposed Mirror-BERT, a simple,", "chunk_summary": "\u8be5\u6bb5\u843d\u4ecb\u7ecd\u4e86\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u7684MLM\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u8bcd\u7f16\u7801\u5668\uff0c\u5e76\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u89c2\u5bdf\u5230\u57fa\u7840MLM\u7684\u5927\u5e45\u6539\u8fdb\u3002\u5728\u9009\u62e9\u7528\u4e8e\u5fae\u8c03\u7684\u793a\u4f8b\u65f6\uff0c\u4f7f\u7528\u6700\u9891\u7e41\u7684\u9886\u57df\u5185\u793a\u4f8b\u4f3c\u4e4e\u662f\u6700\u4f73\u9009\u62e9\u3002\u4e0e\u8be5\u9886\u57df\u7684\u5176\u4ed6\u76f8\u5173\u5de5\u4f5c\u8fdb\u884c\u6bd4\u8f83\u540e\uff0c\u4f5c\u8005\u8ba4\u4e3aMirror-BERT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u5feb\u901f\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u4ece\u9884\u8bad\u7ec3\u7684MLM\u5230\u901a\u7528\u8bed\u8a00\u7f16\u7801\u5668\u7684\u8f6c\u6362\u3002\u4f5c\u8005\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u5b9e\u9a8c\u8bc1\u660e\uff0cMirror-BERT\u5728\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u3001\u9886\u57df\u4e13\u4e1a\u5316\u8868\u793a\u548c\u4ea4\u53c9\u8bed\u8a00\u4efb\u52a1\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}, "31": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "9,10", "chunk_id": "14", "chunk_text": " representations and tasks, and to domain-specialised representations (e.g., for the BEL task). SimCSE (Gao et al., 2021), a work concurrent to ours, adopts the same contrastive loss as Mirror- BERT, and also indicates the importance of data augmentation through dropout. However, they do not investigate random span masking as data aug- mentation in the input space, and limit their model to general-domain English sentence representations only, effectively rendering SimCSE a special case of the Mirror-BERT framework. Other concurrent papers explore a similar idea, such as Self-Guided Contrastive Learning (Kim et al., 2021), ConSERT (Yan et al., 2021), and BSL (Zhang et al., 2021), inter alia. They all create two views of the same sentence for contrastive learning, with different strategies in feature extraction, data augmentation, model updating or choice of loss function. How- ever, they offer less complete empirical \ufb01ndings compared to our work: we additionally evaluate on (1) lexical-level tasks, (2) tasks in a specialised biomedical domain and (3) cross-lingual tasks. 6 Conclusion We proposed Mirror-BERT, a simple, fast, self- supervised, and highly effective approach that trans- forms large pretrained masked language models (MLMs) into universal lexical and sentence en- coders within a minute, and without any external supervision. Mirror-BERT, based on simple un- supervised data augmentation techniques, demon- strates surprisingly strong performance in (word- level and sentence-level) semantic similarity tasks, as well as on biomedical entity linking. The large gains over base MLMs are observed for different languages with different scripts, and across diverse domains. Moreover, we dissected and analysed the main causes behind Mirror-BERT\u2019s ef\ufb01cacy. Acknowledgements We thank the reviewers and the AC for their consid- erate comments. We also thank the LTL members and Xun Wang for insightful feedback. FL is sup- ported by Grace & Thomas C.H. Chan Cambridge Scholarship. AK and IV are supported by the ERC Grant LEXICAL (no. 648909) and the ERC PoC Grant MultiConvAI (no. 957356). NC kindly ac- knowledges grant-in-aid funding from ESRC (grant number ES/T012277/1). 9 References Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I\u00f1igo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 task 2: Semantic tex- tual similarity, English, Spanish and pilot on inter- pretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252\u2013263, Denver, Colorado. Association for Computational Linguistics. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81\u201391, Dublin, Ireland. As- sociation for Computational Linguistics. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evalua- tion (SemEval-2016), pages 497\u2013511, San Diego, California. Association for Computational Linguis- tics. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Compu- tational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385\u2013 393, Montr\u00e9al, Canada. Association for Computa- tional Linguistics. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. *SEM 2013 shared task", "chunk_summary": "\u8be5\u6bb5\u843d\u4ecb\u7ecd\u4e86\u4e0eMirror-BERT\u5e76\u53d1\u7684\u4e00\u4e9b\u65b9\u6cd5\uff0c\u5982SimCSE\u3001Self-Guided Contrastive Learning\u3001ConSERT\u548cBSL\u7b49\uff0c\u5b83\u4eec\u90fd\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u521b\u5efa\u540c\u4e00\u53e5\u5b50\u7684\u4e24\u4e2a\u89c6\u56fe\uff0c\u5e76\u5177\u6709\u4e0d\u540c\u7684\u7b56\u7565\u6765\u63d0\u53d6\u7279\u5f81\u3001\u6570\u636e\u589e\u5f3a\u3001\u6a21\u578b\u66f4\u65b0\u6216\u9009\u62e9\u635f\u5931\u51fd\u6570\u3002\u4f46\u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u5b83\u4eec\u63d0\u4f9b\u7684\u7ecf\u9a8c\u7ed3\u679c\u4e0d\u591f\u5b8c\u6574\u3002Mirror-BERT\u6839\u636e\u7b80\u5355\u7684\u65e0\u76d1\u7763\u6570\u636e\u589e\u5f3a\u6280\u5de7\uff0c\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u8a00\u3001\u4e0d\u540c\u811a\u672c\u548c\u4e0d\u540c\u9886\u57df\u4e2d\u53d6\u5f97\u4e86\u60ca\u4eba\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u6bb5\u843d\u8fd8\u5bf9Mirror-BERT\u7684\u6709\u6548\u6027\u8fdb\u884c\u4e86\u5256\u6790\u548c\u5206\u6790\u3002"}, "32": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "10", "chunk_id": "15", "chunk_text": "itor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evalua- tion (SemEval-2016), pages 497\u2013511, San Diego, California. Association for Computational Linguis- tics. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Compu- tational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385\u2013 393, Montr\u00e9al, Canada. Association for Computa- tional Linguistics. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Seman- tics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32\u201343, Atlanta, Georgia, USA. As- sociation for Computational Linguistics. Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2016. A latent variable model approach to PMI-based word embeddings. Transac- tions of the Association for Computational Linguis- tics, 4:385\u2013399. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 789\u2013798, Melbourne, Australia. As- sociation for Computational Linguistics. Marco Basaldella, Fangyu Liu, Ehsan Shareghi, and Nigel Collier. 2020. COMETA: A corpus for med- ical entity linking in the social media. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3122\u20133137, Online. Association for Computational Linguistics. Elad Ben-Zaken, Shauli Ravfogel, and Yoav Gold- berg. 2020. BitFit: Simple parameter-ef\ufb01cient \ufb01ne-tuningfor transformer-based masked language- model. Technical Report. Olivier Bodenreider. 2004. The uni\ufb01ed medical lan- guage system (umls): integrating biomedical termi- nology. Nucleic acids research, 32(suppl_1):D267\u2013 D270. Xavier Bouthillier, Kishore Konda, Pascal Vincent, and Roland Memisevic. 2015. Dropout as data augmen- tation. ArXiv preprint, abs/1506.08700. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Compu- tational Linguistics. Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. 2021. Isotropy in the contextual embed- ding space: Clusters and manifolds. In 9th Inter- national Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Fredrik Carlsson, Amaru Cuba Gyllensten, Evan- gelia Gogoulou, Erik Ylip\u00e4\u00e4 Hellqvist, and Magnus Sahlgren. 2021. Semantic re-tuning with contrastive tension. In 9th International Conference on Learn- ing Representations, ICLR 2021, Virtual Event, Aus- tria, May 3-7, 2021. OpenReview.net. Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez- Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval", "chunk_summary": "\u8fd9\u6bb5\u6587\u7ae0\u662f\u53c2\u8003\u6587\u732e\u5217\u8868\uff0c\u5217\u51fa\u4e86\u76f8\u5173\u7814\u7a76\u7684\u6587\u732e\u5f15\u7528\u3002\u8fd9\u4e9b\u6587\u732e\u5305\u62ec\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u3001\u8bcd\u5411\u91cf\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u5176\u4e2d\u6d89\u53ca\u7684\u6587\u7ae0\u7684\u5e74\u4efd\u4ece2004\u5e74\u52302021\u5e74\u4e0d\u7b49\u3002"}, "33": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "10,11", "chunk_id": "16", "chunk_text": " pages 632\u2013642, Lisbon, Portugal. Association for Compu- tational Linguistics. Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. 2021. Isotropy in the contextual embed- ding space: Clusters and manifolds. In 9th Inter- national Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Fredrik Carlsson, Amaru Cuba Gyllensten, Evan- gelia Gogoulou, Erik Ylip\u00e4\u00e4 Hellqvist, and Magnus Sahlgren. 2021. Semantic re-tuning with contrastive tension. In 9th International Conference on Learn- ing Representations, ICLR 2021, Virtual Event, Aus- tria, May 3-7, 2021. OpenReview.net. Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez- Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancouver, Canada. Association for Computational Linguistics. Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder for English. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing: System Demonstrations, pages 169\u2013174, Brussels, Belgium. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference 10 of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Rezarta Islamaj Do\u02d8gan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for dis- ease name recognition and concept normalization. Journal of biomedical informatics, 47:1\u201310. Maud Ehrmann, Francesco Cecconi, Daniele Vannella, John Philip McCrae, Philipp Cimiano, and Roberto Navigli. 2014. Representing multilingual data as linked data: the case of BabelNet 2.0. In Proceed- ings of the Ninth International Conference on Lan- guage Resources and Evaluation (LREC\u201914), pages 401\u2013408, Reykjavik, Iceland. European Language Resources Association (ELRA). Kawin Ethayarajh. 2019. How contextual are contex- tualized word representations? comparing the geom- etry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 55\u201365, Hong Kong, China. Association for Computational Linguistics. Christiane Fellbaum. 1998. WordNet. MIT Press. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Language- agnostic bert sentence embedding. ArXiv preprint, abs/2007.01852. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. ArXiv preprint, abs/2104.08821. John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers), pages 879\u2013895, Online. Association for Computational Linguistics. Goran Glava\u0161, Robert Litschko, Sebastian Ruder, and Ivan Vuli\u00b4c. 2019. How to (properly) evaluate cross- lingual word embeddings: On strong baselines, com- parative analyses, and some misconceptions. In Pro- ceedings of the 57th Annual", "chunk_summary": "\u672c\u6bb5\u4e3b\u8981\u5217\u4e3e\u4e86\u4e00\u7cfb\u5217\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u76f8\u5173\u7684\u6587\u7ae0\u548c\u5de5\u5177\uff0c\u4f8b\u5982SemEval-2017\u4efb\u52a11\u3001BERT\u6a21\u578b\u3001WordNet\u7b49\u7b49\u3002\u5176\u4e2d\uff0c\u8fd8\u63d0\u5230\u4e86\u4e00\u4e9b\u6700\u65b0\u7684\u7814\u7a76\u6210\u679c\uff0c\u4f8b\u5982SimCSE\u548cDeCLUTR\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u4e00\u4e9b\u5de5\u5177\u548c\u8bed\u6599\u5e93\uff0c\u5982NCBI Disease Corpus\u548cBabelNet 2.0\u3002"}, "34": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "11", "chunk_id": "17", "chunk_text": " Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Language- agnostic bert sentence embedding. ArXiv preprint, abs/2007.01852. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. ArXiv preprint, abs/2104.08821. John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers), pages 879\u2013895, Online. Association for Computational Linguistics. Goran Glava\u0161, Robert Litschko, Sebastian Ruder, and Ivan Vuli\u00b4c. 2019. How to (properly) evaluate cross- lingual word embeddings: On strong baselines, com- parative analyses, and some misconceptions. In Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics, pages 710\u2013721, Florence, Italy. Association for Computational Lin- guistics. Goran Glava\u0161 and Ivan Vuli\u00b4c. 2021. Is supervised syn- tactic parsing bene\ufb01cial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3090\u20133104, Online. Association for Computational Linguistics. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domain- speci\ufb01c language model pretraining for biomedi- cal natural language processing. ArXiv preprint, abs/2007.15779. Zellig S Harris. 1954. Distributional structure. Word, 10(2-3):146\u2013162. Matthew Henderson, Ivan Vuli\u00b4c, Daniela Gerz, I\u00f1igo Casanueva, Pawe\u0142 Budzianowski, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrk\u0161i\u00b4c, and Pei-Hao Su. 2019. Training neural re- sponse selection for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5392\u20135404, Florence, Italy. Association for Compu- tational Linguistics. Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. 2020. Augmix: A simple data process- ing method to improve robustness and uncertainty. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sen- tences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 1367\u20131377, San Diego, California. Association for Computational Linguistics. Felix Hill, Roi Reichart, and Anna Korhonen. 2015. SimLex-999: Evaluating semantic models with (gen- uine) similarity estimation. Computational Linguis- tics, 41(4):665\u2013695. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architec- tures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Confer- ence on Learning Representations, ICLR 2020, Ad- dis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021. Self-guided contrastive learning for BERT sentence representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2528\u20132540, Online. Association for Computational Linguistics. Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Tor", "chunk_summary": "\u8be5\u6bb5\u843d\u5217\u4e3e\u4e86\u4e00\u4e9b\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6587\u672c\u8868\u793a\u76f8\u5173\u7684\u7814\u7a76\u3002\u5176\u4e2d\u5305\u62ec\u4e0e\u8bed\u8a00\u65e0\u5173\u7684BERT\u53e5\u5b50\u5d4c\u5165\uff0c\u7b80\u5355\u5bf9\u6bd4\u5b66\u4e60\u7684\u53e5\u5b50\u5d4c\u5165\u6a21\u578bSimCSE\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u6587\u672c\u8868\u793a\u6a21\u578bDeCLUTR\uff0c\u8de8\u8bed\u8a00\u8bcd\u5d4c\u5165\u7684\u8bc4\u4f30\u65b9\u6cd5\u7814\u7a76\uff0c\u4ee5\u53ca\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u53d7\u76d1\u7763\u7684\u53e5\u6cd5\u5206\u6790\u662f\u5426\u6709\u76ca\u7684\u5b9e\u8bc1\u7814\u7a76\u7b49\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u57df\u7279\u5b9a\u7684\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u795e\u7ecf\u54cd\u5e94\u9009\u62e9\u7684\u8bad\u7ec3\u3001\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5Augmix\u3001\u4ece\u65e0\u6807\u8bb0\u6570\u636e\u4e2d\u5b66\u4e60\u53e5\u5b50\u7684\u5206\u5e03\u5f0f\u8868\u793a\u3001\u7528\u771f\u5b9e\u76f8\u4f3c\u6027\u4f30\u8ba1\u6765\u8bc4\u4f30\u8bed\u4e49\u6a21\u578b\u7684SimLex-999\u3001\u5feb\u901f\u51c6\u786e\u7684\u591a\u53e5\u5b50\u8bc4\u5206\u9884\u8bad\u7ec3\u7b56\u7565Poly-encoders\u3001\u81ea\u6211\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u7684BERT\u53e5\u5b50\u8868\u793a\u7b49\u3002"}, "35": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "11,12", "chunk_id": "18", "chunk_text": " and Anna Korhonen. 2015. SimLex-999: Evaluating semantic models with (gen- uine) similarity estimation. Computational Linguis- tics, 41(4):665\u2013695. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architec- tures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Confer- ence on Learning Representations, ICLR 2020, Ad- dis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021. Self-guided contrastive learning for BERT sentence representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2528\u20132540, Online. Association for Computational Linguistics. Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. 2015. Skip-thought vec- tors. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Informa- tion Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3294\u20133302. 11 Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 4365\u20134374, Hong Kong, China. Association for Computational Linguistics. Guillaume Lample, Alexis Conneau, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2018. Word translation without parallel data. In 6th Inter- national Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenRe- view.net. Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020. On the sentence embeddings from pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9119\u20139130, Online. Association for Computa- tional Linguistics. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sci- aky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database, 2016. Nut Limsopatham and Nigel Collier. 2016. Normalis- ing medical concepts in social media texts by learn- ing semantic representation. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1014\u20131023, Berlin, Germany. Association for Com- putational Linguistics. Robert Litschko, Ivan Vuli\u00b4c, Simone Paolo Ponzetto, and Goran Glava\u0161. 2021. Evaluating multilin- gual text encoders for unsupervised cross-lingual re- trieval. In Proceedings of 43rd European Confer- ence on Information Retrieval (ECIR 2021), pages 342\u2013358. Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. 2021. Self-alignment pretraining for biomedical entity representations. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 4228\u20134238, Online. Association for Compu- tational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. ArXiv preprint, abs/1907.11692. Lajanugen Logeswaran and Honglak Lee. 2018. An ef\ufb01cient", "chunk_summary": "\u8be5\u6bb5\u843d\u5217\u4e3e\u4e86\u51e0\u7bc7\u76f8\u5173\u7684\u7814\u7a76\u8bba\u6587\uff0c\u8fd9\u4e9b\u8bba\u6587\u63a2\u8ba8\u4e86\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u8bdd\u9898\uff0c\u5305\u62ec\u8bed\u4e49\u6a21\u578b\u8bc4\u4f30\u3001\u591a\u53e5\u5b50\u8bc4\u5206\u3001BERT\u53e5\u5b50\u8868\u793a\u3001\u65e0\u76d1\u7763\u8de8\u8bed\u8a00\u68c0\u7d22\u3001\u533b\u5b66\u5b9e\u4f53\u8868\u793a\u7b49\u3002\u8fd9\u4e9b\u7814\u7a76\u8bba\u6587\u7684\u5185\u5bb9\u53ef\u4ee5\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e00\u4e9b\u53c2\u8003\u3002"}, "36": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "12,13", "chunk_id": "19", "chunk_text": " Goran Glava\u0161. 2021. Evaluating multilin- gual text encoders for unsupervised cross-lingual re- trieval. In Proceedings of 43rd European Confer- ence on Information Retrieval (ECIR 2021), pages 342\u2013358. Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. 2021. Self-alignment pretraining for biomedical entity representations. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 4228\u20134238, Online. Association for Compu- tational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. ArXiv preprint, abs/1907.11692. Lajanugen Logeswaran and Honglak Lee. 2018. An ef\ufb01cient framework for learning sentence represen- tations. In 6th International Conference on Learn- ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Ilya Loshchilov and Frank Hutter. 2019. Decou- pled weight decay regularization. In 7th Inter- national Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zampar- elli. 2014. A SICK cure for the evaluation of compo- sitional distributional semantic models. In Proceed- ings of the Ninth International Conference on Lan- guage Resources and Evaluation (LREC\u201914), pages 216\u2013223, Reykjavik, Iceland. European Language Resources Association (ELRA). Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti- wary, Paul Bennett, Jiawei Han, and Xia Song. 2021. Coco-lm: Correcting and contrasting text sequences for language model pretraining. ArXiv preprint, abs/2102.08473. Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In Ad- vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8- 14, 2019, Vancouver, BC, Canada, pages 14014\u2013 14024. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Ef\ufb01cient estimation of word representations in vector space. ArXiv preprint, abs/1301.3781. Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Ad- vances in pre-training distributed word representa- tions. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Tom\u00e1s Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed rep- resentations of words and phrases and their com- positionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Pro- ceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111\u2013 3119. Jiaqi Mu and Pramod Viswanath. 2018. All-but-the- top: Simple and effective postprocessing for word representations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive pre- dictive coding. ArXiv preprint, abs/1807.03748. 12 Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch.", "chunk_summary": "\u8fd9\u4e2a\u6bb5\u843d\u5217\u51fa\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u8bed\u8a00\u6a21\u578b\u7684\u6587\u7ae0\uff0c\u5305\u62ecGoran Glava\u0161\u5173\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u6587\u672c\u7f16\u7801\u5668\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u7684\u6587\u7ae0\uff0c\u4ee5\u53ca\u9488\u5bf9\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u8868\u793a\u7684\u81ea\u6211\u5bf9\u9f50\u9884\u8bad\u7ec3\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u62ec\u4e86Roberta\u548cAn Efficient Framework for Learning Sentence Representations\u7b49\u6587\u7ae0\u3002\u8be5\u6bb5\u843d\u63d0\u5230\u7684\u5176\u4ed6\u8bba\u6587\u5305\u62ecDecoupled Weight Decay Regularization\u3001A SICK Cure for the Evaluation of Compositional Distributional Semantic Models\u3001Correcting and Contrasting Text Sequences for Language Model Pretraining\u7b49\u3002"}, "37": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "12,13", "chunk_id": "20", "chunk_text": " Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed rep- resentations of words and phrases and their com- positionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Pro- ceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111\u2013 3119. Jiaqi Mu and Pramod Viswanath. 2018. All-but-the- top: Simple and effective postprocessing for word representations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive pre- dictive coding. ArXiv preprint, abs/1807.03748. 12 Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2015. PPDB 2.0: Better paraphrase ranking, \ufb01ne- grained entailment relations, word embeddings, and style classi\ufb01cation. In Proceedings of the 53rd An- nual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer- ence on Natural Language Processing (Volume 2: Short Papers), pages 425\u2013430, Beijing, China. As- sociation for Computational Linguistics. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar. Association for Computational Linguistics. Mohammad Taher Pilehvar, Dimitri Kartsaklis, Vic- tor Prokhorov, and Nigel Collier. 2018. Card-660: Cambridge rare word dataset - a reliable benchmark for infrequent word representation models. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1391\u2013 1401, Brussels, Belgium. Association for Computa- tional Linguistics. Sara Rajaee and Mohammad Taher Pilehvar. 2021. A cluster-based approach for improving isotropy in contextual embedding space. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 2: Short Papers), pages 575\u2013584, Online. As- sociation for Computational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics. Nils Rethmeier and Isabelle Augenstein. 2021. A primer on contrastive pretraining in language pro- cessing: Methods, lessons learned and perspectives. ArXiv preprint, abs/2102.12982. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Associ- ation for Computational Linguistics, 8:842\u2013866. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm\u00e1n. 2021. Wiki- Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351\u20131361, Online. Association for Computational Linguistics. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way", "chunk_summary": "\u8be5\u6bb5\u843d\u5217\u4e3e\u4e86\u8bb8\u591a\u6709\u5173\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u7684\u6587\u732e\u3002\u5176\u4e2d\u5305\u62ec\u6709\u5173\u5206\u5e03\u5f0f\u8868\u793a\u7684\u7814\u7a76\u3001\u5173\u4e8e\u5355\u8bcd\u8868\u793a\u6a21\u578b\u7684\u540e\u5904\u7406\u65b9\u6cd5\u3001\u5bf9\u6bd4\u9884\u6d4b\u7f16\u7801\u7684\u8868\u793a\u5b66\u4e60\u3001\u7528\u4e8e\u7f55\u89c1\u8bcd\u8868\u793a\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3001\u7528\u4e8e\u63d0\u9ad8\u4e0a\u4e0b\u6587\u5d4c\u5165\u7a7a\u95f4\u5747\u8861\u6027\u7684\u805a\u7c7b\u65b9\u6cd5\u3001\u7528\u4e8e\u673a\u5668\u7406\u89e3\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u5b6a\u751fBERT\u7f51\u7edc\u7684\u53e5\u5b50\u5d4c\u5165\u3001\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3001BERT\u5de5\u4f5c\u539f\u7406\u7684\u63cf\u8ff0\u3001\u4ee5\u53ca\u6316\u6398\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u96c6\u7684\u7814\u7a76\u3002"}, "38": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "13,14", "chunk_id": "21", "chunk_text": " pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics. Nils Rethmeier and Isabelle Augenstein. 2021. A primer on contrastive pretraining in language pro- cessing: Methods, lessons learned and perspectives. ArXiv preprint, abs/2102.12982. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Associ- ation for Computational Linguistics, 8:842\u2013866. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm\u00e1n. 2021. Wiki- Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351\u20131361, Online. Association for Computational Linguistics. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research, 15(1):1929\u20131958. Jianlin Su. 2020. Simbert: Integrating retrieval and generation into bert. Technical report. Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. 2021. Whitening sentence representations for bet- ter semantics and faster retrieval. ArXiv preprint, abs/2103.15316. Nandan Thakur, Nils Reimers, Johannes Daxen- berger, and Iryna Gurevych. 2021. Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 296\u2013310, Online. Association for Computa- tional Linguistics. Elena Voita, David Talbot, Fedor Moiseev, Rico Sen- nrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lift- ing, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 5797\u20135808, Florence, Italy. Association for Computational Linguistics. Ivan Vuli\u00b4c, Simon Baker, Edoardo Maria Ponti, Ulla Petti, Ira Leviant, Kelly Wing, Olga Majewska, Eden Bar, Matt Malone, Thierry Poibeau, Roi Reichart, and Anna Korhonen. 2020a. Multi-SimLex: A large- scale evaluation of multilingual and crosslingual lex- ical semantic similarity. Computational Linguistics, 46(4):847\u2013897. Ivan Vuli\u00b4c, Edoardo Maria Ponti, Anna Korhonen, and Goran Glava\u0161. 2021. LexFit: Lexical \ufb01ne-tuning of pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 5269\u20135283, Online. As- sociation for Computational Linguistics. Ivan Vuli\u00b4c, Edoardo Maria Ponti, Robert Litschko, Goran Glava\u0161, and Anna Korhonen. 2020b. Prob- ing pretrained language models for lexical semantics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7222\u20137240, Online. Association for Computa- tional Linguistics. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer 13 Levy, and Samuel R. Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Infor- mation Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261\u20133275. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In 7th International Conference on Learning Representa- tions,", "chunk_summary": "\u8fd9\u6bb5\u6587\u732e\u5f15\u7528\u4e86\u591a\u7bc7\u5173\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u8bba\u6587\uff0c\u5305\u62ec\u4e86\u5bf9BERT\u3001SimBERT\u3001Dropout\u7b49\u7b97\u6cd5\u7684\u8ba8\u8bba\u3002\u540c\u65f6\u4e5f\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u5173\u4e8e\u6570\u636e\u589e\u5f3a\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u7814\u7a76\uff0c\u4ee5\u53ca\u4e00\u4e9b\u65b0\u7684\u591a\u8bed\u79cd\u6587\u672c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5b9e\u9a8c\u7684\u6210\u679c\u3002\u6700\u540e\u8fd8\u63d0\u5230\u4e86\u4e00\u4e9b\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u65b9\u6cd5\uff0c\u5e76\u5f15\u7528\u4e86\u4e00\u4e9bNLP\u7684\u7ade\u8d5b\u4efb\u52a1\uff0c\u4f8b\u5982GLUE\u548cSuperGLUE\u3002"}, "39": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "13,14,15", "chunk_id": "22", "chunk_text": ", Robert Litschko, Goran Glava\u0161, and Anna Korhonen. 2020b. Prob- ing pretrained language models for lexical semantics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7222\u20137240, Online. Association for Computa- tional Linguistics. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer 13 Levy, and Samuel R. Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Infor- mation Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261\u20133275. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In 7th International Conference on Learning Representa- tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Tongzhou Wang and Phillip Isola. 2020. Understand- ing contrastive representation learning through align- ment and uniformity on the hypersphere. In Pro- ceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Ma- chine Learning Research, pages 9929\u20139939. PMLR. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguis- tics. Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. Clear: Con- trastive learning for sentence representation. ArXiv preprint, abs/2012.15466. Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. 2021. ConSERT: A con- trastive framework for self-supervised sentence rep- resentation transfer. In Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 5065\u20135075, Online. Associa- tion for Computational Linguistics. Yan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and Haizhou Li. 2021. Bootstrapped unsupervised sen- tence representation learning. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 5168\u20135180, Online. As- sociation for Computational Linguistics. Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. 2020. An unsupervised sentence embedding method by mutual information maxi- mization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1601\u20131610, Online. Associa- tion for Computational Linguistics. Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, and Ke Xu. 2020. Scheduled DropHead: A regu- larization method for transformer models. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2020, pages 1971\u20131980, Online. As- sociation for Computational Linguistics. 14 A Language Codes EN English ES Spanish FR French PL Polish ET Estonian FI Finnish RU Russian TR Turkish IT Italian ZH Chinese AR Arabic HE Hebrew Table 11: Language abbreviations used in the paper. B Additional Training Details Most Frequent 10k/100k Words by Language. The most frequent 10k words in each language were selected based on the following list: https://github.com/oprogramador/ most-common-words-by-language. The most frequent 100k English words in Wikipedia can be found here: https://gist.github.com", "chunk_summary": "\u8fd9\u4e00\u8282\u5217\u4e3e\u4e86\u4e00\u4e9b\u76f8\u5173\u7684\u8bba\u6587\u3002\u8fd9\u4e9b\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u63a2\u7d22\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u6216\u662f\u63d0\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bed\u8a00\u8868\u5f81\u3002\u5176\u4e2d\u4e00\u4e9b\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e9b\u4e0e\u672c\u6587\u76f8\u4f3c\u7684\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u6bd4\u5982\u4f7f\u7528\u903b\u8f91\u586b\u7a7a\u4efb\u52a1\u7b49\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u4e00\u4e9b\u8bba\u6587\u63d0\u51fa\u4e86\u5176\u4ed6\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u6bd4\u5982\u5bf9\u6bd4\u5b66\u4e60\u3002\u6587\u7ae0\u6700\u540e\u9644\u4e0a\u4e86\u4e00\u4e9b\u8bed\u8a00\u7684\u7b80\u79f0\u548c\u4e00\u4e9b\u8bad\u7ec3\u7ec6\u8282\u3002"}, "40": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "14,15,16", "chunk_id": "23", "chunk_text": "0. An unsupervised sentence embedding method by mutual information maxi- mization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1601\u20131610, Online. Associa- tion for Computational Linguistics. Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, and Ke Xu. 2020. Scheduled DropHead: A regu- larization method for transformer models. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2020, pages 1971\u20131980, Online. As- sociation for Computational Linguistics. 14 A Language Codes EN English ES Spanish FR French PL Polish ET Estonian FI Finnish RU Russian TR Turkish IT Italian ZH Chinese AR Arabic HE Hebrew Table 11: Language abbreviations used in the paper. B Additional Training Details Most Frequent 10k/100k Words by Language. The most frequent 10k words in each language were selected based on the following list: https://github.com/oprogramador/ most-common-words-by-language. The most frequent 100k English words in Wikipedia can be found here: https://gist.github.com/h3xx/ 1976236. [CLS] or Mean-Pooling? For MLMs, the con- sensus in the community, also validated by our own experiments, is that mean-pooling performs better than using [CLS] as the \ufb01nal output rep- resentation. However, for Mirror-BERT models, we found [CLS] (before pooling) generally per- forms better than mean-pooling. The exception is BERT on sentence-level tasks, where we found mean-pooling performs better than [CLS]. In sum, sentence-level BERT+Mirror models are \ufb01ne- tuned and tested with mean-pooling while all other Mirror-BERT models are \ufb01ne-tuned and tested with [CLS]. We also tried representations after the pooling layer, but found no improvement. Training Stability. All task results are reported as averages over three runs with different random seeds (if applicable). In general, \ufb01ne-tuning is very stable and the \ufb02uctuations with different random seeds are very small. For instance, on the sentence- level task STS, the standard deviation is < 0.002. On word-level, standard deviation is a bit higher, but is generally < 0.005. Note that the randomly dropout rate\u2192 0.05 0.1\u2217 0.2 0.3 0.4 BERT + Mirror .740 .743 .748 .748 .731 RoBERTa + Mirror .755 .753 .737 .694 .677 Table 12: Average \u03c1 across STS tasks with different dropout rates. \u2217 default dropout rate for all models in other experiments. random span mask rate\u2192 2 5\u2217 10 15 20 BERT + Mirror .741 .743 .720 .690 .616 RoBERTa + Mirror .750 .753 .757 .743 .706 Table 13: Avg. \u03c1 across STS tasks with different ran- dom span masking rates. \u2217 default mask rates for all models in other experiments. sampled training sets are \ufb01xed across all experi- ments, and changing the training corpus for each run might lead to larger \ufb02uctuations. C Details of Mirror-BERT Trained on Random Strings We pointed out in the main text that BERT+Mirror trained on random strings can outperform MLMs by large margins. With standard training con\ufb01gu- rations, BERT improves from .267 (BERT-mp) to .393 with +Mirror. When learning rate is increased to 5e-5, the MLM \ufb01ne-tuned with random strings performs only around 0.07 lower than the standard BERT+Mirror model \ufb01ne-tuned with the 10k most frequent English words. D Dropout and Random Span Masking Rates Dropout Rate (Tab. 12). The performance trends conditioned on dropout rates are generally the same across word-level, phrase-level and sentence-level \ufb01ne-tuning. Here, we use the STS task as a ref- erence point. BERT prefers larger dropouts (0.2 & 0.3) and is generally more robust. RoBERTa prefers a smaller dropout rate (0.05) and its perfor- mance decreases more steeply with the increase of the dropout rate. For simplicity, as mentioned in the main paper, we use the default value of 0.1 as the dropout rate for all models. Random Span Masking Rate (Tab. 13). Interest- ingly, the opposite holds for random span masking: RoBERTa is more robust to larger masking rates k, and is much more robust than BERT to this hyper- parameter. 15 level\u2192 model", "chunk_summary": "\u8be5\u6bb5\u843d\u4e3b\u8981\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u7ec6\u8282\u548c\u914d\u7f6e\u4fe1\u606f\u3002\u9996\u5148\u5217\u51fa\u4e86\u672c\u6587\u4f7f\u7528\u7684\u8bed\u8a00\u4ee3\u7801\u548c\u6bcf\u79cd\u8bed\u8a00\u7684\u6700\u5e38\u7528\u5355\u8bcd\u4fe1\u606f\u3002\u5176\u6b21\u8ba8\u8bba\u4e86\u5728MLMs\u4e2d\u4f7f\u7528[CLS]\u548cmean-pooling\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa\u8868\u793a\u7684\u6548\u679c\uff0c\u4ee5\u53ca\u5728Mirror-BERT\u6a21\u578b\u4e2d\uff0c[CLS]\uff08\u672a\u8fdb\u884c\u6c60\u5316\u7684\u60c5\u51b5\u4e0b\uff09\u901a\u5e38\u6bd4mean-pooling\u6027\u80fd\u66f4\u597d\u3002\u5728\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u65b9\u9762\uff0c\u6240\u6709\u4efb\u52a1\u7684\u7ed3\u679c\u90fd\u662f\u901a\u8fc7\u4e09\u6b21\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u7684\u5e73\u5747\u503c\u8ba1\u7b97\u5f97\u51fa\u7684\u3002\u6700\u540e\u8ba8\u8bba\u4e86\u4e00\u4e9b\u8d85\u53c2\u6570\uff0c\u5305\u62ecdropout\u548crandom span masking rates\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684BERT\u548cRoBERTa\u6a21\u578b\u5728\u4e0d\u540cdropout\u548cmasking rates\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}, "41": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "15,16", "chunk_id": "24", "chunk_text": ") to .393 with +Mirror. When learning rate is increased to 5e-5, the MLM \ufb01ne-tuned with random strings performs only around 0.07 lower than the standard BERT+Mirror model \ufb01ne-tuned with the 10k most frequent English words. D Dropout and Random Span Masking Rates Dropout Rate (Tab. 12). The performance trends conditioned on dropout rates are generally the same across word-level, phrase-level and sentence-level \ufb01ne-tuning. Here, we use the STS task as a ref- erence point. BERT prefers larger dropouts (0.2 & 0.3) and is generally more robust. RoBERTa prefers a smaller dropout rate (0.05) and its perfor- mance decreases more steeply with the increase of the dropout rate. For simplicity, as mentioned in the main paper, we use the default value of 0.1 as the dropout rate for all models. Random Span Masking Rate (Tab. 13). Interest- ingly, the opposite holds for random span masking: RoBERTa is more robust to larger masking rates k, and is much more robust than BERT to this hyper- parameter. 15 level\u2192 model\u2193 word phrase sentence MVN IS MVN IS MVN IS BERT-CLS 13.79 .043 12.8 .028 12.73 .062 BERT-mp 7.89 .169 6.82 .205 6.93 .222 + Mirror 2.11 .599 5.91 .252 5.57 .265 + Mirror (w/o aug.) 0.71 .825 8.16 .170 5.75 .255 Table 14: Full table for MVN and IS of word-, phrase- , and sentence-level models. Higher is better, that is, more isotropic with IS, while the opposite holds for MVN (lower scores mean more isotropic representa- tion spaces). E Mean-Vector l2-Norm (MVN) To supplement the quantitative evidence already suggested by the Isotropy Score (IS) in the main paper, we additionally compute the mean-vector l2- norm (MVN) of embeddings. In the word embed- ding literature, mean-centering has been a widely studied post-processing technique for inducing bet- ter semantic representations. Mu and Viswanath (2018) point out that mean-centering is essentially increasing spatial isotropy by shifting the centre of the space to the region where actual data points reside in. Given a set of representation vectors V, we de\ufb01ne MVN as follows: MVN(V) = \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd v\u2208V v |V| \ufffd\ufffd\ufffd\ufffd\ufffd 2 . (3) The lower MVN is, the more mean-centered an em- bedding is. As shown in Tab. 14, MVN aligns with the trends observed with IS. This further con\ufb01rms our intuition that +Mirror tuning makes the space more isotropic and shifts the centre of space close to the centre of data points. Very recently, Cai et al. (2021) de\ufb01ned more metrics to measure spatial isotropy. Rajaee and Pilehvar (2021) also used Eq. (2) for analysing sentence embedding\u2019s isotropiness. F Evaluation Dataset Details All datasets used and links to download them can be found in the code repository provided. The Russian STS dataset is provided by https://github.com/deepmipt/ deepPavlovEval. The Quora Ques- tion Pair (QQP) dataset is downloaded at https://www.kaggle.com/c/ quora-question-pairs. G Pretrained Encoders A complete listing of URLs for all used pretrained encoders is provided in Tab. 15. For monolingual MLMs of each language, we made the best effort to select the most popular one (based on download counts). For computational tractability of the large number of experiments conducted, all models are BASE models (instead of LARGE). H Full Tables Here, we provide the complete sets of results. In these tables we include both MLMs w/ features extracted using both mean-pooling (\u201cmp\u201d) and [CLS] (\u201cCLS\u201d). For full multilingual word similarity results, view Tab. 16. For full Spanish, Arabic and Russian STS results, view Tab. 3. For full cross-lingual word similarity results, view Tab. 18. For full BLI results, view Tab. 19. For full ablation study results, view Tab. 20. For full MVN and IS scores, view Tab. 14. I Number of Model Parameters All BERT/RoBERTa models in this paper have \u2248110M parameters. J Hyperparameter Optimisation Tab. 21 lists the hyperparameter search space. Note that the chosen hyperparameters yield the overall best performance, but might be suboptimal on any single", "chunk_summary": "\u8be5\u6bb5\u843d\u4e3b\u8981\u8ba8\u8bba\u4e86MLM\u6a21\u578b\u7684\u8d85\u53c2\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5bf9\u4e8edropout\u7387\uff0cBERT\u6a21\u578b\u66f4\u559c\u6b22\u8f83\u5927\u7684dropout\u7387\uff080.2\u548c0.3\uff09\uff0c\u800cRoBERTa\u6a21\u578b\u66f4\u559c\u6b22\u8f83\u5c0f\u7684dropout\u7387\uff080.05\uff09\u3002\u5bf9\u4e8e\u968f\u673a\u5207\u5757\u63a9\u7801\u7387\uff0cRoBERTa\u6a21\u578b\u5bf9\u66f4\u5927\u7684\u63a9\u7801\u7387\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u6bd4BERT\u6a21\u578b\u5bf9\u8fd9\u4e2a\u8d85\u53c2\u6570\u66f4\u5177\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u5f15\u5165\u4e86\u5747\u503c\u5411\u91cfL2\u8303\u6570\uff08MVN\uff09\u4f5c\u4e3a\u8865\u5145\u8bc1\u636e\uff0c\u8be5\u6307\u6807\u4e0e\u5f62\u72b6\u7b49\u5ea6\u91cf\u6307\u6807\u4e00\u81f4\uff0c\u8868\u660e+Mirror\u8c03\u6574\u4f7f\u7a7a\u95f4\u66f4\u52a0\u7b49\u5ea6\u5316\uff0c\u5e76\u5c06\u7a7a\u95f4\u4e2d\u5fc3\u70b9\u504f\u79fb\u81f3\u5b9e\u9645\u6570\u636e\u70b9\u7684\u533a\u57df\u3002\u6700\u540e\uff0c\u8be5\u6bb5\u843d\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u683c\u548c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684URL\u5217\u8868\u3002"}, "42": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "16,17", "chunk_id": "25", "chunk_text": "oders A complete listing of URLs for all used pretrained encoders is provided in Tab. 15. For monolingual MLMs of each language, we made the best effort to select the most popular one (based on download counts). For computational tractability of the large number of experiments conducted, all models are BASE models (instead of LARGE). H Full Tables Here, we provide the complete sets of results. In these tables we include both MLMs w/ features extracted using both mean-pooling (\u201cmp\u201d) and [CLS] (\u201cCLS\u201d). For full multilingual word similarity results, view Tab. 16. For full Spanish, Arabic and Russian STS results, view Tab. 3. For full cross-lingual word similarity results, view Tab. 18. For full BLI results, view Tab. 19. For full ablation study results, view Tab. 20. For full MVN and IS scores, view Tab. 14. I Number of Model Parameters All BERT/RoBERTa models in this paper have \u2248110M parameters. J Hyperparameter Optimisation Tab. 21 lists the hyperparameter search space. Note that the chosen hyperparameters yield the overall best performance, but might be suboptimal on any single setting (e.g. different base model). K Software and Hardware Dependencies All our experiments are implemented using Py- Torch 1.7.0 and huggingface.co transformers 4.4.2, with Automatic Mixed Precision (AMP)17 turned on during training. Please refer to the GitHub repo for details. The hardware we use is listed in Tab. 22. 17https://pytorch.org/docs/stable/amp. html 16 model URL fastText https://fasttext.cc/docs/en/crawl-vectors.html SBERT https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens SapBERT https://huggingface.co/cambridgeltl/SapBERT-from-PubMedBERT-fulltext BERT (English) https://huggingface.co/bert-base-uncased RoBERTa (English) https://huggingface.co/roberta-base mBERT https://huggingface.co/bert-base-multilingual-uncased Turkish BERT dbmdz/bert-base-turkish-uncased Italian BERT dbmdz/bert-base-italian-uncased French BERT https://huggingface.co/camembert-base Spanish BERT https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased Russian BERT https://huggingface.co/DeepPavlov/rubert-base-cased Chinese BERT https://huggingface.co/bert-base-chinese Arabic BERT https://huggingface.co/aubmindlab/bert-base-arabertv02 Polish BERT https://huggingface.co/dkleczek/bert-base-polish-uncased-v1 Estonian BERT https://huggingface.co/tartuNLP/EstBERT Table 15: A listing of HuggingFace & fastText URLs of all pretrained models used in this work. language\u2192 EN FR ET AR ZH RU ES PL avg. fastText .528 .560 .447 .409 .428 .435 .488 .396 .461 BERT-CLS .105 .050 .160 .210 .277 .177 .152 .257 .174 BERT-mp .267 .020 .106 .220 .398 .202 .177 .217 .201 + Mirror .556 .621 .308 .538 .639 .365 .296 .444 .471 mBERT-CLS .062 .046 .074 .047 .204 .063 .039 .051 .073 mBERT-mp .105 .130 .094 .101 .261 .109 .095 .087 .123 + Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264 Table 16: Word similarity evaluation on Multi-SimLex (Spearman\u2019s \u03c1). model\u2193, lang.\u2192 ES AR RU avg. BERT-CLS .526 .308 .470 .435 BERT-mp .599 .455 .552 .535 + Mirror .709 .669 .673 .684 mBERT-CLS .421 .326 .430 .392 mBERT-mp .610 .447 .616 .558 + Mirror .755 .594 .692 .680 Table 17: Full Spanish, Arabic and Russian STS evaluation. Spearman\u2019s \u03c1 correlation reported. lang.\u2192 EN-FR EN-ZH EN-HE FR-ZH FR-HE ZH-HE avg. mBERT-CLS .059 .053 .032 .042 .024 .050 .043 mBERT-mp .163 .118 .071 .142 .104 .010 .101 + Mirror .454 .385 .133 .465 .163 .179 .297 Table 18: Full cross-lingual word similarity evaluation on Multi-SimLex (Spear", "chunk_summary": "\u672c\u6587\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684URL\u5b8c\u6574\u5217\u8868\u3002\u4e3a\u4e86\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u7684\u53ef\u8ba1\u7b97\u6027\uff0c\u6240\u6709\u6a21\u578b\u5747\u4e3aBASE\u6a21\u578b\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7ed3\u679c\u96c6\uff0c\u5305\u62ec\u591a\u8bed\u8a00\u5355\u8bedMLM\u548c\u4f7f\u7528\u5747\u503c\u6c47\u96c6\u548c[CLS]\u7684MLM\u7684\u529f\u80fd\u63d0\u53d6\u3002\u6b64\u5916\u8fd8\u63d0\u4f9b\u4e86\u7528\u4e8e\u5168\u8bed\u8a00\u8bcd\u76f8\u4f3c\u5ea6\u3001\u897f\u73ed\u7259\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u548c\u4fc4\u8bedSTS\u7ed3\u679c\u3001\u5168\u8bed\u8a00\u8bcd\u76f8\u4f3c\u5ea6\u4ea4\u53c9\u6d4b\u8bd5\u3001BLI\u7ed3\u679c\u3001\u5220\u53bb\u7814\u7a76\u7ed3\u679c\u3001MVN\u548cIS\u5206\u6570\u3001\u6a21\u578b\u53c2\u6570\u6570\u91cf\u3001\u8d85\u53c2\u6570\u4f18\u5316\u3001\u8f6f\u4ef6\u548c\u786c\u4ef6\u4f9d\u8d56\u9879\u7b49\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\u3002"}, "43": {"source": "Fast, Effective, and Self-Supervised Transforming Masked Language.pdf", "source_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5305\u62ecMirror-BERT\u3001SimCSE\u3001DeCLUTR\u548cPoly-encoders\u7b49\uff0c\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8868\u793a\u5b66\u4e60\u5728\u6587\u672c\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0cMirror-BERT\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u901a\u7528\u7684\u8bcd\u6c47\u548c\u53e5\u5b50\u7f16\u7801\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002\u6587\u7ae0\u7efc\u5408\u4e86\u524d\u9762\u63d0\u5230\u7684\u591a\u7bc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u8bed\u8a00\u8868\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6709\u5173\u6570\u636e\u589e\u5f3a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8bed\u4e49\u7ec6\u8c03\u7b49\u65b9\u9762\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6240\u6709\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684HuggingFace\u548cFastText URL\u5217\u8868\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002", "source_page_nums": "18", "source_chunk_nums": "26", "semantic_tags": "", "regular_tags": "", "page_span": "17", "chunk_id": "26", "chunk_text": "mp .105 .130 .094 .101 .261 .109 .095 .087 .123 + Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264 Table 16: Word similarity evaluation on Multi-SimLex (Spearman\u2019s \u03c1). model\u2193, lang.\u2192 ES AR RU avg. BERT-CLS .526 .308 .470 .435 BERT-mp .599 .455 .552 .535 + Mirror .709 .669 .673 .684 mBERT-CLS .421 .326 .430 .392 mBERT-mp .610 .447 .616 .558 + Mirror .755 .594 .692 .680 Table 17: Full Spanish, Arabic and Russian STS evaluation. Spearman\u2019s \u03c1 correlation reported. lang.\u2192 EN-FR EN-ZH EN-HE FR-ZH FR-HE ZH-HE avg. mBERT-CLS .059 .053 .032 .042 .024 .050 .043 mBERT-mp .163 .118 .071 .142 .104 .010 .101 + Mirror .454 .385 .133 .465 .163 .179 .297 Table 18: Full cross-lingual word similarity evaluation on Multi-SimLex (Spearman\u2019s \u03c1). lang.\u2192 EN-FR EN-IT EN-RU EN-TR IT-FR RU-FR avg. BERT-CLS .045 .049 .108 .109 .046 .068 .071 BERT-mp .014 .112 .154 .150 .025 .018 .079 + Mirror .458 .378 .336 .289 .417 .345 .371 Table 19: Full Bilingual Lexicon Induction results (accuracy reported). \u201cEN-FR\u201d means en mapped to FR. 17 model con\ufb01guration\u2193, dataset\u2192 STS12 STS13 STS14 STS15 STS16 STS-b SICK-R avg. BERT + Mirror .674 .796 .713 .814 .743 .764 .703 .744 - dropout .646 .770 .691 .800 .726 .745 .701 .726\u2193.018 - random span masking .641 .775 .684 .777 .737 .749 .658 .717\u2193.027 - dropout & random span masking .587 .695 .617 .688 .683 .674 .614 .651\u2193.093 RoBERTa + Mirror .648 .819 .732 .798 .780 .787 .706 .753 - dropout .619 .795 .706 .802 .777 .727 .698 .732\u2193.021 - random span masking .616 .786 .689 .766 .743 .756 .663 .717\u2193.036 - dropout & random span masking .562 .730 .643 .744 .752 .708 .638 .682\u2193.071 Table 20: Full table for the synergistic effect of dropout and random span masking in sentence similarity tasks. hyperparameters search space learning rate {5e-5, 2e-5\u2217, 1e-5} batch size {100, 200\u2217, 300} training epochs {1\u2217, 2\u2217, 3, 5} \u03c4 in Eq. (1) {0.03, 0.04\u2217, 0.05, 0.07, 0.1, 0.2\u2217, 0.3} Table 21: Hyperparameters along with their search grid. \u2217 marks the values used to obtain the reported results. The hparams are not always optimal in every setting but generally performs (close to) the best. hardware speci\ufb01cation RAM 128 GB CPU AMD Ryzen 9 3900x 12-core processor \u00d7 24 GPU NVIDIA GeForce RTX 2080 Ti (11 GB) \u00d7 2 Table 22: Hardware speci\ufb01cations of the used machine. When encountering out-of-memoery error, we also used a second server with two NVIDIA GeForce RTX 3090 (24 GB). 18 ", "chunk_summary": "\u672c\u6bb5\u843d\u4e3b\u8981\u662f\u5c55\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e0b\u7684\u5355\u8bcd\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u3001\u8de8\u8bed\u8a00\u53e5\u5b50\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u548c\u53cc\u8bed\u8bcd\u6c47\u5f52\u7eb3\u7ed3\u679c\uff0c\u4ee5\u53ca\u5728\u53e5\u5b50\u76f8\u4f3c\u5ea6\u4efb\u52a1\u4e2d\uff0cdropout\u548c\u968f\u673a\u906e\u63a9\u7684\u76f8\u4e92\u4f5c\u7528\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u8fd8\u5728\u6587\u7ae0\u672b\u5c3e\u7ed9\u51fa\u4e86\u6a21\u578b\u6240\u4f7f\u7528\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u3002"}}